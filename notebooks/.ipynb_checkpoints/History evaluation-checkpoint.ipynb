{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from os import chdir\n",
    "import subprocess\n",
    "from dateutil.parser import parse\n",
    "import os\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"...\"\n",
    "headers = {'Authorization': 'token ' + token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"pallets/flask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_responses(token, url):\n",
    "    responses = []\n",
    "    while True:\n",
    "        headers = {'Authorization': 'token ' + token}\n",
    "        rsp = requests.get(url, headers=headers)\n",
    "        responses.append(rsp)\n",
    "        if 'next' in rsp.links:\n",
    "            url = rsp.links['next']['url']\n",
    "        else:\n",
    "            break\n",
    "    return responses\n",
    "\n",
    "def rate_limit(response):\n",
    "    print(\"Rate remaining:\", response.headers['X-RateLimit-Remaining'])\n",
    "    print(\"Rate limit reset:\", datetime.fromtimestamp(int(response.headers['X-RateLimit-Reset'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch PRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.github.com/search/issues?q=repo:pallets/flask+is:merged+base:main&per_page=100'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = f\"https://api.github.com/search/issues?q=repo:{repo}+is:merged+base:main&per_page=100\"\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Response [200]>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses = fetch_responses(token, url)\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate remaining: 29\n",
      "Rate limit reset: 2022-03-25 16:14:51\n"
     ]
    }
   ],
   "source": [
    "rate_limit(responses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = []\n",
    "for response in responses:\n",
    "    items += response.json()['items']\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4492, 4491, 4488, 4487, 4486]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = [x['number'] for x in items]\n",
    "numbers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 20/41 [00:11<00:12,  1.68it/s]"
     ]
    }
   ],
   "source": [
    "pulls_resps = []\n",
    "for number in tqdm(numbers):\n",
    "    url = f\"https://api.github.com/repos/{repo}/pulls/{number}\"\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    pulls_resps.append(resp)\n",
    "print(set([x.status_code for x in pulls_resps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limit(pulls_resps[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pr in pulls_resps[20:30]:\n",
    "    print(pr.json()['merge_commit_sha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulls_resps[0].json().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milestones = set()\n",
    "for pr in pulls_resps:\n",
    "    pr = pr.json()\n",
    "    if pr['milestone']:\n",
    "        milestones.add(pr['milestone']['id'])\n",
    "milestones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = {}\n",
    "for pr in pulls_resps:\n",
    "    pr = pr.json()\n",
    "    sha = pr['merge_commit_sha']\n",
    "    prs[sha] = {\n",
    "        'title': pr['title'],\n",
    "        'sha': sha\n",
    "    }\n",
    "len(prs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://api.github.com/repos/{repo}/releases?per_page=100\"\n",
    "release_resps = fetch_responses(token, url)\n",
    "print(set([x.status_code for x in release_resps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_tags = set()\n",
    "for r in release_resps[0].json():\n",
    "    if r['target_commitish'] == 'main' and r['draft'] == False and r['prerelease'] == False:\n",
    "        release_tags.add(r['tag_name'])\n",
    "print(len(release_tags))\n",
    "list(release_tags)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_limit(release_resps[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_resps[0].json()[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_resps[0].json()[0]['html_url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontrollera att alla PR, tags finns med lokalt\n",
    "\n",
    "- Plocka ut en lista med alla sha lokalt\n",
    "- Map pr->sha för att kolla om alla pr finns i historiken\n",
    "- Kan även kolla för release tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(f\"git clone https://github.com/{repo}\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = repo.split('/')[-1]\n",
    "repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chdir(repo_dir)\n",
    "try:\n",
    "    subprocess.run(\"git checkout main\", shell=True)\n",
    "    \n",
    "    lines = subprocess.getoutput(\"git log --format=format:%H\").split('\\n')\n",
    "    commits = list(reversed(lines))\n",
    "    print(len(commits))\n",
    "    print(commits[:3])\n",
    "    \n",
    "    print()\n",
    "    all_tags = {}\n",
    "    for tag in release_tags:\n",
    "        # OBS: Detta inkluderar även tags från andra branches...\n",
    "        lines = subprocess.getoutput(f\"git rev-list -n 1 {tag}\").split('\\n')\n",
    "        sha = lines[0]\n",
    "        all_tags[sha] = tag\n",
    "    print(all_tags)\n",
    "finally:\n",
    "    chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(commits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in prs.keys():\n",
    "    assert key in commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Endast tag från main\n",
    "tags = {}\n",
    "for sha, tag in all_tags.items():\n",
    "    if sha in commits:\n",
    "        tags[sha] = tag\n",
    "\n",
    "print(len(tags), \"release tags on main\")\n",
    "for key in tags.keys():\n",
    "    assert key in commits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sektionera PR mellan releases\n",
    "\n",
    "Gå igenom alla commits i 2 vändor O(2*n) = O(n)\n",
    "1. Extrahera tags -> sections {start: tag, prs: [], end: tag}\n",
    "2. Öka section för varje tag och lägg till pr på rätt plats\n",
    "\n",
    "Ordningen baseras på commits i main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_commits = [c for c in commits if c in tags]\n",
    "\n",
    "sections = []\n",
    "for i in range(len(tag_commits)-1):\n",
    "    sections.append({\n",
    "        'start': tags[tag_commits[i]],\n",
    "        'prs': [],\n",
    "        'end': tags[tag_commits[i+1]],\n",
    "    })\n",
    "\n",
    "for c in sections:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_commits = [c for c in commits if c in prs]\n",
    "if commits.index(pr_commits[0]) > commits.index(tag_commits[-1]):\n",
    "    print(\"WARNING - alla releases kommer innan första PR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = -1\n",
    "for commit in commits:\n",
    "    if 0 <= sec < len(sections) and commit in prs:\n",
    "        sections[sec]['prs'].append(prs[commit])\n",
    "    if commit in tags:\n",
    "        # Inkludera sista PR om den skulle vara en tag\n",
    "        # OBS: den första ska inte inkuderas - annars bli diffen fel\n",
    "        sec += 1\n",
    "        \n",
    "for section in sections:\n",
    "    print(len(section['prs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporär lösning, hårdkoda några random sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(len(prs))\n",
    "release_indices = [5, 40, 100, 435]\n",
    "\n",
    "sections = []\n",
    "for i in range(len(release_indices)-1):\n",
    "    start = release_indices[i]\n",
    "    end = release_indices[i+1]\n",
    "    sections.append({\n",
    "        'start': pr_commits[start],\n",
    "        'prs': pr_commits[start+1:end],\n",
    "        'end': pr_commits[end],\n",
    "    })\n",
    "\n",
    "sections[0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Changes - modified only (ignore adds, dels)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_file(path):\n",
    "    l = path\n",
    "    return (l.endswith('.md') or l.endswith('.rst')) # and l.startswith('doc/')\n",
    "\n",
    "chdir(repo_dir)\n",
    "try:\n",
    "    subprocess.run(\"git checkout main\", shell=True)\n",
    "    \n",
    "    for section in sections:\n",
    "        # Extract actual changes from history\n",
    "        start = section['start']\n",
    "        end = section['end']\n",
    "        command = f\"git diff '{start}' '{end}' --name-only\"\n",
    "        lines = subprocess.getoutput(command).split('\\n')\n",
    "        changes = list(filter(doc_file, lines))\n",
    "        section['changes'] = changes\n",
    "        print(len(changes), 'changes', changes[:3], '...')\n",
    "        \n",
    "finally:\n",
    "    chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hämta docs filer, så som de såg ut i början av varje PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(start_dir, ext):\n",
    "    \"\"\" Search files recursively \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(start_dir):\n",
    "        path = start_dir + \"/\" + file\n",
    "        if os.path.isdir(path):\n",
    "            files += find(path, ext)\n",
    "        elif os.path.isfile(path) and file.endswith(ext):\n",
    "            files.append(path)\n",
    "    return files\n",
    "\n",
    "def read(path):\n",
    "    with open(path) as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "def extract_docs(exts=['.md', '.rst']):\n",
    "    docs = {}\n",
    "    for ext in exts:\n",
    "        paths = find(repo_dir, ext)\n",
    "        for path in paths:\n",
    "            key = path[len(repo_dir)+1:]\n",
    "            docs[key] = read(path)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_shas = []\n",
    "for section in sections:\n",
    "    pr_shas += [pr['sha'] for pr in section['prs']]\n",
    "print(len(pr_shas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {}\n",
    "for sha in tqdm(pr_shas):\n",
    "    # Modellen behöver veta hur docs var innan PR, därför ^1\n",
    "    # antag att varje PR består av 1 commit i master\n",
    "    # det går att lösa annars också, men det blir mer komplicerat och involverar GitHub's API\n",
    "    subprocess.run(f'git checkout {sha}^1', shell=True)\n",
    "    docs[sha] = extract_docs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TODO: Extrahera filer, filnamn i uppdaterat format (ladda från sido-repo)**\n",
    "- **TODO: Metadata för varje PR**\n",
    "- **TODO: prova om det går att använda base_sha istället för ^1**\n",
    "- **TODO: Hitta repo som fungerar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(pr, all_docs):\n",
    "    return all_docs\n",
    "\n",
    "def predict_none(pr, all_docs):\n",
    "    return []\n",
    "\n",
    "def predict_tfidf(pr, all_docs):\n",
    "    \n",
    "    def search(text, tfidf):\n",
    "        query = vectorizer.transform([text])\n",
    "        cosine_sims = linear_kernel(query, tfidf).flatten()\n",
    "        return sorted(zip(cosine_sims, doc_files), reverse=True)\n",
    "    \n",
    "    doc_files = list(all_docs.keys())\n",
    "    corpus = [all_docs[doc] for doc in doc_files]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    result = []\n",
    "    for match in search(pr['title'], tfidf):\n",
    "        result.append(match[1])\n",
    "    return result[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = predict_tfidf\n",
    "\n",
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for section in sections:\n",
    "    actual_changes = set(section['changes'])\n",
    "    \n",
    "    model_changes = set()\n",
    "    for pr in section['prs']:\n",
    "        prediction = set(predict(pr, docs[pr['sha']]))\n",
    "        model_changes = model_changes.union(prediction)\n",
    "    \n",
    "    tp += len(actual_changes.intersection(model_changes))\n",
    "    fp += len(model_changes.difference(actual_changes))\n",
    "    fn += len(actual_changes.difference(model_changes))\n",
    "    \n",
    "\n",
    "try:\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "except ZeroDivisionError:\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "\n",
    "\n",
    "print(\"f1:\", f1)\n",
    "print(\"precision:\", precision)\n",
    "print(\"recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
