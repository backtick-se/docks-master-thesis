{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042a1ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shabo/Documents/Backtick/exjobb/venv/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ce63af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np\n",
    "from utils import load, dump\n",
    "from extract import Extractor\n",
    "from os import getcwd\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup as Bfs\n",
    "from itertools import chain\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e65d810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load(f'{getcwd()}/../data/prd_backtick-se_cowait_annotated.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4aa1a210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 Pytest marks support for cowait test\n",
      "Targets:\n",
      "cowait/docs/kubernetes/testing.md\n",
      "cowait/docs/get-started/tests.md\n",
      "Predictions:\n",
      "(0.4152569532370608, 'cowait/docs/get-started/tests.md')\n",
      "(0.339346764847835, 'cowait/docs/kubernetes/testing.md')\n",
      "Pytest marks support for cowait test\n",
      "add marks argument to cowait test\n",
      "\n",
      "325 Improve cowait test\n",
      "Targets:\n",
      "cowait/docs/kubernetes/testing.md\n",
      "cowait/docs/get-started/tests.md\n",
      "Predictions:\n",
      "(0.2747287863532659, 'cowait/docs/get-started/tests.md')\n",
      "(0.1722613877015308, 'cowait/docs/kubernetes/testing.md')\n",
      "Improve cowait test\n",
      "Adds two new flags:\n",
      "- --verbose enables verbose output from pytest (false by default)\n",
      "- --capture toggles output capturing (true by default)\n",
      "Improved the pytest argument generation code\n",
      "add verbosity, output capture arguments to cowait test\n",
      "\n",
      "327 Improve logs command\n",
      "Targets:\n",
      "cowait/docs/kubernetes/pushing-and-running.md\n",
      "cowait/docs/get-started/first-steps.md\n",
      "Predictions:\n",
      "(0.20002402860645369, 'cowait/docs/get-started/dashboard.md')\n",
      "(0.1494112924281309, 'cowait/docs/tasks/built-in-tasks.md')\n",
      "Improve logs command\n",
      "- logs command now allows reading logs from terminated Kubernetes pods\n",
      "- Logs show an error if the task appears to have been lost during execution (i.e. OOM killed or manually deleted)\n",
      "read logs from terminated pods\n",
      "show an error in log output if the log stream ends without a return or an error\n",
      "\n",
      "336 Version 0.4.30\n",
      "Targets:\n",
      "cowait/docs/get-started/asyncio.md\n",
      "cowait/docs/get-started/first-steps.md\n",
      "cowait/docs/quick-start.md\n",
      "Predictions:\n",
      "(0.26660524457816404, 'cowait/docs/tasks/type-system.md')\n",
      "(0.2240067927830588, 'cowait/docs/kubernetes/pushing-and-running.md')\n",
      "(0.20868759331238884, 'cowait/docs/get-started/building-and-pushing.md')\n",
      "Version 0.4.30\n",
      "- Added an option to provide a fallback value for environment values (e.g. to provide secret defaults) when using docker.\n",
      "- Properly mark the --capture option for cowait run as a boolean flag.\n",
      "- Improved error reporting for return type errors.\n",
      "- Fix aiohttp to version 3.7.4 to avoid import errors\n",
      "add a fallback value for environment variables when using docker\n",
      "properly mark --capture option as a flag\n",
      "improved errors from invalid return types\n",
      "fix aiohttp to 3.7.4 to avoid async_timeout import errors\n",
      "bump version to 0.4.30\n",
      "\n",
      "345 add simple test for kubernetes provider\n",
      "Targets:\n",
      "cowait/docs/kubernetes/testing.md\n",
      "Predictions:\n",
      "(0.2901650209934391, 'cowait/docs/get-started/tests.md')\n",
      "add simple test for kubernetes provider\n",
      "This test fails if the KubernetesProvider cannot be instantiated. Such a problem has already happened, and is being addressed by #344.\n",
      "add simple test for kubernetes provider\n",
      "\n",
      "350 add node selector field to task definition\n",
      "Targets:\n",
      "cowait/docs/kubernetes/pushing-and-running.md\n",
      "Predictions:\n",
      "(0.21098091642624817, 'cowait/docs/get-started/tests.md')\n",
      "add node selector field to task definition\n",
      "add node selector field to task definition\n",
      "remove dict literals from TaskDefinition defaults\n",
      "\n",
      "accuracy=0.0\n"
     ]
    }
   ],
   "source": [
    "class TFIDFMapper():\n",
    "    DOCT = 1\n",
    "    SECT = 2\n",
    "    STMT = 3\n",
    "    \n",
    "    def __init__(self, data, granularity):\n",
    "        self.gran = granularity\n",
    "        self.data = data\n",
    "    \n",
    "    def prepare(self):\n",
    "        for pr in self.data:\n",
    "            paths = [*zip(*pr['docs'])][0]\n",
    "            contents = [*zip(*pr['docs'])][1]\n",
    "            contents = [*map(self.clear_doc, contents)]\n",
    "            contents = [*map(self.rendered, contents)]\n",
    "            \n",
    "            if self.gran == self.DOCT:\n",
    "                pr['documents'] = np.array(contents)\n",
    "                pr['locations'] = np.array(paths)\n",
    "                \n",
    "            elif self.gran == self.STMT:\n",
    "                ptrn = r'[A-Z].*?[\\.!?][\\s]'\n",
    "                pat = re.compile(ptrn, re.M)\n",
    "                statements = [*map(pat.findall, contents)]\n",
    "                statements = [*map(lambda s: s[:-2], chain(*statements))]\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        has_content = lambda pr: bool(''.join(pr['documents']))\n",
    "        self.data = [*filter(has_content, self.data)]\n",
    "    \n",
    "    def fit(self):\n",
    "        for pr in self.data:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            pr['vectorizer'] = vectorizer\n",
    "            pr['tfidf'] = vectorizer.fit_transform(pr['documents'])\n",
    "    \n",
    "    def predict(self):\n",
    "        for pr in self.data:\n",
    "            query_input = self.query_input(pr)\n",
    "            query = pr['vectorizer'].transform([query_input])\n",
    "            cosine_sims = linear_kernel(query, pr['tfidf']).flatten()\n",
    "            pr['prediction'] = sorted(zip(cosine_sims, pr['locations']), reverse=True)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        y_hat = []\n",
    "        y_tru = []\n",
    "        \n",
    "        for pr in self.data:\n",
    "            target = pr['target']\n",
    "            tarlen = len(target)\n",
    "            title = pr['title']\n",
    "            number = pr['number']\n",
    "\n",
    "            preds = pr['prediction'][:tarlen]\n",
    "\n",
    "            if tarlen:\n",
    "                for i, pred in enumerate(preds):\n",
    "                    y_tru.append(pred[1])\n",
    "                    y_hat.append(target[i])\n",
    "                \n",
    "                print(number, title)\n",
    "                \n",
    "                print('Targets:')\n",
    "                for tar in target:\n",
    "                    print(tar)\n",
    "                print('Predictions:')\n",
    "                for pred in preds:\n",
    "                    print(pred)\n",
    "                print()\n",
    "\n",
    "        accuracy = accuracy_score(y_tru, y_hat)\n",
    "        print(f'{accuracy=}')\n",
    "    \n",
    "    def query_input(self, pr):\n",
    "        \"\"\"\n",
    "        PR title\n",
    "        PR body\n",
    "        commit msg 1\n",
    "        commit msg 2\n",
    "        ...\n",
    "        \"\"\"\n",
    "        \n",
    "        title = pr['title']\n",
    "        body = pr['body'] if pr['body'] else ''\n",
    "        query_input = f'{title}\\n{body}'\n",
    "        \n",
    "        for commit in pr['commits']:\n",
    "            msg = commit['commit']['message']\n",
    "            query_input += f'\\n{msg}'\n",
    "        \n",
    "        return self.rendered(query_input)\n",
    "    \n",
    "    def dump(self, file):\n",
    "        dump(self.data, file)\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear_doc(md):\n",
    "        # Remove title table\n",
    "        pattern = r'(---\\ntitle:.*\\n---\\n)'\n",
    "        return ''.join(re.split(pattern, md)[2:])\n",
    "    \n",
    "    @staticmethod\n",
    "    def rendered(md):\n",
    "        if md:\n",
    "            html = markdown(md)\n",
    "            return ''.join(Bfs(html).findAll(text=True))\n",
    "        return ''\n",
    "    \n",
    "mapper = TFIDFMapper(dataset, SemanticMapper.DOCT)\n",
    "mapper.prepare()\n",
    "mapper.fit()\n",
    "mapper.predict()\n",
    "mapper.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb50f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_content(md):\n",
    "    # Remove title table\n",
    "    pattern = r'(---\\ntitle:.*\\n---\\n)'\n",
    "    txt = ''.join(re.split(pattern, md)[2:])\n",
    "    \n",
    "    # Split on sections\n",
    "    #pattern = r'## .*\\n'\n",
    "    #split = lambda d: re.split(pattern, d)\n",
    "    #data = [*map(split, data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1cc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
