{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042a1ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shabo/Documents/Backtick/exjobb/venv/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ce63af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np\n",
    "from utils import load, dump\n",
    "from extract import Extractor\n",
    "from os import getcwd\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup as Bfs\n",
    "from itertools import chain\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e65d810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load(f'{getcwd()}/../data/prd_backtick-se_cowait_annotated.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6bc17b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFMapper():\n",
    "    DOCT = 1\n",
    "    #SECT = 2\n",
    "    STMT = 3\n",
    "    \n",
    "    def __init__(self, data, granularity):\n",
    "        self.gran = granularity\n",
    "        self.data = data\n",
    "    \n",
    "    def prepare(self):\n",
    "        for pr in self.data:\n",
    "            paths = [*zip(*pr['docs'])][0]\n",
    "            contents = [*zip(*pr['docs'])][1]\n",
    "            contents = [*map(self.clear_doc, contents)]\n",
    "            contents = [*map(self.rendered, contents)]\n",
    "            \n",
    "            if self.gran == self.DOCT:\n",
    "                pr['documents'] = np.array(contents)\n",
    "                pr['locations'] = np.array(paths)\n",
    "                \n",
    "            elif self.gran == self.STMT:\n",
    "                ptrn = r'[A-Z].*?[\\.!?][\\s]'\n",
    "                pat = re.compile(ptrn, re.M)\n",
    "                \n",
    "                contents = [*map(pat.findall, contents)] # [[stmt, stmt]]\n",
    "                locations = [[paths[i]]*len(s) for i, s in enumerate(contents)]\n",
    "                locations = [*chain(*locations)]\n",
    "                contents = [*map(lambda s: s[:-2], chain(*contents))] # [stmt, stmt]\n",
    "                \n",
    "                pr['documents'] = np.array(contents)\n",
    "                pr['locations'] = np.array(locations)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        has_content = lambda pr: bool(''.join(pr['documents']))\n",
    "        self.data = [*filter(has_content, self.data)]\n",
    "    \n",
    "    def fit(self):\n",
    "        for pr in self.data:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            pr['vectorizer'] = vectorizer\n",
    "            pr['tfidf'] = vectorizer.fit_transform(pr['documents'])\n",
    "    \n",
    "    def predict(self):\n",
    "        for pr in self.data:\n",
    "            query_input = self.query_input(pr)\n",
    "            query = pr['vectorizer'].transform([query_input])\n",
    "            cosine_sims = linear_kernel(query, pr['tfidf']).flatten()\n",
    "            pr['prediction'] = zip(cosine_sims, pr['locations']) #, reverse=True)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        y_hat = []\n",
    "        y_tru = []\n",
    "        \n",
    "        for pr in self.data:\n",
    "            target = pr['target']\n",
    "            tarlen = len(target)\n",
    "            title = pr['title']\n",
    "            number = pr['number']\n",
    "            \n",
    "            preds = [(pred, pr['documents'][i]) for i, pred in enumerate(pr['prediction'])]\n",
    "            preds = sorted(preds, key=lambda pred: pred[0][0], reverse=True)[:tarlen]\n",
    "\n",
    "            if tarlen:\n",
    "                for i, pred in enumerate(preds):\n",
    "                    y_tru.append(pred[0][1])\n",
    "                    y_hat.append(target[i])\n",
    "                \n",
    "                print(number, title)\n",
    "                \n",
    "                print('Targets:')\n",
    "                for tar in target:\n",
    "                    print(tar)\n",
    "                print('Predictions:')\n",
    "                for pred in preds:\n",
    "                    print(pred[0], pred[1][:50].replace('\\n', '|'))\n",
    "                print()\n",
    "\n",
    "        accuracy = accuracy_score(y_tru, y_hat)\n",
    "        print(f'{accuracy=}')\n",
    "    \n",
    "    def query_input(self, pr):\n",
    "        \"\"\"\n",
    "        PR title\n",
    "        PR body\n",
    "        commit msg 1\n",
    "        commit msg 2\n",
    "        ...\n",
    "        \"\"\"\n",
    "        \n",
    "        title = pr['title']\n",
    "        body = pr['body'] if pr['body'] else ''\n",
    "        query_input = f'{title}\\n{body}'\n",
    "        \n",
    "        for commit in pr['commits']:\n",
    "            msg = commit['commit']['message']\n",
    "            query_input += f'\\n{msg}'\n",
    "        \n",
    "        return self.rendered(query_input)\n",
    "    \n",
    "    def dump(self, file):\n",
    "        dump(self.data, file)\n",
    "    \n",
    "    @staticmethod\n",
    "    def clear_doc(md):\n",
    "        # Remove title table\n",
    "        pattern = r'(---\\ntitle:.*\\n---\\n)'\n",
    "        return ''.join(re.split(pattern, md)[2:])\n",
    "    \n",
    "    @staticmethod\n",
    "    def rendered(md):\n",
    "        if md:\n",
    "            html = markdown(md)\n",
    "            return ''.join(Bfs(html).findAll(text=True))\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b7c1cc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 Pytest marks support for cowait test\n",
      "Targets:\n",
      "cowait/docs/kubernetes/testing.md\n",
      "cowait/docs/get-started/tests.md\n",
      "Predictions:\n",
      "(0.42230213616830314, 'cowait/docs/get-started/tests.md') Cowait uses pytest\n",
      "(0.32438759482839985, 'cowait/docs/kubernetes/testing.md') To make sure your tasks work in a cluster environm\n",
      "\n",
      "325 Improve cowait test\n",
      "Targets:\n",
      "cowait/docs/kubernetes/testing.md\n",
      "cowait/docs/get-started/tests.md\n",
      "Predictions:\n",
      "(0.44711151456154896, 'cowait/docs/get-started/tests.md') New output location\n",
      "(0.34265977775040146, 'cowait/docs/get-started/tests.md') Cowait uses pytest\n",
      "\n",
      "327 Improve logs command\n",
      "Targets:\n",
      "cowait/docs/kubernetes/pushing-and-running.md\n",
      "cowait/docs/get-started/first-steps.md\n",
      "Predictions:\n",
      "(0.46192819007450653, 'cowait/docs/tasks/built-in-tasks.md') Container logs are forwarded to the task log\n",
      "(0.19545901515824535, 'cowait/docs/quick-start.md') This allows us to wait for other tasks in an async\n",
      "\n",
      "336 Version 0.4.30\n",
      "Targets:\n",
      "cowait/docs/get-started/asyncio.md\n",
      "cowait/docs/get-started/first-steps.md\n",
      "cowait/docs/quick-start.md\n",
      "Predictions:\n",
      "(0.2546496769428354, 'cowait/docs/kubernetes/pushing-and-running.md') To use the kubernetes task provider, simply use th\n",
      "(0.22733895753429317, 'cowait/docs/get-started/building-and-pushing.md') In a scenario when you want to run your task(s) on\n",
      "(0.21679509501252217, 'cowait/docs/get-started/installation.md') Note that tasks you use to test your new feature o\n",
      "\n",
      "345 add simple test for kubernetes provider\n",
      "Targets:\n",
      "cowait/docs/kubernetes/testing.md\n",
      "Predictions:\n",
      "(0.3486017090314232, 'cowait/docs/kubernetes/pushing-and-running.md') To use the kubernetes task provider, simply use th\n",
      "\n",
      "350 add node selector field to task definition\n",
      "Targets:\n",
      "cowait/docs/kubernetes/pushing-and-running.md\n",
      "Predictions:\n",
      "(0.28031203089321777, 'cowait/docs/extras/spark.md') Add it to your requirements.txt (or install it in \n",
      "\n",
      "accuracy=0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "mapper = TFIDFMapper(dataset, SemanticMapper.STMT)\n",
    "mapper.prepare()\n",
    "mapper.fit()\n",
    "mapper.predict()\n",
    "mapper.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99612b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
