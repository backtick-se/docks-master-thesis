{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114e71da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from utils import load, dump, distill, categories, load_scraped_data, load_diff_data\n",
    "from evaluate import eval, Evaluator\n",
    "from os import getcwd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root = f'{getcwd()}/..'\n",
    "path = lambda p: f'{root}/../out/{p}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65692512",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = f'{root}/../scraped_diffs.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06dbf100",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nl = path('distilbert-base-uncased_scraped_5e-04_thaw1.pt')\n",
    "best_diff = path('CodeBERTa-small-v1_diffs_pp_dis_solid.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c145b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_nl = Evaluator(best_nl)\n",
    "eval_diff = Evaluator(best_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "061c0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_in, nl_labels = load_scraped_data(datafile)\n",
    "di_in, di_labels = load_diff_data(datafile)\n",
    "\n",
    "assert(nl_labels == di_labels)\n",
    "\n",
    "inputs = [*zip(nl_in, di_in)]\n",
    "labels = nl_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68afeb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_percent = 0.8\n",
    "split=round(len(inputs)*split_percent)\n",
    "\n",
    "#inputs = inputs[split:]\n",
    "#labels = labels[split:]\n",
    "\n",
    "assert(len(inputs) == len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73d53fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 19674/38550 [2:46:23<2:33:31,  2.05it/s] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 81%|████████  | 31239/38550 [4:25:54<56:11,  2.17it/s]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 38550/38550 [5:27:53<00:00,  1.96it/s]  \n"
     ]
    }
   ],
   "source": [
    "metaset = {'inputs': [], 'labels': []}\n",
    "\n",
    "# Stacked outputs from both classifiers\n",
    "# M1 (nl) = [n1, n2, n3, n4]\n",
    "# M2 (diff) = [d1, d2, d3, d4]\n",
    "# Metaset = [[n1, n2, n3, n4, d1, d2, d3, d4]]\n",
    "with torch.no_grad():\n",
    "    for (nl, diff), label in tqdm([*zip(inputs, labels)]):\n",
    "        m1 = eval_nl.predict(nl, raw=True)\n",
    "        m2 = eval_diff.predict(diff, raw=True)\n",
    "        stacked = torch.cat((m1, m2), -1)\n",
    "        metaset['inputs'].append(stacked)\n",
    "        metaset['labels'].append(label)\n",
    "\n",
    "dump(metaset, 'metaset2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1fa58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaset = load('metaset2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e0bb57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_percent = 0.8\n",
    "split=round(len(metaset['inputs'])*split_percent)\n",
    "\n",
    "train_inputs = metaset['inputs'][:split]\n",
    "train_labels = metaset['labels'][:split]\n",
    "valid_inputs = metaset['inputs'][split:]\n",
    "valid_labels = metaset['labels'][split:]\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "      'inputs': train_inputs,\n",
    "      'labels': [categories.index(c) for c in train_labels]\n",
    "    }),\n",
    "    'valid': Dataset.from_dict({\n",
    "      'inputs': valid_inputs,\n",
    "      'labels': [categories.index(c) for c in valid_labels]\n",
    "    }),\n",
    "})\n",
    "\n",
    "datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc466939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, inp, out):\n",
    "\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(inp, out)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        outputs = self.linear(x)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "128964ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "model = LogisticRegression(8, 4)\n",
    "lossFunc = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "seed=42\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(datasets[\"train\"].shuffle(seed=seed), shuffle=True, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(datasets[\"valid\"].shuffle(seed=seed), batch_size=batch_size)\n",
    "\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "\n",
    "PATH = f'{root}/../out/meta_classifier2.pt'\n",
    "metrics = []\n",
    "best_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f9aea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15420/15420 [04:44<00:00, 54.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.611, val_loss: 2.155\n",
      "Epoch time: 5.147 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15420/15420 [04:46<00:00, 53.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.549, val_loss: 2.279\n",
      "Epoch time: 5.182 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15420/15420 [04:46<00:00, 53.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.544, val_loss: 1.583\n",
      "Epoch time: 5.182 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15420/15420 [04:46<00:00, 53.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.542, val_loss: 3.289\n",
      "Epoch time: 5.178 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15420/15420 [04:47<00:00, 53.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.541, val_loss: 1.497\n",
      "Epoch time: 5.190 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15420/15420 [04:48<00:00, 53.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.540, val_loss: 3.201\n",
      "Epoch time: 5.206 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15420/15420 [04:56<00:00, 52.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.540, val_loss: 2.484\n",
      "Epoch time: 5.344 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15420/15420 [04:50<00:00, 53.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.539, val_loss: 1.552\n",
      "Epoch time: 5.235 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15420/15420 [04:47<00:00, 53.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.539, val_loss: 2.425\n",
      "Epoch time: 5.195 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15420/15420 [04:47<00:00, 53.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.539, val_loss: 1.546\n",
      "Epoch time: 5.191 minutes\n",
      "10/10 done\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "def get_target(like, labels):\n",
    "    target = torch.zeros_like(like).to(device)\n",
    "\n",
    "    for i, l in enumerate(labels):\n",
    "        target[i][l] = 1\n",
    "        \n",
    "    return target\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_acc = load_metric('accuracy')\n",
    "    train_f1 = load_metric('f1')\n",
    "    val_acc = load_metric('accuracy')\n",
    "    val_f1 = load_metric('f1')\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_dataloader):\n",
    "        inputs = batch['inputs'][0].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        target = get_target(outputs, labels)\n",
    "\n",
    "        loss = lossFunc(outputs, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        train_acc.add_batch(predictions=preds, references=labels)\n",
    "        train_f1.add_batch(predictions=preds, references=labels)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for batch in eval_dataloader:\n",
    "        inputs = batch['inputs'][0].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = lossFunc(outputs, target)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        val_acc.add_batch(predictions=preds, references=labels)\n",
    "        val_f1.add_batch(predictions=preds, references=labels)\n",
    "\n",
    "      train_acc = train_acc.compute()\n",
    "      train_f1 = train_f1.compute(average='macro')\n",
    "      val_acc = val_acc.compute()\n",
    "      val_f1 = val_f1.compute(average='macro')\n",
    "\n",
    "      vpoint = {\n",
    "          'train_acc': train_acc,\n",
    "          'train_f1': train_f1,\n",
    "          'train_loss': train_loss / len(train_dataloader),\n",
    "          'val_acc': val_acc,\n",
    "          'val_f1': val_f1,\n",
    "          'val_loss': val_loss / len(eval_dataloader)\n",
    "      }\n",
    "\n",
    "      metrics.append(vpoint)\n",
    "\n",
    "      print(f'train_loss: {vpoint[\"train_loss\"]:.3f}, val_loss: {vpoint[\"val_loss\"]:.3f}')\n",
    "    \n",
    "    print(f\"Epoch time: {((time.time() - start_time) / 60):.3f} minutes\")\n",
    "    \n",
    "    curr_state = {\n",
    "        **vpoint,\n",
    "        'model_state_dict': model.state_dict()\n",
    "    }\n",
    "    \n",
    "    if not best_state or best_state['val_loss'] > curr_state['val_loss']:\n",
    "        best_state = curr_state\n",
    "      \n",
    "    torch.save({\n",
    "      'epoch': epoch,\n",
    "      'model_state_dict': model.state_dict(),\n",
    "      'optimizer_state_dict': optimizer.state_dict(),\n",
    "      'best_state': best_state,\n",
    "      'metrics': metrics,\n",
    "    }, PATH)\n",
    "\n",
    "print(f'{epochs}/{epochs} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579254df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
