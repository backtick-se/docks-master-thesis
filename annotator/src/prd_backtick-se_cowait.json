[{"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 350, "title": "add node selector field to task definition", "labels": [{"id": 1562464622, "node_id": "MDU6TGFiZWwxNTYyNDY0NjIy", "url": "https://api.github.com/repos/backtick-se/cowait/labels/feature", "name": "feature", "color": "7057ff", "default": false, "description": "New feature"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/350", "html_url": "https://github.com/backtick-se/cowait/pull/350", "diff_url": "https://github.com/backtick-se/cowait/pull/350.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/350.patch", "merged_at": "2022-03-11T13:12:36Z"}, "body": null, "commits": [{"sha": "9ea67fb6cfe6e16f7db704a4053346c7145b470c", "html_url": "https://github.com/backtick-se/cowait/commit/9ea67fb6cfe6e16f7db704a4053346c7145b470c", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2022-03-11T13:01:26Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2022-03-11T13:01:26Z"}, "message": "add node selector field to task definition", "tree": {"sha": "63021703f910ad842c054af7f8ab063344106932", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/63021703f910ad842c054af7f8ab063344106932"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/9ea67fb6cfe6e16f7db704a4053346c7145b470c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "2e7441b13f411ecfbd04c7254312dd249092e140", "filename": "cowait/engine/kubernetes/kubernetes.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/backtick-se/cowait/blob/9ea67fb6cfe6e16f7db704a4053346c7145b470c/cowait/engine/kubernetes/kubernetes.py", "raw_url": "https://github.com/backtick-se/cowait/raw/9ea67fb6cfe6e16f7db704a4053346c7145b470c/cowait/engine/kubernetes/kubernetes.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/kubernetes/kubernetes.py?ref=9ea67fb6cfe6e16f7db704a4053346c7145b470c", "patch": "@@ -102,6 +102,7 @@ def spawn(self, taskdef: TaskDefinition, deploy: bool = False) -> KubernetesTask\n                         restart_policy='Always' if deploy else 'Never',\n                         image_pull_secrets=self.get_pull_secrets(),\n                         volumes=volumes,\n+                        node_selector=taskdef.nodes,\n \n                         containers=[container],\n                         service_account_name=self.service_account,"}, {"sha": "5c5baf3c61e88230d0b5a7cfd51d256723afb8e2", "filename": "cowait/tasks/definition.py", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/9ea67fb6cfe6e16f7db704a4053346c7145b470c/cowait/tasks/definition.py", "raw_url": "https://github.com/backtick-se/cowait/raw/9ea67fb6cfe6e16f7db704a4053346c7145b470c/cowait/tasks/definition.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/tasks/definition.py?ref=9ea67fb6cfe6e16f7db704a4053346c7145b470c", "patch": "@@ -47,6 +47,8 @@ class TaskDefinition(object):\n         cpu_limit (str): CPU limit\n         memory (str): Memory request\n         memory_limit (str): Memory limit\n+        affinity (str): Node affinity\n+        nodes (dict): Node selector labels\n         owner (str): Owner name\n         created_at (DateTime): Creation date\n     \"\"\"\n@@ -69,6 +71,7 @@ def __init__(\n         memory:       str = None,\n         memory_limit: str = None,\n         affinity:     str = None,\n+        nodes:        dict = {},\n         owner:        str = '',\n         created_at:   datetime = None,\n     ):\n@@ -109,6 +112,7 @@ def __init__(\n         self.owner = owner\n         self.volumes = volumes\n         self.affinity = affinity\n+        self.nodes = nodes\n \n         if created_at is None:\n             self.created_at = datetime.now(timezone.utc)\n@@ -137,6 +141,7 @@ def serialize(self) -> dict:\n             'memory': self.memory,\n             'memory_limit': self.memory_limit,\n             'affinity': self.affinity,\n+            'nodes': self.nodes,\n             'owner': self.owner,\n             'created_at': self.created_at.isoformat(),\n             'volumes': self.volumes,\n@@ -161,6 +166,7 @@ def deserialize(taskdef: dict) -> TaskDefinition:\n             memory=taskdef.get('memory', None),\n             memory_limit=taskdef.get('memory_limit', None),\n             affinity=taskdef.get('affinity', None),\n+            nodes=taskdef.get('nodes', {}),\n             owner=taskdef.get('owner', None),\n             created_at=datetime.fromisoformat(taskdef.get(\n                 'created_at', datetime.now().isoformat())),"}, {"sha": "ee3f68055b20ad86d8f8ea7bf0318216a346fb1e", "filename": "test/tasks/test_definition.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/backtick-se/cowait/blob/9ea67fb6cfe6e16f7db704a4053346c7145b470c/test/tasks/test_definition.py", "raw_url": "https://github.com/backtick-se/cowait/raw/9ea67fb6cfe6e16f7db704a4053346c7145b470c/test/tasks/test_definition.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/test/tasks/test_definition.py?ref=9ea67fb6cfe6e16f7db704a4053346c7145b470c", "patch": "@@ -20,6 +20,7 @@ def test_taskdef_serialization():\n         'memory':       '128m',\n         'memory_limit': '256m',\n         'affinity':     'stack',\n+        'nodes':        {'pool': 'omega'},\n         'owner':        'santa',\n     }\n "}], "stats": {"total": 8, "additions": 8, "deletions": 0}}, {"sha": "fa16a4207b9fbd371d0096c45ecbdfcb792c81e9", "html_url": "https://github.com/backtick-se/cowait/commit/fa16a4207b9fbd371d0096c45ecbdfcb792c81e9", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2022-03-11T13:01:47Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2022-03-11T13:01:47Z"}, "message": "remove dict literals from TaskDefinition defaults", "tree": {"sha": "eb7c95b3022c6f3070c25ac2fbb6237dd1a43e20", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/eb7c95b3022c6f3070c25ac2fbb6237dd1a43e20"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/fa16a4207b9fbd371d0096c45ecbdfcb792c81e9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "ed8ffdb4d92b3c24eb3f64a79d7ae56201395a81", "filename": "cowait/tasks/definition.py", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "blob_url": "https://github.com/backtick-se/cowait/blob/fa16a4207b9fbd371d0096c45ecbdfcb792c81e9/cowait/tasks/definition.py", "raw_url": "https://github.com/backtick-se/cowait/raw/fa16a4207b9fbd371d0096c45ecbdfcb792c81e9/cowait/tasks/definition.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/tasks/definition.py?ref=fa16a4207b9fbd371d0096c45ecbdfcb792c81e9", "patch": "@@ -60,19 +60,19 @@ def __init__(\n         id:           str = None,\n         upstream:     str = None,\n         parent:       str = None,\n-        inputs:       dict = {},\n-        meta:         dict = {},\n-        env:          dict = {},\n-        ports:        dict = {},\n-        routes:       dict = {},\n-        volumes:      dict = {},\n+        inputs:       dict = None,\n+        meta:         dict = None,\n+        env:          dict = None,\n+        ports:        dict = None,\n+        routes:       dict = None,\n+        volumes:      dict = None,\n         cpu:          str = None,\n         cpu_limit:    str = None,\n         memory:       str = None,\n         memory_limit: str = None,\n         affinity:     str = None,\n-        nodes:        dict = {},\n-        owner:        str = '',\n+        nodes:        dict = None,\n+        owner:        str = None,\n         created_at:   datetime = None,\n     ):\n         \"\"\"\n@@ -100,19 +100,19 @@ def __init__(\n         self.image = image\n         self.parent = parent\n         self.upstream = upstream\n-        self.inputs = inputs\n-        self.meta = meta\n-        self.env = env\n-        self.ports = ports\n-        self.routes = routes\n+        self.inputs = inputs or {}\n+        self.meta = meta or {}\n+        self.env = env or {}\n+        self.ports = ports or {}\n+        self.routes = routes or {}\n         self.cpu = cpu\n         self.memory = memory\n-        self.cpu_limit = cpu_limit if cpu_limit else cpu\n-        self.memory_limit = memory_limit if memory_limit else memory\n-        self.owner = owner\n-        self.volumes = volumes\n+        self.cpu_limit = cpu_limit or cpu\n+        self.memory_limit = memory_limit or memory\n+        self.owner = owner or ''\n+        self.volumes = volumes or {}\n         self.affinity = affinity\n-        self.nodes = nodes\n+        self.nodes = nodes or {}\n \n         if created_at is None:\n             self.created_at = datetime.now(timezone.utc)"}], "stats": {"total": 36, "additions": 18, "deletions": 18}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 349, "title": "Update server.py", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/349", "html_url": "https://github.com/backtick-se/cowait/pull/349", "diff_url": "https://github.com/backtick-se/cowait/pull/349.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/349.patch", "merged_at": "2022-03-09T14:02:37Z"}, "body": "Fix bug when emitting SocketError", "commits": [{"sha": "69968800347c93a0081007fa1a33036f4a647f8f", "html_url": "https://github.com/backtick-se/cowait/commit/69968800347c93a0081007fa1a33036f4a647f8f", "commit": {"author": {"name": "Emil W\u00e5reus", "email": "emil.wareus47@gmail.com", "date": "2022-03-05T08:32:13Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2022-03-05T08:32:13Z"}, "message": "Update server.py\n\nFix bug when emitting SocketError", "tree": {"sha": "45913b82f6c6e63c63d77c6b2faad741817078a7", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/45913b82f6c6e63c63d77c6b2faad741817078a7"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/69968800347c93a0081007fa1a33036f4a647f8f", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJiIyANCRBK7hj4Ov3rIwAAQIsIAAMmtSf8SH6KOUnyV9WxRj5q\ntYuHu396uSXoY3tA0tggKcm4+yAu5ecXdI9/oNtkXqJfT73NYa5JkiBk0qP+1bVP\ncGDXyVX/tehDvcaWL5mjjSAOkCavH+8/+KVlI9E5Wgf97w3sYNqefVRHGWxfuxUp\nyRIGOe9s0VAaesFTwqQbM4B2ukh+f59cKzwLgsE/v7d+ItpnLU2s4ISgMQEzBlPc\nRQgcVfvp5FH51qGEe/v3GR69RaJrupNbL2vtLGuep2iUnS6q+QVsiDJTbWc79eIi\nPbCUEeY0OhIICLUkWGb7rx/ty+AJywReujoPG3NX9AUvKaQkgG/asUnlD+aI9do=\n=WqJB\n-----END PGP SIGNATURE-----\n", "payload": "tree 45913b82f6c6e63c63d77c6b2faad741817078a7\nparent b6c25caf5a8b507f56853f304396bce2cad0f32a\nauthor Emil W\u00e5reus <emil.wareus47@gmail.com> 1646469133 +0100\ncommitter GitHub <noreply@github.com> 1646469133 +0100\n\nUpdate server.py\n\nFix bug when emitting SocketError"}}, "files": [{"sha": "1c953fe00998ce5fcbfb3ccbb533fae990a528e4", "filename": "cowait/network/server.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/69968800347c93a0081007fa1a33036f4a647f8f/cowait/network/server.py", "raw_url": "https://github.com/backtick-se/cowait/raw/69968800347c93a0081007fa1a33036f4a647f8f/cowait/network/server.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/network/server.py?ref=69968800347c93a0081007fa1a33036f4a647f8f", "patch": "@@ -88,7 +88,7 @@ async def handle_client(self, request):\n         except CancelledError:\n             await self.emit(type=ON_CLOSE, conn=conn)\n \n-        except SocketError:\n+        except SocketError as e:\n             await self.emit(type=ON_CLOSE, conn=conn, error=str(e))\n \n         finally:"}], "stats": {"total": 2, "additions": 1, "deletions": 1}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 348, "title": "Fix build on M1 processors", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/348", "html_url": "https://github.com/backtick-se/cowait/pull/348", "diff_url": "https://github.com/backtick-se/cowait/pull/348.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/348.patch", "merged_at": "2022-03-09T14:03:33Z"}, "body": "This PR solves #347 using the third alternative mentioned in the issue", "commits": [{"sha": "f2a04d5dc571c5fb45ef415a3d1287d8198ad8a6", "html_url": "https://github.com/backtick-se/cowait/commit/f2a04d5dc571c5fb45ef415a3d1287d8198ad8a6", "commit": {"author": {"name": "Martin Jakobsson", "email": "martin@backtick.se", "date": "2022-03-04T12:29:50Z"}, "committer": {"name": "Martin Jakobsson", "email": "martin@backtick.se", "date": "2022-03-04T12:29:50Z"}, "message": "fix build on M1 processors", "tree": {"sha": "8ca0af06b619ba7910433fe7390277b128d1ec42", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/8ca0af06b619ba7910433fe7390277b128d1ec42"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/f2a04d5dc571c5fb45ef415a3d1287d8198ad8a6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "83a083b2b9c206b0846b0ddd6d1bee4393904843", "filename": "build.sh", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/f2a04d5dc571c5fb45ef415a3d1287d8198ad8a6/build.sh", "raw_url": "https://github.com/backtick-se/cowait/raw/f2a04d5dc571c5fb45ef415a3d1287d8198ad8a6/build.sh", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/build.sh?ref=f2a04d5dc571c5fb45ef415a3d1287d8198ad8a6", "patch": "@@ -32,7 +32,7 @@ fi\n build() {\n     # build image\n     echo \"Building $1:$version\"\n-    docker build --tag \"$1:latest\" $2\n+    docker build --platform linux/amd64 --tag \"$1:latest\" $2\n \n     # tag versions\n     if [[ $args == *--tag* ]] || [[ $args == *--push* ]]; then"}], "stats": {"total": 2, "additions": 1, "deletions": 1}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 346, "title": "Bump url-parse from 1.5.7 to 1.5.10 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/346", "html_url": "https://github.com/backtick-se/cowait/pull/346", "diff_url": "https://github.com/backtick-se/cowait/pull/346.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/346.patch", "merged_at": "2022-03-09T14:03:07Z"}, "body": "Bumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.7 to 1.5.10.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/8cd4c6c6435c1ea32243ec20c9cfe535251ec524\"><code>8cd4c6c</code></a> 1.5.10</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/ce7a01f2e10738b17812f57c7b6b5de4ea4c0298\"><code>ce7a01f</code></a> [fix] Improve handling of empty port</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/00714900ea1e8ba0a1f87b9f8399001e47f060ec\"><code>0071490</code></a> [doc] Update JSDoc comment</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/a7044e3e8bb2308ac0f74264d01951aeaca0d66f\"><code>a7044e3</code></a> [minor] Use more descriptive variable name</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/d547792414a414b2f341a805141beafee728addf\"><code>d547792</code></a> [security] Add credits for CVE-2022-0691</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/ad23357ad5fd9a6b011d049466e9ecff723e52b8\"><code>ad23357</code></a> 1.5.9</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/0e3fb542d60ddbf6933f22eb9b1e06e25eaa5b63\"><code>0e3fb54</code></a> [fix] Strip all control characters from the beginning of the URL</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/61864a8eccff714a45d23db85a814e3c6ee0baba\"><code>61864a8</code></a> [security] Add credits for CVE-2022-0686</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/bb0104d6439cf7c2662afbd9411e0772a9639664\"><code>bb0104d</code></a> 1.5.8</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/d5c64791ef496ca5459ae7f2176a31ea53b127e5\"><code>d5c6479</code></a> [fix] Handle the case where the port is specified but empty</li>\n<li>Additional commits viewable in <a href=\"https://github.com/unshiftio/url-parse/compare/1.5.7...1.5.10\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=url-parse&package-manager=npm_and_yarn&previous-version=1.5.7&new-version=1.5.10)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "33ce3f5b6d91565d90d1114b4c7e1641b05959ed", "html_url": "https://github.com/backtick-se/cowait/commit/33ce3f5b6d91565d90d1114b4c7e1641b05959ed", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2022-02-28T04:40:40Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2022-02-28T04:40:40Z"}, "message": "Bump url-parse from 1.5.7 to 1.5.10 in /cloud\n\nBumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.7 to 1.5.10.\n- [Release notes](https://github.com/unshiftio/url-parse/releases)\n- [Commits](https://github.com/unshiftio/url-parse/compare/1.5.7...1.5.10)\n\n---\nupdated-dependencies:\n- dependency-name: url-parse\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "e6bea9e79f167b0d9ed234578f28185328567cc7", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/e6bea9e79f167b0d9ed234578f28185328567cc7"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/33ce3f5b6d91565d90d1114b4c7e1641b05959ed", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJiHFJICRBK7hj4Ov3rIwAAb0UIADfsMkeAyrZMwhbm+4hI6Kxk\nkv3Ss8K5qcc9ZYfUUP3xRJ/gGa6LxfleuRrXxsM5e4lZ/KiJviY6jbTh7JxYokGA\n5VOFs5mKRMxcVJsibHgy3nkBKafqaK35dUHwuRlQb6rU6UpE4LyXxaNkXC7j+9Cs\nOMCXAExttoGJ2QdW9Yif/Z5CjRiNHGg4BqS3COMARV67MxJ8RJQnVsTR9UEmg9y8\nJExnZXyaqS+RzEFOHkISKV+slfEnURMExa0r7sS2Ks7HtHTkOpbR3wzkz+BpBHfT\nhCDJWwTsLI3NtEE9/KlU+hypLZNs5PwCMhcpkZrf0AIzCP7f5q+nFhfgGT/XHho=\n=HwtW\n-----END PGP SIGNATURE-----\n", "payload": "tree e6bea9e79f167b0d9ed234578f28185328567cc7\nparent b6c25caf5a8b507f56853f304396bce2cad0f32a\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1646023240 +0000\ncommitter GitHub <noreply@github.com> 1646023240 +0000\n\nBump url-parse from 1.5.7 to 1.5.10 in /cloud\n\nBumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.7 to 1.5.10.\n- [Release notes](https://github.com/unshiftio/url-parse/releases)\n- [Commits](https://github.com/unshiftio/url-parse/compare/1.5.7...1.5.10)\n\n---\nupdated-dependencies:\n- dependency-name: url-parse\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "838a89890df0f0ae8622a2c244d5b8e66cdc5582", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/33ce3f5b6d91565d90d1114b4c7e1641b05959ed/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/33ce3f5b6d91565d90d1114b4c7e1641b05959ed/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=33ce3f5b6d91565d90d1114b4c7e1641b05959ed", "patch": "@@ -10907,9 +10907,9 @@ url-loader@2.1.0:\n     schema-utils \"^2.0.0\"\n \n url-parse@^1.4.3:\n-  version \"1.5.7\"\n-  resolved \"https://registry.yarnpkg.com/url-parse/-/url-parse-1.5.7.tgz#00780f60dbdae90181f51ed85fb24109422c932a\"\n-  integrity sha512-HxWkieX+STA38EDk7CE9MEryFeHCKzgagxlGvsdS7WBImq9Mk+PGwiT56w82WI3aicwJA8REp42Cxo98c8FZMA==\n+  version \"1.5.10\"\n+  resolved \"https://registry.yarnpkg.com/url-parse/-/url-parse-1.5.10.tgz#9d3c2f736c1d75dd3bd2be507dcc111f1e2ea9c1\"\n+  integrity sha512-WypcfiRhfeUP9vvF0j6rw0J3hrWrw6iZv3+22h6iRMJ/8z1Tj6XfLP4DsUix5MhMPnXpiHDoKyoZ/bdCkwBCiQ==\n   dependencies:\n     querystringify \"^2.1.1\"\n     requires-port \"^1.0.0\""}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 345, "title": "add simple test for kubernetes provider", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/345", "html_url": "https://github.com/backtick-se/cowait/pull/345", "diff_url": "https://github.com/backtick-se/cowait/pull/345.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/345.patch", "merged_at": "2022-02-26T12:28:48Z"}, "body": "This test fails if the `KubernetesProvider` cannot be instantiated. Such a problem has already happened, and is being addressed by #344.", "commits": [{"sha": "2502efd127467ad9e0b8e5939594c1062224e217", "html_url": "https://github.com/backtick-se/cowait/commit/2502efd127467ad9e0b8e5939594c1062224e217", "commit": {"author": {"name": "Martin Jakobsson", "email": "martin@backtick.se", "date": "2022-02-25T17:59:06Z"}, "committer": {"name": "Martin Jakobsson", "email": "martin@backtick.se", "date": "2022-02-25T17:59:06Z"}, "message": "add simple test for kubernetes provider", "tree": {"sha": "eecd3cef48de935fecd4af90780cbbb130d5ae94", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/eecd3cef48de935fecd4af90780cbbb130d5ae94"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/2502efd127467ad9e0b8e5939594c1062224e217", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "ef5c314be27104144a1e0cff6b7ea251196654e0", "filename": "cowait/engine/kubernetes/kubernetes.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/backtick-se/cowait/blob/2502efd127467ad9e0b8e5939594c1062224e217/cowait/engine/kubernetes/kubernetes.py", "raw_url": "https://github.com/backtick-se/cowait/raw/2502efd127467ad9e0b8e5939594c1062224e217/cowait/engine/kubernetes/kubernetes.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/kubernetes/kubernetes.py?ref=2502efd127467ad9e0b8e5939594c1062224e217", "patch": "@@ -29,7 +29,8 @@ def __init__(self, args={}):\n             config.load_incluster_config()\n         except kubernetes.config.ConfigException:\n             # load local config\n-            config.load_kube_config(context=self.args.get('context', None))\n+            config.load_kube_config(config_file=self.args.get('kube_config_file', None),\n+                                    context=self.args.get('context', None))\n \n         self.core = client.CoreV1Api()\n         self.ext = client.ExtensionsV1beta1Api()"}, {"sha": "be7d7d9698babc31dcd6dd535ffda40169a42e7f", "filename": "test/engine/kubernetes/test_kubernetes.py", "status": "modified", "additions": 43, "deletions": 0, "changes": 43, "blob_url": "https://github.com/backtick-se/cowait/blob/2502efd127467ad9e0b8e5939594c1062224e217/test/engine/kubernetes/test_kubernetes.py", "raw_url": "https://github.com/backtick-se/cowait/raw/2502efd127467ad9e0b8e5939594c1062224e217/test/engine/kubernetes/test_kubernetes.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/test/engine/kubernetes/test_kubernetes.py?ref=2502efd127467ad9e0b8e5939594c1062224e217", "patch": "@@ -1,6 +1,34 @@\n import pytest\n+\n+import tempfile\n from kubernetes import client\n+\n from cowait.engine.kubernetes.volumes import create_volumes\n+from cowait.engine.kubernetes import KubernetesProvider\n+\n+EXAMPLE_KUBE_CONFIG = '''\n+current-context: test-context\n+apiVersion: v1\n+kind: Config\n+\n+clusters:\n+- cluster:\n+    api-version: v1\n+    server: http://localhost:8080\n+  name: test-cluster\n+\n+users:\n+- name: test-user\n+  user:\n+    token: test-token\n+\n+contexts:\n+- context:\n+    cluster: test-cluster\n+    namespace: test-ns\n+    user: test-user\n+  name: test-context\n+'''\n \n \n @pytest.mark.kubernetes\n@@ -30,3 +58,18 @@ def test_create_volumes():\n     assert mount.name == 'volume1'\n     assert mount.mount_path == target_path\n     assert mount.read_only\n+\n+\n+@pytest.mark.kubernetes\n+def test_provider__custom_kube_config():\n+    with tempfile.NamedTemporaryFile(mode='w+t') as kube_config_file:\n+        kube_config_file.write(EXAMPLE_KUBE_CONFIG)\n+        kube_config_file.seek(0)\n+\n+        provider = KubernetesProvider(args={\n+            'context': 'test-context',\n+            'namespace': 'the-namespace',\n+            'kube_config_file': kube_config_file.name\n+        })\n+\n+        assert provider.namespace == 'the-namespace'  # i.e. NOT 'test-ns' specified in the context"}], "stats": {"total": 46, "additions": 45, "deletions": 1}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 343, "title": "Bump url-parse from 1.5.1 to 1.5.7 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/343", "html_url": "https://github.com/backtick-se/cowait/pull/343", "diff_url": "https://github.com/backtick-se/cowait/pull/343.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/343.patch", "merged_at": "2022-02-21T09:51:14Z"}, "body": "Bumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.1 to 1.5.7.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/8b3f5f2c88a4cfc2880f2319c307994cb25bb10a\"><code>8b3f5f2</code></a> 1.5.7</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/ef45a1355375a8244063793a19059b4f62fc8788\"><code>ef45a13</code></a> [fix] Readd the empty userinfo to <code>url.href</code> (<a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/226\">#226</a>)</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/88df2346855f70cec9713b362ca32a4691dc271a\"><code>88df234</code></a> [doc] Add soft deprecation notice</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/78e9f2f41285d83e7d91706be5bd439656fe3bc3\"><code>78e9f2f</code></a> [security] Fix nits</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/e6fa43422c52f34c73146552ec9916125dc59525\"><code>e6fa434</code></a> [security] Add credits for incorrect handling of userinfo vulnerability</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/4c9fa234c01dca52698666378360ad2fdfb05470\"><code>4c9fa23</code></a> 1.5.6</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/7b0b8a6671f806458e88b1f44feb0fdd742cdf06\"><code>7b0b8a6</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/223\">#223</a> from unshiftio/fix/at-sign-handling-in-userinfo</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/e4a5807d95b971577e4d888f5b99d64a40851386\"><code>e4a5807</code></a> 1.5.5</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/193b44baf3d203560735e05eedc99d8244c9e16c\"><code>193b44b</code></a> [minor] Simplify whitespace regex</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/319851bf1c294796fc73e29ff31b14d9084e4a0d\"><code>319851b</code></a> [fix] Remove CR, HT, and LF</li>\n<li>Additional commits viewable in <a href=\"https://github.com/unshiftio/url-parse/compare/1.5.1...1.5.7\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=url-parse&package-manager=npm_and_yarn&previous-version=1.5.1&new-version=1.5.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "55ca974ac11fb0d8936243ceadd382933c323212", "html_url": "https://github.com/backtick-se/cowait/commit/55ca974ac11fb0d8936243ceadd382933c323212", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2022-02-19T03:24:01Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2022-02-19T03:24:01Z"}, "message": "Bump url-parse from 1.5.1 to 1.5.7 in /cloud\n\nBumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.1 to 1.5.7.\n- [Release notes](https://github.com/unshiftio/url-parse/releases)\n- [Commits](https://github.com/unshiftio/url-parse/compare/1.5.1...1.5.7)\n\n---\nupdated-dependencies:\n- dependency-name: url-parse\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "d2ae8df7a4305430c2fe61adceaa033a1e0133d9", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/d2ae8df7a4305430c2fe61adceaa033a1e0133d9"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/55ca974ac11fb0d8936243ceadd382933c323212", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJiEGLRCRBK7hj4Ov3rIwAARHkIAGNEePl2+yIJp6AYVAZzM5c0\n0MJm1xG4jnk7jpRLcSmlZZJhA/GOzrhkFApFkez9JeiIhebdwuJpsD75EkE/cXGa\nzs7nFgEWpPHXGJngtoA2ntHNybWDiC/+N+VUGyDUkUQFvt6Al3wD09P2qYogrgi1\ntxJLOEGFiIN30UmGL93HoaH0ZBYvr6XK/L+1XB4Je7Xk72gEWA16BPGV43szVLqr\nloHUN214BzHLdGcltI9TcTcFMX/WYtqkyO2Vt3ZfCZ595VosPc6r+qJlnWhKabNm\n+goZUxGYVo5jkV2KzHhtc3lZIsd2BJtpWm4azdXZH9Py7LRlDPqEqhMslRTZIF0=\n=+OVM\n-----END PGP SIGNATURE-----\n", "payload": "tree d2ae8df7a4305430c2fe61adceaa033a1e0133d9\nparent 760ddb3ded1b3995bc68f4b74cf28af0c094481f\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1645241041 +0000\ncommitter GitHub <noreply@github.com> 1645241041 +0000\n\nBump url-parse from 1.5.1 to 1.5.7 in /cloud\n\nBumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.1 to 1.5.7.\n- [Release notes](https://github.com/unshiftio/url-parse/releases)\n- [Commits](https://github.com/unshiftio/url-parse/compare/1.5.1...1.5.7)\n\n---\nupdated-dependencies:\n- dependency-name: url-parse\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "9b41462c89e159026fc6e8cec932424e93ed4674", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/55ca974ac11fb0d8936243ceadd382933c323212/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/55ca974ac11fb0d8936243ceadd382933c323212/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=55ca974ac11fb0d8936243ceadd382933c323212", "patch": "@@ -10907,9 +10907,9 @@ url-loader@2.1.0:\n     schema-utils \"^2.0.0\"\n \n url-parse@^1.4.3:\n-  version \"1.5.1\"\n-  resolved \"https://registry.yarnpkg.com/url-parse/-/url-parse-1.5.1.tgz#d5fa9890af8a5e1f274a2c98376510f6425f6e3b\"\n-  integrity sha512-HOfCOUJt7iSYzEx/UqgtwKRMC6EU91NFhsCHMv9oM03VJcVo2Qrp8T8kI9D7amFf1cu+/3CEhgb3rF9zL7k85Q==\n+  version \"1.5.7\"\n+  resolved \"https://registry.yarnpkg.com/url-parse/-/url-parse-1.5.7.tgz#00780f60dbdae90181f51ed85fb24109422c932a\"\n+  integrity sha512-HxWkieX+STA38EDk7CE9MEryFeHCKzgagxlGvsdS7WBImq9Mk+PGwiT56w82WI3aicwJA8REp42Cxo98c8FZMA==\n   dependencies:\n     querystringify \"^2.1.1\"\n     requires-port \"^1.0.0\""}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 342, "title": "Bump follow-redirects from 1.13.0 to 1.14.8 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/342", "html_url": "https://github.com/backtick-se/cowait/pull/342", "diff_url": "https://github.com/backtick-se/cowait/pull/342.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/342.patch", "merged_at": "2022-02-21T09:51:23Z"}, "body": "Bumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.13.0 to 1.14.8.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/3d81dc3237b4ffe8b722bb3d1c70a7866657166e\"><code>3d81dc3</code></a> Release version 1.14.8 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/62e546a99c07c3ee5e4e0718c84a6ca127c5c445\"><code>62e546a</code></a> Drop confidential headers across schemes.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/2ede36d7c60d3acdcd324dcd99a9dbd52e4fb3a6\"><code>2ede36d</code></a> Release version 1.14.7 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/8b347cbcef7c7b72a6e9be20f5710c17d6163c22\"><code>8b347cb</code></a> Drop Cookie header across domains.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/6f5029ae1a0fdab4dc25f6379a5ee303c2319070\"><code>6f5029a</code></a> Release version 1.14.6 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/af706bee57de954414c0bde0a9f33e62beea3e52\"><code>af706be</code></a> Ignore null headers.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/d01ab7a5c5df3617c7a40a03de7af6427fdfac55\"><code>d01ab7a</code></a> Release version 1.14.5 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/40052ea8aa13559becee5795715c1d45b1f0eb76\"><code>40052ea</code></a> Make compatible with Node 17.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/86f7572f9365dadc39f85916259b58973819617f\"><code>86f7572</code></a> Fix: clear internal timer on request abort to avoid leakage</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/2e1eaf0218c5315a2ab27f53964d0535d4dafb51\"><code>2e1eaf0</code></a> Keep Authorization header on subdomain redirects.</li>\n<li>Additional commits viewable in <a href=\"https://github.com/follow-redirects/follow-redirects/compare/v1.13.0...v1.14.8\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=follow-redirects&package-manager=npm_and_yarn&previous-version=1.13.0&new-version=1.14.8)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "50a35ba06cda3019424fdbf4e4b3479907305c22", "html_url": "https://github.com/backtick-se/cowait/commit/50a35ba06cda3019424fdbf4e4b3479907305c22", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2022-02-13T15:31:05Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2022-02-13T15:31:05Z"}, "message": "Bump follow-redirects from 1.13.0 to 1.14.8 in /cloud\n\nBumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.13.0 to 1.14.8.\n- [Release notes](https://github.com/follow-redirects/follow-redirects/releases)\n- [Commits](https://github.com/follow-redirects/follow-redirects/compare/v1.13.0...v1.14.8)\n\n---\nupdated-dependencies:\n- dependency-name: follow-redirects\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "961448b5a9ba188c3c4b8cefd65795f9b8978168", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/961448b5a9ba188c3c4b8cefd65795f9b8978168"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/50a35ba06cda3019424fdbf4e4b3479907305c22", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJiCSQ5CRBK7hj4Ov3rIwAA/xMIAImd7EMQY24JxLbBZ3+dFalv\n0sMulbuVBBV4wqHdMBu3NrTKyvjd8eyLTh4IY0+eiYSoaT7yIJYwxRQALig+OOOp\nhDp4XQeRuCM9wZKBcPFSUG+g48/L/buyzRpQuPJ9B5WNUaDgUsKyhW2TeBMgtuz8\nvPdt7kkRhT9CprQqHklm75XEfdclRilD8FS3T54SOQ7eHcBj2Hk1ohcWnoyFYggG\n4uFUVoyVBZojz5DTuOgqxPeXTuk4p7ecpJM+nYLn2f4g5qrNIcdhwZyPl10xRMLy\nowETP7EryubQwZPuxSEvpy7BqOS0rLkxSD9Nn4v6hLJlhY4YVZKQ8x6tB1SdjOg=\n=hOTq\n-----END PGP SIGNATURE-----\n", "payload": "tree 961448b5a9ba188c3c4b8cefd65795f9b8978168\nparent 760ddb3ded1b3995bc68f4b74cf28af0c094481f\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1644766265 +0000\ncommitter GitHub <noreply@github.com> 1644766265 +0000\n\nBump follow-redirects from 1.13.0 to 1.14.8 in /cloud\n\nBumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.13.0 to 1.14.8.\n- [Release notes](https://github.com/follow-redirects/follow-redirects/releases)\n- [Commits](https://github.com/follow-redirects/follow-redirects/compare/v1.13.0...v1.14.8)\n\n---\nupdated-dependencies:\n- dependency-name: follow-redirects\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "83f6185580cc7dfb727166118ea18429da7a3dd6", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/50a35ba06cda3019424fdbf4e4b3479907305c22/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/50a35ba06cda3019424fdbf4e4b3479907305c22/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=50a35ba06cda3019424fdbf4e4b3479907305c22", "patch": "@@ -4800,9 +4800,9 @@ flush-write-stream@^1.0.0:\n     readable-stream \"^2.3.6\"\n \n follow-redirects@^1.0.0:\n-  version \"1.13.0\"\n-  resolved \"https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.13.0.tgz#b42e8d93a2a7eea5ed88633676d6597bc8e384db\"\n-  integrity sha512-aq6gF1BEKje4a9i9+5jimNFIpq4Q1WiwBToeRK5NvZBd/TRsmW8BsJfOEGkr76TbOyPVD3OVDN910EcUNtRYEA==\n+  version \"1.14.8\"\n+  resolved \"https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.14.8.tgz#016996fb9a11a100566398b1c6839337d7bfa8fc\"\n+  integrity sha512-1x0S9UVJHsQprFcEC/qnNzBLcIxsjAV905f/UkQxbclCsoTWlacCNOpQa/anodLl2uaEKFhfWOvM2Qg77+15zA==\n \n for-in@^0.1.3:\n   version \"0.1.8\""}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 339, "title": "Bump follow-redirects from 1.13.0 to 1.14.7 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/339", "html_url": "https://github.com/backtick-se/cowait/pull/339", "diff_url": "https://github.com/backtick-se/cowait/pull/339.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/339.patch", "merged_at": null}, "body": "Bumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.13.0 to 1.14.7.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/2ede36d7c60d3acdcd324dcd99a9dbd52e4fb3a6\"><code>2ede36d</code></a> Release version 1.14.7 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/8b347cbcef7c7b72a6e9be20f5710c17d6163c22\"><code>8b347cb</code></a> Drop Cookie header across domains.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/6f5029ae1a0fdab4dc25f6379a5ee303c2319070\"><code>6f5029a</code></a> Release version 1.14.6 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/af706bee57de954414c0bde0a9f33e62beea3e52\"><code>af706be</code></a> Ignore null headers.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/d01ab7a5c5df3617c7a40a03de7af6427fdfac55\"><code>d01ab7a</code></a> Release version 1.14.5 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/40052ea8aa13559becee5795715c1d45b1f0eb76\"><code>40052ea</code></a> Make compatible with Node 17.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/86f7572f9365dadc39f85916259b58973819617f\"><code>86f7572</code></a> Fix: clear internal timer on request abort to avoid leakage</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/2e1eaf0218c5315a2ab27f53964d0535d4dafb51\"><code>2e1eaf0</code></a> Keep Authorization header on subdomain redirects.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/2ad9e82b6277ae2104f7770e9ff1186cc6da29d4\"><code>2ad9e82</code></a> Carry over Host header on relative redirects (<a href=\"https://github-redirect.dependabot.com/follow-redirects/follow-redirects/issues/172\">#172</a>)</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/77e2a581e1d1811674b7b74745a9c20a5b939488\"><code>77e2a58</code></a> Release version 1.14.4 of the npm package.</li>\n<li>Additional commits viewable in <a href=\"https://github.com/follow-redirects/follow-redirects/compare/v1.13.0...v1.14.7\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=follow-redirects&package-manager=npm_and_yarn&previous-version=1.13.0&new-version=1.14.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "bcc3d09203ec273b7d272541d919ac85e5cad5ba", "html_url": "https://github.com/backtick-se/cowait/commit/bcc3d09203ec273b7d272541d919ac85e5cad5ba", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2022-01-14T11:43:39Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2022-01-14T11:43:39Z"}, "message": "Bump follow-redirects from 1.13.0 to 1.14.7 in /cloud\n\nBumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.13.0 to 1.14.7.\n- [Release notes](https://github.com/follow-redirects/follow-redirects/releases)\n- [Commits](https://github.com/follow-redirects/follow-redirects/compare/v1.13.0...v1.14.7)\n\n---\nupdated-dependencies:\n- dependency-name: follow-redirects\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "4f401c3a39b2c1e238bc5a768a47691ebe42030b", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/4f401c3a39b2c1e238bc5a768a47691ebe42030b"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/bcc3d09203ec273b7d272541d919ac85e5cad5ba", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJh4WHrCRBK7hj4Ov3rIwAAKX8IABeS3wZ0Vcij/6SdPai6AHeU\nmQKwS1YdNgX/VRfZuHwTAEWzfqfEe1+wMwLMMtliYYI0/KT7MRx8xaZB5HJBo+ad\nikTrcc0Q6mWc6sEfdBd6iXc6qy/sYD4oPzkBWrwwpzem+YM7s9x1u72etHefIGA+\nSc0kN2UQsit8JW9xkEJYxfZIDDAzvSr3BIctuydPh8Yasalg7u8i+MHpH1B2C6n2\nnSOtqxiVxPP+HGrN8wg0QEo7wz6iaovmJ6aL2BaveyQEetb/gHlYjEQ7R6Y9sjSr\nBtzsyP54XImQ+3saFT0ku8rtexYzd4DSNn0b+9A5kMlVcio5ci+xb0/HliXB8l8=\n=M0Gs\n-----END PGP SIGNATURE-----\n", "payload": "tree 4f401c3a39b2c1e238bc5a768a47691ebe42030b\nparent 760ddb3ded1b3995bc68f4b74cf28af0c094481f\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1642160619 +0000\ncommitter GitHub <noreply@github.com> 1642160619 +0000\n\nBump follow-redirects from 1.13.0 to 1.14.7 in /cloud\n\nBumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.13.0 to 1.14.7.\n- [Release notes](https://github.com/follow-redirects/follow-redirects/releases)\n- [Commits](https://github.com/follow-redirects/follow-redirects/compare/v1.13.0...v1.14.7)\n\n---\nupdated-dependencies:\n- dependency-name: follow-redirects\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "3a4f83e0fc3ccab88dfb5ac696bd5999129f7c43", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/bcc3d09203ec273b7d272541d919ac85e5cad5ba/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/bcc3d09203ec273b7d272541d919ac85e5cad5ba/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=bcc3d09203ec273b7d272541d919ac85e5cad5ba", "patch": "@@ -4800,9 +4800,9 @@ flush-write-stream@^1.0.0:\n     readable-stream \"^2.3.6\"\n \n follow-redirects@^1.0.0:\n-  version \"1.13.0\"\n-  resolved \"https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.13.0.tgz#b42e8d93a2a7eea5ed88633676d6597bc8e384db\"\n-  integrity sha512-aq6gF1BEKje4a9i9+5jimNFIpq4Q1WiwBToeRK5NvZBd/TRsmW8BsJfOEGkr76TbOyPVD3OVDN910EcUNtRYEA==\n+  version \"1.14.7\"\n+  resolved \"https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.14.7.tgz#2004c02eb9436eee9a21446a6477debf17e81685\"\n+  integrity sha512-+hbxoLbFMbRKDwohX8GkTataGqO6Jb7jGwpAlwgy2bIz25XtRm7KEzJM76R1WiNT5SwZkX4Y75SwBolkpmE7iQ==\n \n for-in@^0.1.3:\n   version \"0.1.8\""}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 338, "title": "Bump tensorflow-cpu from 2.4.0 to 2.5.2 in /examples/10-imdb", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2588709704, "node_id": "MDU6TGFiZWwyNTg4NzA5NzA0", "url": "https://api.github.com/repos/backtick-se/cowait/labels/python", "name": "python", "color": "2b67c6", "default": false, "description": "Pull requests that update Python code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/338", "html_url": "https://github.com/backtick-se/cowait/pull/338", "diff_url": "https://github.com/backtick-se/cowait/pull/338.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/338.patch", "merged_at": null}, "body": "Bumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.2.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/releases\">tensorflow-cpu's releases</a>.</em></p>\n<blockquote>\n<h2>TensorFlow 2.5.2</h2>\n<h1>Release 2.5.2</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a code injection issue in <code>saved_model_cli</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41228\">CVE-2021-41228</a>)</li>\n<li>Fixes a vulnerability due to use of uninitialized value in Tensorflow (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41225\">CVE-2021-41225</a>)</li>\n<li>Fixes a heap OOB in <code>FusedBatchNorm</code> kernels (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41223\">CVE-2021-41223</a>)</li>\n<li>Fixes an arbitrary memory read in <code>ImmutableConst</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41227\">CVE-2021-41227</a>)</li>\n<li>Fixes a heap OOB in <code>SparseBinCount</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41226\">CVE-2021-41226</a>)</li>\n<li>Fixes a heap OOB in <code>SparseFillEmptyRows</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41224\">CVE-2021-41224</a>)</li>\n<li>Fixes a segfault due to negative splits in <code>SplitV</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41222\">CVE-2021-41222</a>)</li>\n<li>Fixes segfaults and vulnerabilities caused by accesses to invalid memory during shape inference in <code>Cudnn*</code> ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41221\">CVE-2021-41221</a>)</li>\n<li>Fixes a null pointer exception when <code>Exit</code> node is not preceded by <code>Enter</code> op (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41217\">CVE-2021-41217</a>)</li>\n<li>Fixes an integer division by 0 in <code>tf.raw_ops.AllToAll</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41218\">CVE-2021-41218</a>)</li>\n<li>Fixes an undefined behavior via <code>nullptr</code> reference binding in sparse matrix multiplication (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41219\">CVE-2021-41219</a>)</li>\n<li>Fixes a heap buffer overflow in <code>Transpose</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41216\">CVE-2021-41216</a>)</li>\n<li>Prevents deadlocks arising from mutually recursive <code>tf.function</code> objects (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41213\">CVE-2021-41213</a>)</li>\n<li>Fixes a null pointer exception in <code>DeserializeSparse</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41215\">CVE-2021-41215</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to <code>nullptr</code> in <code>tf.ragged.cross</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41214\">CVE-2021-41214</a>)</li>\n<li>Fixes a heap OOB read in <code>tf.ragged.cross</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41212\">CVE-2021-41212</a>)</li>\n<li>Fixes a heap OOB read in all <code>tf.raw_ops.QuantizeAndDequantizeV*</code> ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41205\">CVE-2021-41205</a>)</li>\n<li>Fixes an FPE in <code>ParallelConcat</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207\">CVE-2021-41207</a>)</li>\n<li>Fixes FPE issues in convolutions with zero size filters (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41209\">CVE-2021-41209</a>)</li>\n<li>Fixes a heap OOB read in <code>tf.raw_ops.SparseCountSparseOutput</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41210\">CVE-2021-41210</a>)</li>\n<li>Fixes vulnerabilities caused by incomplete validation in boosted trees code (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208\">CVE-2021-41208</a>)</li>\n<li>Fixes vulnerabilities caused by incomplete validation of shapes in multiple TF ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41206\">CVE-2021-41206</a>)</li>\n<li>Fixes a segfault produced while copying constant resource tensor (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41204\">CVE-2021-41204</a>)</li>\n<li>Fixes a vulnerability caused by unitialized access in <code>EinsumHelper::ParseEquation</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41201\">CVE-2021-41201</a>)</li>\n<li>Fixes several vulnerabilities and segfaults caused by missing validation during checkpoint loading (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41203\">CVE-2021-41203</a>)</li>\n<li>Fixes an overflow producing a crash in <code>tf.range</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41202\">CVE-2021-41202</a>)</li>\n<li>Fixes an overflow producing a crash in <code>tf.image.resize</code> when size is large (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41199\">CVE-2021-41199</a>)</li>\n<li>Fixes an overflow producing a crash in <code>tf.tile</code> when tiling tensor is large (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41198\">CVE-2021-41198</a>)</li>\n<li>Fixes a vulnerability produced due to incomplete validation in <code>tf.summary.create_file_writer</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41200\">CVE-2021-41200</a>)</li>\n<li>Fixes multiple crashes due to overflow and <code>CHECK</code>-fail in ops with large tensor shapes (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197\">CVE-2021-41197</a>)</li>\n<li>Fixes a crash in <code>max_pool3d</code> when size argument is 0 or negative (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41196\">CVE-2021-41196</a>)</li>\n<li>Fixes a crash in <code>tf.math.segment_*</code> operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41195\">CVE-2021-41195</a>)</li>\n<li>Updates <code>curl</code> to <code>7.78.0</code> to handle <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22922\">CVE-2021-22922</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22923\">CVE-2021-22923</a>,  <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22924\">CVE-2021-22924</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22925\">CVE-2021-22925</a>, and <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22926\">CVE-2021-22926</a>.</li>\n</ul>\n<h2>TensorFlow 2.5.1</h2>\n<h1>Release 2.5.1</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a heap out of bounds access in sparse reduction operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635\">CVE-2021-37635</a>)</li>\n<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636\">CVE-2021-37636</a>)</li>\n<li>Fixes a null pointer dereference in <code>CompressElement</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637\">CVE-2021-37637</a>)</li>\n<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638\">CVE-2021-37638</a>)</li>\n<li>Fixes a null pointer dereference and a heap OOB read arising from operations restoring tensors (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639\">CVE-2021-37639</a>)</li>\n<li>Fixes an integer division by 0 in sparse reshaping (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640\">CVE-2021-37640</a>)</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\">tensorflow-cpu's changelog</a>.</em></p>\n<blockquote>\n<h1>Release 2.5.2</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a code injection issue in <code>saved_model_cli</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41228\">CVE-2021-41228</a>)</li>\n<li>Fixes a vulnerability due to use of uninitialized value in Tensorflow\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41225\">CVE-2021-41225</a>)</li>\n<li>Fixes a heap OOB in <code>FusedBatchNorm</code> kernels\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41223\">CVE-2021-41223</a>)</li>\n<li>Fixes an arbitrary memory read in <code>ImmutableConst</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41227\">CVE-2021-41227</a>)</li>\n<li>Fixes a heap OOB in <code>SparseBinCount</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41226\">CVE-2021-41226</a>)</li>\n<li>Fixes a heap OOB in <code>SparseFillEmptyRows</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41224\">CVE-2021-41224</a>)</li>\n<li>Fixes a segfault due to negative splits in <code>SplitV</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41222\">CVE-2021-41222</a>)</li>\n<li>Fixes segfaults and vulnerabilities caused by accesses to invalid memory\nduring shape inference in <code>Cudnn*</code> ops\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41221\">CVE-2021-41221</a>)</li>\n<li>Fixes a null pointer exception when <code>Exit</code> node is not preceded by\n<code>Enter</code> op (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41217\">CVE-2021-41217</a>)</li>\n<li>Fixes an integer division by 0 in <code>tf.raw_ops.AllToAll</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41218\">CVE-2021-41218</a>)</li>\n<li>Fixes an undefined behavior via <code>nullptr</code> reference binding in sparse matrix\nmultiplication (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41219\">CVE-2021-41219</a>)</li>\n<li>Fixes a heap buffer overflow in <code>Transpose</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41216\">CVE-2021-41216</a>)</li>\n<li>Prevents deadlocks arising from mutually recursive <code>tf.function</code> objects\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41213\">CVE-2021-41213</a>)</li>\n<li>Fixes a null pointer exception in <code>DeserializeSparse</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41215\">CVE-2021-41215</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to\n<code>nullptr</code> in <code>tf.ragged.cross</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41214\">CVE-2021-41214</a>)</li>\n<li>Fixes a heap OOB read in <code>tf.ragged.cross</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41212\">CVE-2021-41212</a>)</li>\n<li>Fixes a heap OOB read in all <code>tf.raw_ops.QuantizeAndDequantizeV*</code>\nops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41205\">CVE-2021-41205</a>)</li>\n<li>Fixes an FPE in <code>ParallelConcat</code> ([CVE-2021-41207]\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207\">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207</a>))</li>\n<li>Fixes FPE issues in convolutions with zero size filters\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41209\">CVE-2021-41209</a>)</li>\n<li>Fixes a heap OOB read in <code>tf.raw_ops.SparseCountSparseOutput</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41210\">CVE-2021-41210</a>)</li>\n<li>Fixes vulnerabilities caused by incomplete validation in boosted trees code\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208\">CVE-2021-41208</a>)</li>\n<li>Fixes vulnerabilities caused by incomplete validation of shapes in multiple\nTF ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41206\">CVE-2021-41206</a>)</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/957590ea15cc03ee2e00fc61934647d54836676f\"><code>957590e</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52873\">#52873</a> from tensorflow-jenkins/relnotes-2.5.2-20787</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/2e1d16d7aac34983e4ff0d55f434e4d07fea7bce\"><code>2e1d16d</code></a> Update RELEASE.md</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/2fa6dd95659985a9ee9429d146c29c27f12e342c\"><code>2fa6dd9</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52877\">#52877</a> from tensorflow-jenkins/version-numbers-2.5.2-192</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/480748994b151d28818cdfc659f4332bce8a97b2\"><code>4807489</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52881\">#52881</a> from tensorflow/fix-build-1-on-r2.5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/d398bdfd5d2190a3274141416c54f3e7207e96f3\"><code>d398bdf</code></a> Disable failing test</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/857ad5ef1eb23bbeb378b1f3c3cbe6f38286c2d0\"><code>857ad5e</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52878\">#52878</a> from tensorflow/fix-build-1-on-r2.5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/6c2a215be0afd10008046bd072daaf81228a19a1\"><code>6c2a215</code></a> Disable failing test</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/f5c57d495753bbc166abc928430ea808aa8aa6b3\"><code>f5c57d4</code></a> Update version numbers to 2.5.2</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/e51f9495418eb373074a652b5bf4bba1c41aa132\"><code>e51f949</code></a> Insert release notes place-fill</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/2620d2cd5c4e03df6d02ae18fda2a9fdc2466738\"><code>2620d2c</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52863\">#52863</a> from tensorflow/fix-build-3-on-r2.5</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.2\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-cpu&package-manager=pip&previous-version=2.4.0&new-version=2.5.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "90c12bb219c36b3d460d027166eeda96f700bcda", "html_url": "https://github.com/backtick-se/cowait/commit/90c12bb219c36b3d460d027166eeda96f700bcda", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-11-10T19:38:40Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-11-10T19:38:40Z"}, "message": "Bump tensorflow-cpu from 2.4.0 to 2.5.2 in /examples/10-imdb\n\nBumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.2.\n- [Release notes](https://github.com/tensorflow/tensorflow/releases)\n- [Changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md)\n- [Commits](https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.2)\n\n---\nupdated-dependencies:\n- dependency-name: tensorflow-cpu\n  dependency-type: direct:production\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "5caf2ef8c019785b0e380c2e1b9167e9977bd5d2", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/5caf2ef8c019785b0e380c2e1b9167e9977bd5d2"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/90c12bb219c36b3d460d027166eeda96f700bcda", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJhjB/ACRBK7hj4Ov3rIwAA+rsIAJhUttEOeCdtEnHFJ5nR6GjI\nuKTNb1u0eo9G259vcmzZqnBI8toMM9UsUnve42ZSJnzmSeCKF/9n0ho7LlluWZy6\nL1/K0Jdx3cJrDGsEtd1IweRoame3cMSPPw+YB3CjT+Qw9Ei0NULdpXwTzyn7EGeK\nVhytou2EYvMiDPRp6A+BCLhqPPjlxWL4qUOboT4NEI/pDvMI+i3NPgiI7ibcb9Nr\ndgoTzfddmqsm/zlqL1XeQ/PuVSndu3gBUQp4XGMN2auKfRfk5KO9r23C8Qd4dmMk\n1XSuiNMatzFnrTX8b1sHuZPAYKnWxLBGr9xS/OeGK+7ZfKMiTt4sS8UUoBqnGgA=\n=EoDb\n-----END PGP SIGNATURE-----\n", "payload": "tree 5caf2ef8c019785b0e380c2e1b9167e9977bd5d2\nparent 760ddb3ded1b3995bc68f4b74cf28af0c094481f\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1636573120 +0000\ncommitter GitHub <noreply@github.com> 1636573120 +0000\n\nBump tensorflow-cpu from 2.4.0 to 2.5.2 in /examples/10-imdb\n\nBumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.2.\n- [Release notes](https://github.com/tensorflow/tensorflow/releases)\n- [Changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md)\n- [Commits](https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.2)\n\n---\nupdated-dependencies:\n- dependency-name: tensorflow-cpu\n  dependency-type: direct:production\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "98e6bd283315d58bd572235ca4c81da4d14f8888", "filename": "examples/10-imdb/requirements.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/90c12bb219c36b3d460d027166eeda96f700bcda/examples/10-imdb/requirements.txt", "raw_url": "https://github.com/backtick-se/cowait/raw/90c12bb219c36b3d460d027166eeda96f700bcda/examples/10-imdb/requirements.txt", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/examples/10-imdb/requirements.txt?ref=90c12bb219c36b3d460d027166eeda96f700bcda", "patch": "@@ -1,2 +1,2 @@\n-tensorflow-cpu==2.4.0\n+tensorflow-cpu==2.5.2\n keras==2.4.0"}], "stats": {"total": 2, "additions": 1, "deletions": 1}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 336, "title": "Version 0.4.30", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/336", "html_url": "https://github.com/backtick-se/cowait/pull/336", "diff_url": "https://github.com/backtick-se/cowait/pull/336.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/336.patch", "merged_at": "2021-11-01T09:50:18Z"}, "body": "- Added an option to provide a fallback value for environment values (e.g. to provide secret defaults) when using docker.\r\n- Properly mark the `--capture` option for `cowait run` as a boolean flag.\r\n- Improved error reporting for return type errors.\r\n- Fix `aiohttp` to version `3.7.4` to avoid import errors", "commits": [{"sha": "44487d1e69df9939257b7a2d10432035623bc714", "html_url": "https://github.com/backtick-se/cowait/commit/44487d1e69df9939257b7a2d10432035623bc714", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-11-01T09:35:33Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-11-01T09:35:33Z"}, "message": "add a fallback value for environment variables when using docker", "tree": {"sha": "f62d9d9ba8e6e58f7cce0ece818f7fc7ed191a9d", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/f62d9d9ba8e6e58f7cce0ece818f7fc7ed191a9d"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/44487d1e69df9939257b7a2d10432035623bc714", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "32474df2e4540062e2e101bbb804209523c9a5ef", "filename": "cowait/engine/docker/utils.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/backtick-se/cowait/blob/44487d1e69df9939257b7a2d10432035623bc714/cowait/engine/docker/utils.py", "raw_url": "https://github.com/backtick-se/cowait/raw/44487d1e69df9939257b7a2d10432035623bc714/cowait/engine/docker/utils.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/docker/utils.py?ref=44487d1e69df9939257b7a2d10432035623bc714", "patch": "@@ -11,7 +11,7 @@ def create_ports(taskdef):\n \n def create_env(cluster, taskdef):\n     env = base_environment(cluster, taskdef)\n-    \n+\n     # check total length of environment data\n     length = 0\n     for key, value in env.items():\n@@ -20,6 +20,8 @@ def create_env(cluster, taskdef):\n             # try to inherit the setting from the host\n             if key in os.environ:\n                 value = os.environ[key]\n+            elif 'fallback' in value:\n+                value = value['fallback']\n             else:\n                 source = value.get('source', '<unset>')\n                 print(f'Warning: unset environment variable {key} with source \"{source}\"')"}], "stats": {"total": 4, "additions": 3, "deletions": 1}}, {"sha": "c2a133860e980f83c60162d0e2718aa87627e3e0", "html_url": "https://github.com/backtick-se/cowait/commit/c2a133860e980f83c60162d0e2718aa87627e3e0", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-11-01T09:36:22Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-11-01T09:36:22Z"}, "message": "properly mark --capture option as a flag", "tree": {"sha": "4ce3a206aff5e04ca3189d176e696f053f73a260", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/4ce3a206aff5e04ca3189d176e696f053f73a260"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/c2a133860e980f83c60162d0e2718aa87627e3e0", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "9483da91497ce208a69a428eeaf6069080239d1b", "filename": "cowait/cli/app/task.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/backtick-se/cowait/blob/c2a133860e980f83c60162d0e2718aa87627e3e0/cowait/cli/app/task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/c2a133860e980f83c60162d0e2718aa87627e3e0/cowait/cli/app/task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/app/task.py?ref=c2a133860e980f83c60162d0e2718aa87627e3e0", "patch": "@@ -163,7 +163,7 @@ def run(\n               default=False)\n @click.option('--capture',\n               help='toggle pytest output capture',\n-              type=bool,\n+              type=bool, is_flag=True,\n               default=True)\n @click.pass_context\n def test(\n@@ -207,6 +207,7 @@ def ps(ctx, cluster: str):\n def kill(ctx, cluster: str, task: str):\n     cowait.cli.kill(ctx.obj, task, cluster_name=cluster)\n \n+\n @click.command(help='deploy cowait agent')\n @click.option('-c', '--cluster',\n               default=None,"}], "stats": {"total": 3, "additions": 2, "deletions": 1}}, {"sha": "50c63b0d59899e9cd3ed464dfc2cf6d65a041b14", "html_url": "https://github.com/backtick-se/cowait/commit/50c63b0d59899e9cd3ed464dfc2cf6d65a041b14", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-11-01T09:36:55Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-11-01T09:36:55Z"}, "message": "improved errors from invalid return types", "tree": {"sha": "af90e084bbd95d9b787a7f3eedcd42d0b3b69acd", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/af90e084bbd95d9b787a7f3eedcd42d0b3b69acd"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/50c63b0d59899e9cd3ed464dfc2cf6d65a041b14", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "280304aed5205be194cedbb5ca25f4a577966978", "filename": "cowait/types/utils.py", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "blob_url": "https://github.com/backtick-se/cowait/blob/50c63b0d59899e9cd3ed464dfc2cf6d65a041b14/cowait/types/utils.py", "raw_url": "https://github.com/backtick-se/cowait/raw/50c63b0d59899e9cd3ed464dfc2cf6d65a041b14/cowait/types/utils.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/types/utils.py?ref=50c63b0d59899e9cd3ed464dfc2cf6d65a041b14", "patch": "@@ -86,9 +86,14 @@ def typed_return(func: callable, result: object) -> object:\n         result_type = guess_type(result)\n \n     # serialize & validate\n-    data = result_type.serialize(result)\n-    result_type.validate(data, 'Return')\n-    return data, result_type\n+    try:\n+        data = result_type.serialize(result)\n+        result_type.validate(data, 'Return')\n+        return data, result_type\n+    except TypeError:\n+        expect = type(result_type)\n+        got = type(result)\n+        raise TypeError(f'Expected task to return value of type {expect}, got {got}')\n \n \n async def typed_call(func: callable, args: dict) -> object:"}], "stats": {"total": 11, "additions": 8, "deletions": 3}}, {"sha": "d975bad402c02c8a45c0e0e9964b4a9138095b37", "html_url": "https://github.com/backtick-se/cowait/commit/d975bad402c02c8a45c0e0e9964b4a9138095b37", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-11-01T09:41:54Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-11-01T09:41:54Z"}, "message": "fix aiohttp to 3.7.4 to avoid async_timeout import errors", "tree": {"sha": "48e590b2fba398d913b93ded93d142cd57993ef7", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/48e590b2fba398d913b93ded93d142cd57993ef7"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/d975bad402c02c8a45c0e0e9964b4a9138095b37", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "f1e5027f4073fbf162c9b517a98bcaa97721908a", "filename": "poetry.lock", "status": "modified", "additions": 395, "deletions": 287, "changes": 682, "blob_url": "https://github.com/backtick-se/cowait/blob/d975bad402c02c8a45c0e0e9964b4a9138095b37/poetry.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/d975bad402c02c8a45c0e0e9964b4a9138095b37/poetry.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/poetry.lock?ref=d975bad402c02c8a45c0e0e9964b4a9138095b37", "patch": "@@ -61,41 +61,41 @@ python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n \n [[package]]\n name = \"attrs\"\n-version = \"20.3.0\"\n+version = \"21.2.0\"\n description = \"Classes Without Boilerplate\"\n category = \"main\"\n optional = false\n-python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n+python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\"\n \n [package.extras]\n-dev = [\"coverage[toml] (>=5.0.2)\", \"hypothesis\", \"pympler\", \"pytest (>=4.3.0)\", \"six\", \"zope.interface\", \"furo\", \"sphinx\", \"pre-commit\"]\n-docs = [\"furo\", \"sphinx\", \"zope.interface\"]\n-tests = [\"coverage[toml] (>=5.0.2)\", \"hypothesis\", \"pympler\", \"pytest (>=4.3.0)\", \"six\", \"zope.interface\"]\n-tests_no_zope = [\"coverage[toml] (>=5.0.2)\", \"hypothesis\", \"pympler\", \"pytest (>=4.3.0)\", \"six\"]\n+dev = [\"coverage[toml] (>=5.0.2)\", \"hypothesis\", \"pympler\", \"pytest (>=4.3.0)\", \"six\", \"mypy\", \"pytest-mypy-plugins\", \"zope.interface\", \"furo\", \"sphinx\", \"sphinx-notfound-page\", \"pre-commit\"]\n+docs = [\"furo\", \"sphinx\", \"zope.interface\", \"sphinx-notfound-page\"]\n+tests = [\"coverage[toml] (>=5.0.2)\", \"hypothesis\", \"pympler\", \"pytest (>=4.3.0)\", \"six\", \"mypy\", \"pytest-mypy-plugins\", \"zope.interface\"]\n+tests_no_zope = [\"coverage[toml] (>=5.0.2)\", \"hypothesis\", \"pympler\", \"pytest (>=4.3.0)\", \"six\", \"mypy\", \"pytest-mypy-plugins\"]\n \n [[package]]\n name = \"autopep8\"\n-version = \"1.5.5\"\n+version = \"1.5.7\"\n description = \"A tool that automatically formats Python code to conform to the PEP 8 style guide\"\n category = \"dev\"\n optional = false\n python-versions = \"*\"\n \n [package.dependencies]\n-pycodestyle = \">=2.6.0\"\n+pycodestyle = \">=2.7.0\"\n toml = \"*\"\n \n [[package]]\n name = \"cachetools\"\n-version = \"4.2.1\"\n+version = \"4.2.4\"\n description = \"Extensible memoizing collections and decorators\"\n category = \"main\"\n optional = false\n python-versions = \"~=3.5\"\n \n [[package]]\n name = \"certifi\"\n-version = \"2020.12.5\"\n+version = \"2021.10.8\"\n description = \"Python package for providing Mozilla's CA Bundle.\"\n category = \"main\"\n optional = false\n@@ -109,6 +109,17 @@ category = \"main\"\n optional = false\n python-versions = \"*\"\n \n+[[package]]\n+name = \"charset-normalizer\"\n+version = \"2.0.7\"\n+description = \"The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet.\"\n+category = \"main\"\n+optional = false\n+python-versions = \">=3.5.0\"\n+\n+[package.extras]\n+unicode_backport = [\"unicodedata2\"]\n+\n [[package]]\n name = \"click\"\n version = \"7.1.2\"\n@@ -119,11 +130,11 @@ python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\"\n \n [[package]]\n name = \"cloudpickle\"\n-version = \"1.6.0\"\n+version = \"2.0.0\"\n description = \"Extended pickling support for Python objects\"\n category = \"main\"\n optional = false\n-python-versions = \">=3.5\"\n+python-versions = \">=3.6\"\n \n [[package]]\n name = \"colorama\"\n@@ -135,14 +146,14 @@ python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\"\n \n [[package]]\n name = \"coverage\"\n-version = \"5.5\"\n+version = \"6.1.1\"\n description = \"Code coverage measurement for Python\"\n category = \"main\"\n optional = false\n-python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, <4\"\n+python-versions = \">=3.6\"\n \n [package.extras]\n-toml = [\"toml\"]\n+toml = [\"tomli\"]\n \n [[package]]\n name = \"dask\"\n@@ -190,39 +201,38 @@ zict = \">=0.1.3\"\n \n [[package]]\n name = \"docker\"\n-version = \"4.4.4\"\n+version = \"5.0.3\"\n description = \"A Python library for the Docker Engine API.\"\n category = \"main\"\n optional = false\n-python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\"\n+python-versions = \">=3.6\"\n \n [package.dependencies]\n pywin32 = {version = \"227\", markers = \"sys_platform == \\\"win32\\\"\"}\n requests = \">=2.14.2,<2.18.0 || >2.18.0\"\n-six = \">=1.4.0\"\n websocket-client = \">=0.32.0\"\n \n [package.extras]\n ssh = [\"paramiko (>=2.4.2)\"]\n-tls = [\"pyOpenSSL (>=17.5.0)\", \"cryptography (>=1.3.4)\", \"idna (>=2.0.0)\"]\n+tls = [\"pyOpenSSL (>=17.5.0)\", \"cryptography (>=3.4.7)\", \"idna (>=2.0.0)\"]\n \n [[package]]\n name = \"flake8\"\n-version = \"3.8.4\"\n+version = \"3.9.2\"\n description = \"the modular source code checker: pep8 pyflakes and co\"\n category = \"dev\"\n optional = false\n-python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,>=2.7\"\n+python-versions = \"!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,>=2.7\"\n \n [package.dependencies]\n importlib-metadata = {version = \"*\", markers = \"python_version < \\\"3.8\\\"\"}\n mccabe = \">=0.6.0,<0.7.0\"\n-pycodestyle = \">=2.6.0a1,<2.7.0\"\n-pyflakes = \">=2.2.0,<2.3.0\"\n+pycodestyle = \">=2.7.0,<2.8.0\"\n+pyflakes = \">=2.3.0,<2.4.0\"\n \n [[package]]\n name = \"google-auth\"\n-version = \"1.27.0\"\n+version = \"2.3.2\"\n description = \"Google Authentication Library\"\n category = \"main\"\n optional = false\n@@ -235,8 +245,9 @@ rsa = {version = \">=3.1.4,<5\", markers = \"python_version >= \\\"3.6\\\"\"}\n six = \">=1.9.0\"\n \n [package.extras]\n-aiohttp = [\"aiohttp (>=3.6.2,<4.0.0dev)\"]\n+aiohttp = [\"requests (>=2.20.0,<3.0.0dev)\", \"aiohttp (>=3.6.2,<4.0.0dev)\"]\n pyopenssl = [\"pyopenssl (>=20.0.0)\"]\n+reauth = [\"pyu2f (>=0.1.5)\"]\n \n [[package]]\n name = \"heapdict\"\n@@ -248,15 +259,15 @@ python-versions = \"*\"\n \n [[package]]\n name = \"idna\"\n-version = \"2.10\"\n+version = \"3.3\"\n description = \"Internationalized Domain Names in Applications (IDNA)\"\n category = \"main\"\n optional = false\n-python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n+python-versions = \">=3.5\"\n \n [[package]]\n name = \"importlib-metadata\"\n-version = \"3.7.0\"\n+version = \"4.8.1\"\n description = \"Read metadata from Python packages\"\n category = \"main\"\n optional = false\n@@ -268,7 +279,8 @@ zipp = \">=0.5\"\n \n [package.extras]\n docs = [\"sphinx\", \"jaraco.packaging (>=8.2)\", \"rst.linker (>=1.9)\"]\n-testing = [\"pytest (>=3.5,!=3.7.3)\", \"pytest-checkdocs (>=1.2.3)\", \"pytest-flake8\", \"pytest-cov\", \"pytest-enabler\", \"packaging\", \"pep517\", \"pyfakefs\", \"flufl.flake8\", \"pytest-black (>=0.3.7)\", \"pytest-mypy\", \"importlib-resources (>=1.3)\"]\n+perf = [\"ipython\"]\n+testing = [\"pytest (>=4.6)\", \"pytest-checkdocs (>=2.4)\", \"pytest-flake8\", \"pytest-cov\", \"pytest-enabler (>=1.0.1)\", \"packaging\", \"pep517\", \"pyfakefs\", \"flufl.flake8\", \"pytest-perf (>=0.9.2)\", \"pytest-black (>=0.3.7)\", \"pytest-mypy\", \"importlib-resources (>=1.3)\"]\n \n [[package]]\n name = \"iniconfig\"\n@@ -280,17 +292,17 @@ python-versions = \"*\"\n \n [[package]]\n name = \"kubernetes\"\n-version = \"12.0.1\"\n+version = \"19.15.0\"\n description = \"Kubernetes python client\"\n category = \"main\"\n optional = false\n-python-versions = \"*\"\n+python-versions = \">=3.6\"\n \n [package.dependencies]\n certifi = \">=14.05.14\"\n google-auth = \">=1.0.1\"\n python-dateutil = \">=2.5.3\"\n-pyyaml = \">=3.12\"\n+pyyaml = \">=5.4.1\"\n requests = \"*\"\n requests-oauthlib = \"*\"\n six = \">=1.9.0\"\n@@ -302,7 +314,7 @@ adal = [\"adal (>=1.0.2)\"]\n \n [[package]]\n name = \"mako\"\n-version = \"1.1.4\"\n+version = \"1.1.5\"\n description = \"A super-fast templating language that borrows the  best ideas from the existing templating languages.\"\n category = \"dev\"\n optional = false\n@@ -331,11 +343,11 @@ testing = [\"coverage\", \"pyyaml\"]\n \n [[package]]\n name = \"markupsafe\"\n-version = \"1.1.1\"\n+version = \"2.0.1\"\n description = \"Safely add untrusted strings to HTML/XML markup.\"\n category = \"dev\"\n optional = false\n-python-versions = \">=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*\"\n+python-versions = \">=3.6\"\n \n [[package]]\n name = \"mccabe\"\n@@ -355,7 +367,7 @@ python-versions = \"*\"\n \n [[package]]\n name = \"multidict\"\n-version = \"5.1.0\"\n+version = \"5.2.0\"\n description = \"multidict implementation\"\n category = \"main\"\n optional = false\n@@ -371,27 +383,27 @@ python-versions = \">=3.5\"\n \n [[package]]\n name = \"oauthlib\"\n-version = \"3.1.0\"\n+version = \"3.1.1\"\n description = \"A generic, spec-compliant, thorough implementation of the OAuth request-signing logic\"\n category = \"main\"\n optional = false\n-python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n+python-versions = \">=3.6\"\n \n [package.extras]\n-rsa = [\"cryptography\"]\n-signals = [\"blinker\"]\n-signedtoken = [\"cryptography\", \"pyjwt (>=1.0.0)\"]\n+rsa = [\"cryptography (>=3.0.0,<4)\"]\n+signals = [\"blinker (>=1.4.0)\"]\n+signedtoken = [\"cryptography (>=3.0.0,<4)\", \"pyjwt (>=2.0.0,<3)\"]\n \n [[package]]\n name = \"packaging\"\n-version = \"20.9\"\n+version = \"21.2\"\n description = \"Core utilities for Python packages\"\n category = \"main\"\n optional = false\n-python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n+python-versions = \">=3.6\"\n \n [package.dependencies]\n-pyparsing = \">=2.0.2\"\n+pyparsing = \">=2.0.2,<3\"\n \n [[package]]\n name = \"pdoc3\"\n@@ -407,17 +419,18 @@ markdown = \">=3.0\"\n \n [[package]]\n name = \"pluggy\"\n-version = \"0.13.1\"\n+version = \"1.0.0\"\n description = \"plugin and hook calling mechanisms for python\"\n category = \"main\"\n optional = false\n-python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n+python-versions = \">=3.6\"\n \n [package.dependencies]\n importlib-metadata = {version = \">=0.12\", markers = \"python_version < \\\"3.8\\\"\"}\n \n [package.extras]\n dev = [\"pre-commit\", \"tox\"]\n+testing = [\"pytest\", \"pytest-benchmark\"]\n \n [[package]]\n name = \"psutil\"\n@@ -459,15 +472,15 @@ pyasn1 = \">=0.4.6,<0.5.0\"\n \n [[package]]\n name = \"pycodestyle\"\n-version = \"2.6.0\"\n+version = \"2.7.0\"\n description = \"Python style guide checker\"\n category = \"dev\"\n optional = false\n python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n \n [[package]]\n name = \"pyflakes\"\n-version = \"2.2.0\"\n+version = \"2.3.1\"\n description = \"passive checker of Python programs\"\n category = \"dev\"\n optional = false\n@@ -483,7 +496,7 @@ python-versions = \">=2.6, !=3.0.*, !=3.1.*, !=3.2.*\"\n \n [[package]]\n name = \"pytest\"\n-version = \"6.2.2\"\n+version = \"6.2.5\"\n description = \"pytest: simple powerful testing with Python\"\n category = \"main\"\n optional = false\n@@ -496,7 +509,7 @@ colorama = {version = \"*\", markers = \"sys_platform == \\\"win32\\\"\"}\n importlib-metadata = {version = \">=0.12\", markers = \"python_version < \\\"3.8\\\"\"}\n iniconfig = \"*\"\n packaging = \"*\"\n-pluggy = \">=0.12,<1.0.0a1\"\n+pluggy = \">=0.12,<2.0\"\n py = \">=1.8.2\"\n toml = \"*\"\n \n@@ -505,7 +518,7 @@ testing = [\"argcomplete\", \"hypothesis (>=3.56)\", \"mock\", \"nose\", \"requests\", \"xm\n \n [[package]]\n name = \"pytest-cov\"\n-version = \"2.11.1\"\n+version = \"2.12.1\"\n description = \"Pytest plugin for measuring coverage.\"\n category = \"main\"\n optional = false\n@@ -514,9 +527,10 @@ python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\"\n [package.dependencies]\n coverage = \">=5.2.1\"\n pytest = \">=4.6\"\n+toml = \"*\"\n \n [package.extras]\n-testing = [\"fields\", \"hunter\", \"process-tests (==2.0.2)\", \"six\", \"pytest-xdist\", \"virtualenv\"]\n+testing = [\"fields\", \"hunter\", \"process-tests\", \"six\", \"pytest-xdist\", \"virtualenv\"]\n \n [[package]]\n name = \"pytest-sugar\"\n@@ -533,7 +547,7 @@ termcolor = \">=1.1.0\"\n \n [[package]]\n name = \"python-dateutil\"\n-version = \"2.8.1\"\n+version = \"2.8.2\"\n description = \"Extensions to the standard Python datetime module\"\n category = \"main\"\n optional = false\n@@ -571,21 +585,21 @@ python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, !=3.5.*\"\n \n [[package]]\n name = \"requests\"\n-version = \"2.25.1\"\n+version = \"2.26.0\"\n description = \"Python HTTP for Humans.\"\n category = \"main\"\n optional = false\n-python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\"\n+python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, !=3.5.*\"\n \n [package.dependencies]\n certifi = \">=2017.4.17\"\n-chardet = \">=3.0.2,<5\"\n-idna = \">=2.5,<3\"\n+charset-normalizer = {version = \">=2.0.0,<2.1.0\", markers = \"python_version >= \\\"3\\\"\"}\n+idna = {version = \">=2.5,<4\", markers = \"python_version >= \\\"3\\\"\"}\n urllib3 = \">=1.21.1,<1.27\"\n \n [package.extras]\n-security = [\"pyOpenSSL (>=0.14)\", \"cryptography (>=1.3.4)\"]\n socks = [\"PySocks (>=1.5.6,!=1.5.7)\", \"win-inet-pton\"]\n+use_chardet_on_py3 = [\"chardet (>=3.0.2,<5)\"]\n \n [[package]]\n name = \"requests-oauthlib\"\n@@ -615,23 +629,23 @@ pyasn1 = \">=0.1.3\"\n \n [[package]]\n name = \"six\"\n-version = \"1.15.0\"\n+version = \"1.16.0\"\n description = \"Python 2 and 3 compatibility utilities\"\n category = \"main\"\n optional = false\n python-versions = \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*\"\n \n [[package]]\n name = \"sortedcontainers\"\n-version = \"2.3.0\"\n+version = \"2.4.0\"\n description = \"Sorted Containers -- Sorted List, Sorted Dict, Sorted Set\"\n category = \"main\"\n optional = false\n python-versions = \"*\"\n \n [[package]]\n name = \"sty\"\n-version = \"1.0.0rc1\"\n+version = \"1.0.0rc2\"\n description = \"String styling for your terminal\"\n category = \"main\"\n optional = false\n@@ -679,15 +693,15 @@ python-versions = \">= 3.5\"\n \n [[package]]\n name = \"typing-extensions\"\n-version = \"3.7.4.3\"\n+version = \"3.10.0.2\"\n description = \"Backported and Experimental Type Hints for Python 3.5+\"\n category = \"main\"\n optional = false\n python-versions = \"*\"\n \n [[package]]\n name = \"urllib3\"\n-version = \"1.26.3\"\n+version = \"1.26.7\"\n description = \"HTTP library with thread-safe connection pooling, file post, and more.\"\n category = \"main\"\n optional = false\n@@ -700,18 +714,19 @@ socks = [\"PySocks (>=1.5.6,!=1.5.7,<2.0)\"]\n \n [[package]]\n name = \"websocket-client\"\n-version = \"0.57.0\"\n-description = \"WebSocket client for Python. hybi13 is supported.\"\n+version = \"1.2.1\"\n+description = \"WebSocket client for Python with low level API options\"\n category = \"main\"\n optional = false\n-python-versions = \">=2.6, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\"\n+python-versions = \">=3.6\"\n \n-[package.dependencies]\n-six = \"*\"\n+[package.extras]\n+optional = [\"python-socks\", \"wsaccel\"]\n+test = [\"websockets\"]\n \n [[package]]\n name = \"yarl\"\n-version = \"1.6.3\"\n+version = \"1.7.0\"\n description = \"Yet another URL library\"\n category = \"main\"\n optional = false\n@@ -735,20 +750,20 @@ heapdict = \"*\"\n \n [[package]]\n name = \"zipp\"\n-version = \"3.4.0\"\n+version = \"3.6.0\"\n description = \"Backport of pathlib-compatible object wrapper for zip files\"\n category = \"main\"\n optional = false\n python-versions = \">=3.6\"\n \n [package.extras]\n-docs = [\"sphinx\", \"jaraco.packaging (>=3.2)\", \"rst.linker (>=1.9)\"]\n-testing = [\"pytest (>=3.5,!=3.7.3)\", \"pytest-checkdocs (>=1.2.3)\", \"pytest-flake8\", \"pytest-cov\", \"jaraco.test (>=3.2.0)\", \"jaraco.itertools\", \"func-timeout\", \"pytest-black (>=0.3.7)\", \"pytest-mypy\"]\n+docs = [\"sphinx\", \"jaraco.packaging (>=8.2)\", \"rst.linker (>=1.9)\"]\n+testing = [\"pytest (>=4.6)\", \"pytest-checkdocs (>=2.4)\", \"pytest-flake8\", \"pytest-cov\", \"pytest-enabler (>=1.0.1)\", \"jaraco.itertools\", \"func-timeout\", \"pytest-black (>=0.3.7)\", \"pytest-mypy\"]\n \n [metadata]\n lock-version = \"1.1\"\n python-versions = \"^3.7\"\n-content-hash = \"d81e21969cd3770a13d044440066f1627210682c7ac5f41366b7b9c217b58654\"\n+content-hash = \"272abb5c2ebdab3a6c43f26d03260e7e879cf326b9b78471b37fc08586e30e03\"\n \n [metadata.files]\n aiohttp = [\n@@ -806,89 +821,90 @@ atomicwrites = [\n     {file = \"atomicwrites-1.4.0.tar.gz\", hash = \"sha256:ae70396ad1a434f9c7046fd2dd196fc04b12f9e91ffb859164193be8b6168a7a\"},\n ]\n attrs = [\n-    {file = \"attrs-20.3.0-py2.py3-none-any.whl\", hash = \"sha256:31b2eced602aa8423c2aea9c76a724617ed67cf9513173fd3a4f03e3a929c7e6\"},\n-    {file = \"attrs-20.3.0.tar.gz\", hash = \"sha256:832aa3cde19744e49938b91fea06d69ecb9e649c93ba974535d08ad92164f700\"},\n+    {file = \"attrs-21.2.0-py2.py3-none-any.whl\", hash = \"sha256:149e90d6d8ac20db7a955ad60cf0e6881a3f20d37096140088356da6c716b0b1\"},\n+    {file = \"attrs-21.2.0.tar.gz\", hash = \"sha256:ef6aaac3ca6cd92904cdd0d83f629a15f18053ec84e6432106f7a4d04ae4f5fb\"},\n ]\n autopep8 = [\n-    {file = \"autopep8-1.5.5-py2.py3-none-any.whl\", hash = \"sha256:9e136c472c475f4ee4978b51a88a494bfcd4e3ed17950a44a988d9e434837bea\"},\n-    {file = \"autopep8-1.5.5.tar.gz\", hash = \"sha256:cae4bc0fb616408191af41d062d7ec7ef8679c7f27b068875ca3a9e2878d5443\"},\n+    {file = \"autopep8-1.5.7-py2.py3-none-any.whl\", hash = \"sha256:aa213493c30dcdac99537249ee65b24af0b2c29f2e83cd8b3f68760441ed0db9\"},\n+    {file = \"autopep8-1.5.7.tar.gz\", hash = \"sha256:276ced7e9e3cb22e5d7c14748384a5cf5d9002257c0ed50c0e075b68011bb6d0\"},\n ]\n cachetools = [\n-    {file = \"cachetools-4.2.1-py3-none-any.whl\", hash = \"sha256:1d9d5f567be80f7c07d765e21b814326d78c61eb0c3a637dffc0e5d1796cb2e2\"},\n-    {file = \"cachetools-4.2.1.tar.gz\", hash = \"sha256:f469e29e7aa4cff64d8de4aad95ce76de8ea1125a16c68e0d93f65c3c3dc92e9\"},\n+    {file = \"cachetools-4.2.4-py3-none-any.whl\", hash = \"sha256:92971d3cb7d2a97efff7c7bb1657f21a8f5fb309a37530537c71b1774189f2d1\"},\n+    {file = \"cachetools-4.2.4.tar.gz\", hash = \"sha256:89ea6f1b638d5a73a4f9226be57ac5e4f399d22770b92355f92dcb0f7f001693\"},\n ]\n certifi = [\n-    {file = \"certifi-2020.12.5-py2.py3-none-any.whl\", hash = \"sha256:719a74fb9e33b9bd44cc7f3a8d94bc35e4049deebe19ba7d8e108280cfd59830\"},\n-    {file = \"certifi-2020.12.5.tar.gz\", hash = \"sha256:1a4995114262bffbc2413b159f2a1a480c969de6e6eb13ee966d470af86af59c\"},\n+    {file = \"certifi-2021.10.8-py2.py3-none-any.whl\", hash = \"sha256:d62a0163eb4c2344ac042ab2bdf75399a71a2d8c7d47eac2e2ee91b9d6339569\"},\n+    {file = \"certifi-2021.10.8.tar.gz\", hash = \"sha256:78884e7c1d4b00ce3cea67b44566851c4343c120abd683433ce934a68ea58872\"},\n ]\n chardet = [\n     {file = \"chardet-3.0.4-py2.py3-none-any.whl\", hash = \"sha256:fc323ffcaeaed0e0a02bf4d117757b98aed530d9ed4531e3e15460124c106691\"},\n     {file = \"chardet-3.0.4.tar.gz\", hash = \"sha256:84ab92ed1c4d4f16916e05906b6b75a6c0fb5db821cc65e70cbd64a3e2a5eaae\"},\n ]\n+charset-normalizer = [\n+    {file = \"charset-normalizer-2.0.7.tar.gz\", hash = \"sha256:e019de665e2bcf9c2b64e2e5aa025fa991da8720daa3c1138cadd2fd1856aed0\"},\n+    {file = \"charset_normalizer-2.0.7-py3-none-any.whl\", hash = \"sha256:f7af805c321bfa1ce6714c51f254e0d5bb5e5834039bc17db7ebe3a4cec9492b\"},\n+]\n click = [\n     {file = \"click-7.1.2-py2.py3-none-any.whl\", hash = \"sha256:dacca89f4bfadd5de3d7489b7c8a566eee0d3676333fbb50030263894c38c0dc\"},\n     {file = \"click-7.1.2.tar.gz\", hash = \"sha256:d2b5255c7c6349bc1bd1e59e08cd12acbbd63ce649f2588755783aa94dfb6b1a\"},\n ]\n cloudpickle = [\n-    {file = \"cloudpickle-1.6.0-py3-none-any.whl\", hash = \"sha256:3a32d0eb0bc6f4d0c57fbc4f3e3780f7a81e6fee0fa935072884d58ae8e1cc7c\"},\n-    {file = \"cloudpickle-1.6.0.tar.gz\", hash = \"sha256:9bc994f9e9447593bd0a45371f0e7ac7333710fcf64a4eb9834bf149f4ef2f32\"},\n+    {file = \"cloudpickle-2.0.0-py3-none-any.whl\", hash = \"sha256:6b2df9741d06f43839a3275c4e6632f7df6487a1f181f5f46a052d3c917c3d11\"},\n+    {file = \"cloudpickle-2.0.0.tar.gz\", hash = \"sha256:5cd02f3b417a783ba84a4ec3e290ff7929009fe51f6405423cfccfadd43ba4a4\"},\n ]\n colorama = [\n     {file = \"colorama-0.4.4-py2.py3-none-any.whl\", hash = \"sha256:9f47eda37229f68eee03b24b9748937c7dc3868f906e8ba69fbcbdd3bc5dc3e2\"},\n ]\n coverage = [\n-    {file = \"coverage-5.5-cp27-cp27m-macosx_10_9_x86_64.whl\", hash = \"sha256:b6d534e4b2ab35c9f93f46229363e17f63c53ad01330df9f2d6bd1187e5eaacf\"},\n-    {file = \"coverage-5.5-cp27-cp27m-manylinux1_i686.whl\", hash = \"sha256:b7895207b4c843c76a25ab8c1e866261bcfe27bfaa20c192de5190121770672b\"},\n-    {file = \"coverage-5.5-cp27-cp27m-manylinux1_x86_64.whl\", hash = \"sha256:c2723d347ab06e7ddad1a58b2a821218239249a9e4365eaff6649d31180c1669\"},\n-    {file = \"coverage-5.5-cp27-cp27m-manylinux2010_i686.whl\", hash = \"sha256:900fbf7759501bc7807fd6638c947d7a831fc9fdf742dc10f02956ff7220fa90\"},\n-    {file = \"coverage-5.5-cp27-cp27m-manylinux2010_x86_64.whl\", hash = \"sha256:004d1880bed2d97151facef49f08e255a20ceb6f9432df75f4eef018fdd5a78c\"},\n-    {file = \"coverage-5.5-cp27-cp27m-win32.whl\", hash = \"sha256:06191eb60f8d8a5bc046f3799f8a07a2d7aefb9504b0209aff0b47298333302a\"},\n-    {file = \"coverage-5.5-cp27-cp27m-win_amd64.whl\", hash = \"sha256:7501140f755b725495941b43347ba8a2777407fc7f250d4f5a7d2a1050ba8e82\"},\n-    {file = \"coverage-5.5-cp27-cp27mu-manylinux1_i686.whl\", hash = \"sha256:372da284cfd642d8e08ef606917846fa2ee350f64994bebfbd3afb0040436905\"},\n-    {file = \"coverage-5.5-cp27-cp27mu-manylinux1_x86_64.whl\", hash = \"sha256:8963a499849a1fc54b35b1c9f162f4108017b2e6db2c46c1bed93a72262ed083\"},\n-    {file = \"coverage-5.5-cp27-cp27mu-manylinux2010_i686.whl\", hash = \"sha256:869a64f53488f40fa5b5b9dcb9e9b2962a66a87dab37790f3fcfb5144b996ef5\"},\n-    {file = \"coverage-5.5-cp27-cp27mu-manylinux2010_x86_64.whl\", hash = \"sha256:4a7697d8cb0f27399b0e393c0b90f0f1e40c82023ea4d45d22bce7032a5d7b81\"},\n-    {file = \"coverage-5.5-cp310-cp310-macosx_10_14_x86_64.whl\", hash = \"sha256:8d0a0725ad7c1a0bcd8d1b437e191107d457e2ec1084b9f190630a4fb1af78e6\"},\n-    {file = \"coverage-5.5-cp310-cp310-manylinux1_x86_64.whl\", hash = \"sha256:51cb9476a3987c8967ebab3f0fe144819781fca264f57f89760037a2ea191cb0\"},\n-    {file = \"coverage-5.5-cp310-cp310-win_amd64.whl\", hash = \"sha256:c0891a6a97b09c1f3e073a890514d5012eb256845c451bd48f7968ef939bf4ae\"},\n-    {file = \"coverage-5.5-cp35-cp35m-macosx_10_9_x86_64.whl\", hash = \"sha256:3487286bc29a5aa4b93a072e9592f22254291ce96a9fbc5251f566b6b7343cdb\"},\n-    {file = \"coverage-5.5-cp35-cp35m-manylinux1_i686.whl\", hash = \"sha256:deee1077aae10d8fa88cb02c845cfba9b62c55e1183f52f6ae6a2df6a2187160\"},\n-    {file = \"coverage-5.5-cp35-cp35m-manylinux1_x86_64.whl\", hash = \"sha256:f11642dddbb0253cc8853254301b51390ba0081750a8ac03f20ea8103f0c56b6\"},\n-    {file = \"coverage-5.5-cp35-cp35m-manylinux2010_i686.whl\", hash = \"sha256:6c90e11318f0d3c436a42409f2749ee1a115cd8b067d7f14c148f1ce5574d701\"},\n-    {file = \"coverage-5.5-cp35-cp35m-manylinux2010_x86_64.whl\", hash = \"sha256:30c77c1dc9f253283e34c27935fded5015f7d1abe83bc7821680ac444eaf7793\"},\n-    {file = \"coverage-5.5-cp35-cp35m-win32.whl\", hash = \"sha256:9a1ef3b66e38ef8618ce5fdc7bea3d9f45f3624e2a66295eea5e57966c85909e\"},\n-    {file = \"coverage-5.5-cp35-cp35m-win_amd64.whl\", hash = \"sha256:972c85d205b51e30e59525694670de6a8a89691186012535f9d7dbaa230e42c3\"},\n-    {file = \"coverage-5.5-cp36-cp36m-macosx_10_9_x86_64.whl\", hash = \"sha256:af0e781009aaf59e25c5a678122391cb0f345ac0ec272c7961dc5455e1c40066\"},\n-    {file = \"coverage-5.5-cp36-cp36m-manylinux1_i686.whl\", hash = \"sha256:74d881fc777ebb11c63736622b60cb9e4aee5cace591ce274fb69e582a12a61a\"},\n-    {file = \"coverage-5.5-cp36-cp36m-manylinux1_x86_64.whl\", hash = \"sha256:92b017ce34b68a7d67bd6d117e6d443a9bf63a2ecf8567bb3d8c6c7bc5014465\"},\n-    {file = \"coverage-5.5-cp36-cp36m-manylinux2010_i686.whl\", hash = \"sha256:d636598c8305e1f90b439dbf4f66437de4a5e3c31fdf47ad29542478c8508bbb\"},\n-    {file = \"coverage-5.5-cp36-cp36m-manylinux2010_x86_64.whl\", hash = \"sha256:41179b8a845742d1eb60449bdb2992196e211341818565abded11cfa90efb821\"},\n-    {file = \"coverage-5.5-cp36-cp36m-win32.whl\", hash = \"sha256:040af6c32813fa3eae5305d53f18875bedd079960822ef8ec067a66dd8afcd45\"},\n-    {file = \"coverage-5.5-cp36-cp36m-win_amd64.whl\", hash = \"sha256:5fec2d43a2cc6965edc0bb9e83e1e4b557f76f843a77a2496cbe719583ce8184\"},\n-    {file = \"coverage-5.5-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:18ba8bbede96a2c3dde7b868de9dcbd55670690af0988713f0603f037848418a\"},\n-    {file = \"coverage-5.5-cp37-cp37m-manylinux1_i686.whl\", hash = \"sha256:2910f4d36a6a9b4214bb7038d537f015346f413a975d57ca6b43bf23d6563b53\"},\n-    {file = \"coverage-5.5-cp37-cp37m-manylinux1_x86_64.whl\", hash = \"sha256:f0b278ce10936db1a37e6954e15a3730bea96a0997c26d7fee88e6c396c2086d\"},\n-    {file = \"coverage-5.5-cp37-cp37m-manylinux2010_i686.whl\", hash = \"sha256:796c9c3c79747146ebd278dbe1e5c5c05dd6b10cc3bcb8389dfdf844f3ead638\"},\n-    {file = \"coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl\", hash = \"sha256:53194af30d5bad77fcba80e23a1441c71abfb3e01192034f8246e0d8f99528f3\"},\n-    {file = \"coverage-5.5-cp37-cp37m-win32.whl\", hash = \"sha256:184a47bbe0aa6400ed2d41d8e9ed868b8205046518c52464fde713ea06e3a74a\"},\n-    {file = \"coverage-5.5-cp37-cp37m-win_amd64.whl\", hash = \"sha256:2949cad1c5208b8298d5686d5a85b66aae46d73eec2c3e08c817dd3513e5848a\"},\n-    {file = \"coverage-5.5-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:217658ec7187497e3f3ebd901afdca1af062b42cfe3e0dafea4cced3983739f6\"},\n-    {file = \"coverage-5.5-cp38-cp38-manylinux1_i686.whl\", hash = \"sha256:1aa846f56c3d49205c952d8318e76ccc2ae23303351d9270ab220004c580cfe2\"},\n-    {file = \"coverage-5.5-cp38-cp38-manylinux1_x86_64.whl\", hash = \"sha256:24d4a7de75446be83244eabbff746d66b9240ae020ced65d060815fac3423759\"},\n-    {file = \"coverage-5.5-cp38-cp38-manylinux2010_i686.whl\", hash = \"sha256:d1f8bf7b90ba55699b3a5e44930e93ff0189aa27186e96071fac7dd0d06a1873\"},\n-    {file = \"coverage-5.5-cp38-cp38-manylinux2010_x86_64.whl\", hash = \"sha256:970284a88b99673ccb2e4e334cfb38a10aab7cd44f7457564d11898a74b62d0a\"},\n-    {file = \"coverage-5.5-cp38-cp38-win32.whl\", hash = \"sha256:01d84219b5cdbfc8122223b39a954820929497a1cb1422824bb86b07b74594b6\"},\n-    {file = \"coverage-5.5-cp38-cp38-win_amd64.whl\", hash = \"sha256:2e0d881ad471768bf6e6c2bf905d183543f10098e3b3640fc029509530091502\"},\n-    {file = \"coverage-5.5-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:d1f9ce122f83b2305592c11d64f181b87153fc2c2bbd3bb4a3dde8303cfb1a6b\"},\n-    {file = \"coverage-5.5-cp39-cp39-manylinux1_i686.whl\", hash = \"sha256:13c4ee887eca0f4c5a247b75398d4114c37882658300e153113dafb1d76de529\"},\n-    {file = \"coverage-5.5-cp39-cp39-manylinux1_x86_64.whl\", hash = \"sha256:52596d3d0e8bdf3af43db3e9ba8dcdaac724ba7b5ca3f6358529d56f7a166f8b\"},\n-    {file = \"coverage-5.5-cp39-cp39-manylinux2010_i686.whl\", hash = \"sha256:2cafbbb3af0733db200c9b5f798d18953b1a304d3f86a938367de1567f4b5bff\"},\n-    {file = \"coverage-5.5-cp39-cp39-manylinux2010_x86_64.whl\", hash = \"sha256:44d654437b8ddd9eee7d1eaee28b7219bec228520ff809af170488fd2fed3e2b\"},\n-    {file = \"coverage-5.5-cp39-cp39-win32.whl\", hash = \"sha256:d314ed732c25d29775e84a960c3c60808b682c08d86602ec2c3008e1202e3bb6\"},\n-    {file = \"coverage-5.5-cp39-cp39-win_amd64.whl\", hash = \"sha256:13034c4409db851670bc9acd836243aeee299949bd5673e11844befcb0149f03\"},\n-    {file = \"coverage-5.5-pp36-none-any.whl\", hash = \"sha256:f030f8873312a16414c0d8e1a1ddff2d3235655a2174e3648b4fa66b3f2f1079\"},\n-    {file = \"coverage-5.5-pp37-none-any.whl\", hash = \"sha256:2a3859cb82dcbda1cfd3e6f71c27081d18aa251d20a17d87d26d4cd216fb0af4\"},\n-    {file = \"coverage-5.5.tar.gz\", hash = \"sha256:ebe78fe9a0e874362175b02371bdfbee64d8edc42a044253ddf4ee7d3c15212c\"},\n+    {file = \"coverage-6.1.1-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:42a1fb5dee3355df90b635906bb99126faa7936d87dfc97eacc5293397618cb7\"},\n+    {file = \"coverage-6.1.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a00284dbfb53b42e35c7dd99fc0e26ef89b4a34efff68078ed29d03ccb28402a\"},\n+    {file = \"coverage-6.1.1-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:51a441011a30d693e71dea198b2a6f53ba029afc39f8e2aeb5b77245c1b282ef\"},\n+    {file = \"coverage-6.1.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:e76f017b6d4140a038c5ff12be1581183d7874e41f1c0af58ecf07748d36a336\"},\n+    {file = \"coverage-6.1.1-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:7833c872718dc913f18e51ee97ea0dece61d9930893a58b20b3daf09bb1af6b6\"},\n+    {file = \"coverage-6.1.1-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:8186b5a4730c896cbe1e4b645bdc524e62d874351ae50e1db7c3e9f5dc81dc26\"},\n+    {file = \"coverage-6.1.1-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:bbca34dca5a2d60f81326d908d77313816fad23d11b6069031a3d6b8c97a54f9\"},\n+    {file = \"coverage-6.1.1-cp310-cp310-win32.whl\", hash = \"sha256:72bf437d54186d104388cbae73c9f2b0f8a3e11b6e8d7deb593bd14625c96026\"},\n+    {file = \"coverage-6.1.1-cp310-cp310-win_amd64.whl\", hash = \"sha256:994ce5a7b3d20981b81d83618aa4882f955bfa573efdbef033d5632b58597ba9\"},\n+    {file = \"coverage-6.1.1-cp36-cp36m-macosx_10_9_x86_64.whl\", hash = \"sha256:ab6a0fe4c96f8058d41948ddf134420d3ef8c42d5508b5a341a440cce7a37a1d\"},\n+    {file = \"coverage-6.1.1-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:10ab138b153e4cc408b43792cb7f518f9ee02f4ff55cd1ab67ad6fd7e9905c7e\"},\n+    {file = \"coverage-6.1.1-cp36-cp36m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:7e083d32965d2eb6638a77e65b622be32a094fdc0250f28ce6039b0732fbcaa8\"},\n+    {file = \"coverage-6.1.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:359a32515e94e398a5c0fa057e5887a42e647a9502d8e41165cf5cb8d3d1ca67\"},\n+    {file = \"coverage-6.1.1-cp36-cp36m-musllinux_1_1_aarch64.whl\", hash = \"sha256:bf656cd74ff7b4ed7006cdb2a6728150aaad69c7242b42a2a532f77b63ea233f\"},\n+    {file = \"coverage-6.1.1-cp36-cp36m-musllinux_1_1_i686.whl\", hash = \"sha256:dc5023be1c2a8b0a0ab5e31389e62c28b2453eb31dd069f4b8d1a0f9814d951a\"},\n+    {file = \"coverage-6.1.1-cp36-cp36m-musllinux_1_1_x86_64.whl\", hash = \"sha256:557594a50bfe3fb0b1b57460f6789affe8850ad19c1acf2d14a3e12b2757d489\"},\n+    {file = \"coverage-6.1.1-cp36-cp36m-win32.whl\", hash = \"sha256:9eb0a1923354e0fdd1c8a6f53f5db2e6180d670e2b587914bf2e79fa8acfd003\"},\n+    {file = \"coverage-6.1.1-cp36-cp36m-win_amd64.whl\", hash = \"sha256:04a92a6cf9afd99f9979c61348ec79725a9f9342fb45e63c889e33c04610d97b\"},\n+    {file = \"coverage-6.1.1-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:479228e1b798d3c246ac89b09897ee706c51b3e5f8f8d778067f38db73ccc717\"},\n+    {file = \"coverage-6.1.1-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:78287731e3601ea5ce9d6468c82d88a12ef8fe625d6b7bdec9b45d96c1ad6533\"},\n+    {file = \"coverage-6.1.1-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:c95257aa2ccf75d3d91d772060538d5fea7f625e48157f8ca44594f94d41cb33\"},\n+    {file = \"coverage-6.1.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:9ad5895938a894c368d49d8470fe9f519909e5ebc6b8f8ea5190bd0df6aa4271\"},\n+    {file = \"coverage-6.1.1-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:326d944aad0189603733d646e8d4a7d952f7145684da973c463ec2eefe1387c2\"},\n+    {file = \"coverage-6.1.1-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:e7d5606b9240ed4def9cbdf35be4308047d11e858b9c88a6c26974758d6225ce\"},\n+    {file = \"coverage-6.1.1-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:572f917267f363101eec375c109c9c1118037c7cc98041440b5eabda3185ac7b\"},\n+    {file = \"coverage-6.1.1-cp37-cp37m-win32.whl\", hash = \"sha256:35cd2230e1ed76df7d0081a997f0fe705be1f7d8696264eb508076e0d0b5a685\"},\n+    {file = \"coverage-6.1.1-cp37-cp37m-win_amd64.whl\", hash = \"sha256:65ad3ff837c89a229d626b8004f0ee32110f9bfdb6a88b76a80df36ccc60d926\"},\n+    {file = \"coverage-6.1.1-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:977ce557d79577a3dd510844904d5d968bfef9489f512be65e2882e1c6eed7d8\"},\n+    {file = \"coverage-6.1.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:62512c0ec5d307f56d86504c58eace11c1bc2afcdf44e3ff20de8ca427ca1d0e\"},\n+    {file = \"coverage-6.1.1-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:2e5b9c17a56b8bf0c0a9477fcd30d357deb486e4e1b389ed154f608f18556c8a\"},\n+    {file = \"coverage-6.1.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:666c6b32b69e56221ad1551d377f718ed00e6167c7a1b9257f780b105a101271\"},\n+    {file = \"coverage-6.1.1-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:fb2fa2f6506c03c48ca42e3fe5a692d7470d290c047ee6de7c0f3e5fa7639ac9\"},\n+    {file = \"coverage-6.1.1-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:f0f80e323a17af63eac6a9db0c9188c10f1fd815c3ab299727150cc0eb92c7a4\"},\n+    {file = \"coverage-6.1.1-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:738e823a746841248b56f0f3bd6abf3b73af191d1fd65e4c723b9c456216f0ad\"},\n+    {file = \"coverage-6.1.1-cp38-cp38-win32.whl\", hash = \"sha256:8605add58e6a960729aa40c0fd9a20a55909dd9b586d3e8104cc7f45869e4c6b\"},\n+    {file = \"coverage-6.1.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:6e994003e719458420e14ffb43c08f4c14990e20d9e077cb5cad7a3e419bbb54\"},\n+    {file = \"coverage-6.1.1-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:e3c4f5211394cd0bf6874ac5d29684a495f9c374919833dcfff0bd6d37f96201\"},\n+    {file = \"coverage-6.1.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:e14bceb1f3ae8a14374be2b2d7bc12a59226872285f91d66d301e5f41705d4d6\"},\n+    {file = \"coverage-6.1.1-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:0147f7833c41927d84f5af9219d9b32f875c0689e5e74ac8ca3cb61e73a698f9\"},\n+    {file = \"coverage-6.1.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:b1d0a1bce919de0dd8da5cff4e616b2d9e6ebf3bd1410ff645318c3dd615010a\"},\n+    {file = \"coverage-6.1.1-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:ae6de0e41f44794e68d23644636544ed8003ce24845f213b24de097cbf44997f\"},\n+    {file = \"coverage-6.1.1-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:db2797ed7a7e883b9ab76e8e778bb4c859fc2037d6fd0644d8675e64d58d1653\"},\n+    {file = \"coverage-6.1.1-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:c40966b683d92869b72ea3c11fd6b99a091fd30e12652727eca117273fc97366\"},\n+    {file = \"coverage-6.1.1-cp39-cp39-win32.whl\", hash = \"sha256:a11a2c019324fc111485e79d55907e7289e53d0031275a6c8daed30690bc50c0\"},\n+    {file = \"coverage-6.1.1-cp39-cp39-win_amd64.whl\", hash = \"sha256:4d8b453764b9b26b0dd2afb83086a7c3f9379134e340288d2a52f8a91592394b\"},\n+    {file = \"coverage-6.1.1-pp36-none-any.whl\", hash = \"sha256:3b270c6b48d3ff5a35deb3648028ba2643ad8434b07836782b1139cf9c66313f\"},\n+    {file = \"coverage-6.1.1-pp37-none-any.whl\", hash = \"sha256:ffa8fee2b1b9e60b531c4c27cf528d6b5d5da46b1730db1f4d6eee56ff282e07\"},\n+    {file = \"coverage-6.1.1-pp38-none-any.whl\", hash = \"sha256:4cd919057636f63ab299ccb86ea0e78b87812400c76abab245ca385f17d19fb5\"},\n+    {file = \"coverage-6.1.1.tar.gz\", hash = \"sha256:b8e4f15b672c9156c1154249a9c5746e86ac9ae9edc3799ee3afebc323d9d9e0\"},\n ]\n dask = [\n     {file = \"dask-2.30.0-py3-none-any.whl\", hash = \"sha256:4c215aa55951f570b5a294b2ce964ed801c6efd766231c53447460e60f73ea14\"},\n@@ -899,78 +915,100 @@ distributed = [\n     {file = \"distributed-2.30.1.tar.gz\", hash = \"sha256:1421d3b84a0885aeb2c4bdc9e8896729c0f053a9375596c9de8864e055e2ac8e\"},\n ]\n docker = [\n-    {file = \"docker-4.4.4-py2.py3-none-any.whl\", hash = \"sha256:f3607d5695be025fa405a12aca2e5df702a57db63790c73b927eb6a94aac60af\"},\n-    {file = \"docker-4.4.4.tar.gz\", hash = \"sha256:d3393c878f575d3a9ca3b94471a3c89a6d960b35feb92f033c0de36cc9d934db\"},\n+    {file = \"docker-5.0.3-py2.py3-none-any.whl\", hash = \"sha256:7a79bb439e3df59d0a72621775d600bc8bc8b422d285824cb37103eab91d1ce0\"},\n+    {file = \"docker-5.0.3.tar.gz\", hash = \"sha256:d916a26b62970e7c2f554110ed6af04c7ccff8e9f81ad17d0d40c75637e227fb\"},\n ]\n flake8 = [\n-    {file = \"flake8-3.8.4-py2.py3-none-any.whl\", hash = \"sha256:749dbbd6bfd0cf1318af27bf97a14e28e5ff548ef8e5b1566ccfb25a11e7c839\"},\n-    {file = \"flake8-3.8.4.tar.gz\", hash = \"sha256:aadae8761ec651813c24be05c6f7b4680857ef6afaae4651a4eccaef97ce6c3b\"},\n+    {file = \"flake8-3.9.2-py2.py3-none-any.whl\", hash = \"sha256:bf8fd333346d844f616e8d47905ef3a3384edae6b4e9beb0c5101e25e3110907\"},\n+    {file = \"flake8-3.9.2.tar.gz\", hash = \"sha256:07528381786f2a6237b061f6e96610a4167b226cb926e2aa2b6b1d78057c576b\"},\n ]\n google-auth = [\n-    {file = \"google-auth-1.27.0.tar.gz\", hash = \"sha256:da5218cbf33b8461d7661d6b4ad91c12c0107e2767904d5e3ae6408031d5463e\"},\n-    {file = \"google_auth-1.27.0-py2.py3-none-any.whl\", hash = \"sha256:d3640ea61ee025d5af00e3ffd82ba0a06dd99724adaf50bdd52f49daf29f3f65\"},\n+    {file = \"google-auth-2.3.2.tar.gz\", hash = \"sha256:2dc5218ee1192f9d67147cece18f47a929a9ef746cb69c50ab5ff5cfc983647b\"},\n+    {file = \"google_auth-2.3.2-py2.py3-none-any.whl\", hash = \"sha256:6e99f4b3b099feb50de20302f2f8987c1c36e80a3f856ce852675bdf7a0935d3\"},\n ]\n heapdict = [\n     {file = \"HeapDict-1.0.1-py3-none-any.whl\", hash = \"sha256:6065f90933ab1bb7e50db403b90cab653c853690c5992e69294c2de2b253fc92\"},\n     {file = \"HeapDict-1.0.1.tar.gz\", hash = \"sha256:8495f57b3e03d8e46d5f1b2cc62ca881aca392fd5cc048dc0aa2e1a6d23ecdb6\"},\n ]\n idna = [\n-    {file = \"idna-2.10-py2.py3-none-any.whl\", hash = \"sha256:b97d804b1e9b523befed77c48dacec60e6dcb0b5391d57af6a65a312a90648c0\"},\n-    {file = \"idna-2.10.tar.gz\", hash = \"sha256:b307872f855b18632ce0c21c5e45be78c0ea7ae4c15c828c20788b26921eb3f6\"},\n+    {file = \"idna-3.3-py3-none-any.whl\", hash = \"sha256:84d9dd047ffa80596e0f246e2eab0b391788b0503584e8945f2368256d2735ff\"},\n+    {file = \"idna-3.3.tar.gz\", hash = \"sha256:9d643ff0a55b762d5cdb124b8eaa99c66322e2157b69160bc32796e824360e6d\"},\n ]\n importlib-metadata = [\n-    {file = \"importlib_metadata-3.7.0-py3-none-any.whl\", hash = \"sha256:c6af5dbf1126cd959c4a8d8efd61d4d3c83bddb0459a17e554284a077574b614\"},\n-    {file = \"importlib_metadata-3.7.0.tar.gz\", hash = \"sha256:24499ffde1b80be08284100393955842be4a59c7c16bbf2738aad0e464a8e0aa\"},\n+    {file = \"importlib_metadata-4.8.1-py3-none-any.whl\", hash = \"sha256:b618b6d2d5ffa2f16add5697cf57a46c76a56229b0ed1c438322e4e95645bd15\"},\n+    {file = \"importlib_metadata-4.8.1.tar.gz\", hash = \"sha256:f284b3e11256ad1e5d03ab86bb2ccd6f5339688ff17a4d797a0fe7df326f23b1\"},\n ]\n iniconfig = [\n     {file = \"iniconfig-1.1.1-py2.py3-none-any.whl\", hash = \"sha256:011e24c64b7f47f6ebd835bb12a743f2fbe9a26d4cecaa7f53bc4f35ee9da8b3\"},\n     {file = \"iniconfig-1.1.1.tar.gz\", hash = \"sha256:bc3af051d7d14b2ee5ef9969666def0cd1a000e121eaea580d4a313df4b37f32\"},\n ]\n kubernetes = [\n-    {file = \"kubernetes-12.0.1-py2.py3-none-any.whl\", hash = \"sha256:23c85d8571df8f56e773f1a413bc081537536dc47e2b5e8dc2e6262edb2c57ca\"},\n-    {file = \"kubernetes-12.0.1.tar.gz\", hash = \"sha256:ec52ea01d52e2ec3da255992f7e859f3a76f2bdb51cf65ba8cd71dfc309d8daa\"},\n+    {file = \"kubernetes-19.15.0-py2.py3-none-any.whl\", hash = \"sha256:52312adda60d92ba45b325f2c1505924656389222005f7e089718e1ad03bc07f\"},\n+    {file = \"kubernetes-19.15.0.tar.gz\", hash = \"sha256:08c93f300a9837104282ecc81458b903a56444c5c1ec3d990d237557312af47f\"},\n ]\n mako = [\n-    {file = \"Mako-1.1.4.tar.gz\", hash = \"sha256:17831f0b7087c313c0ffae2bcbbd3c1d5ba9eeac9c38f2eb7b50e8c99fe9d5ab\"},\n+    {file = \"Mako-1.1.5-py2.py3-none-any.whl\", hash = \"sha256:6804ee66a7f6a6416910463b00d76a7b25194cd27f1918500c5bd7be2a088a23\"},\n+    {file = \"Mako-1.1.5.tar.gz\", hash = \"sha256:169fa52af22a91900d852e937400e79f535496191c63712e3b9fda5a9bed6fc3\"},\n ]\n markdown = [\n     {file = \"Markdown-3.3.4-py3-none-any.whl\", hash = \"sha256:96c3ba1261de2f7547b46a00ea8463832c921d3f9d6aba3f255a6f71386db20c\"},\n     {file = \"Markdown-3.3.4.tar.gz\", hash = \"sha256:31b5b491868dcc87d6c24b7e3d19a0d730d59d3e46f4eea6430a321bed387a49\"},\n ]\n markupsafe = [\n-    {file = \"MarkupSafe-1.1.1-cp27-cp27m-macosx_10_6_intel.whl\", hash = \"sha256:09027a7803a62ca78792ad89403b1b7a73a01c8cb65909cd876f7fcebd79b161\"},\n-    {file = \"MarkupSafe-1.1.1-cp27-cp27m-manylinux1_i686.whl\", hash = \"sha256:e249096428b3ae81b08327a63a485ad0878de3fb939049038579ac0ef61e17e7\"},\n-    {file = \"MarkupSafe-1.1.1-cp27-cp27m-manylinux1_x86_64.whl\", hash = \"sha256:500d4957e52ddc3351cabf489e79c91c17f6e0899158447047588650b5e69183\"},\n-    {file = \"MarkupSafe-1.1.1-cp27-cp27m-win32.whl\", hash = \"sha256:b2051432115498d3562c084a49bba65d97cf251f5a331c64a12ee7e04dacc51b\"},\n-    {file = \"MarkupSafe-1.1.1-cp27-cp27m-win_amd64.whl\", hash = \"sha256:98c7086708b163d425c67c7a91bad6e466bb99d797aa64f965e9d25c12111a5e\"},\n-    {file = \"MarkupSafe-1.1.1-cp27-cp27mu-manylinux1_i686.whl\", hash = \"sha256:cd5df75523866410809ca100dc9681e301e3c27567cf498077e8551b6d20e42f\"},\n-    {file = \"MarkupSafe-1.1.1-cp27-cp27mu-manylinux1_x86_64.whl\", hash = \"sha256:43a55c2930bbc139570ac2452adf3d70cdbb3cfe5912c71cdce1c2c6bbd9c5d1\"},\n-    {file = \"MarkupSafe-1.1.1-cp34-cp34m-macosx_10_6_intel.whl\", hash = \"sha256:1027c282dad077d0bae18be6794e6b6b8c91d58ed8a8d89a89d59693b9131db5\"},\n-    {file = \"MarkupSafe-1.1.1-cp34-cp34m-manylinux1_i686.whl\", hash = \"sha256:62fe6c95e3ec8a7fad637b7f3d372c15ec1caa01ab47926cfdf7a75b40e0eac1\"},\n-    {file = \"MarkupSafe-1.1.1-cp34-cp34m-manylinux1_x86_64.whl\", hash = \"sha256:88e5fcfb52ee7b911e8bb6d6aa2fd21fbecc674eadd44118a9cc3863f938e735\"},\n-    {file = \"MarkupSafe-1.1.1-cp34-cp34m-win32.whl\", hash = \"sha256:ade5e387d2ad0d7ebf59146cc00c8044acbd863725f887353a10df825fc8ae21\"},\n-    {file = \"MarkupSafe-1.1.1-cp34-cp34m-win_amd64.whl\", hash = \"sha256:09c4b7f37d6c648cb13f9230d847adf22f8171b1ccc4d5682398e77f40309235\"},\n-    {file = \"MarkupSafe-1.1.1-cp35-cp35m-macosx_10_6_intel.whl\", hash = \"sha256:79855e1c5b8da654cf486b830bd42c06e8780cea587384cf6545b7d9ac013a0b\"},\n-    {file = \"MarkupSafe-1.1.1-cp35-cp35m-manylinux1_i686.whl\", hash = \"sha256:c8716a48d94b06bb3b2524c2b77e055fb313aeb4ea620c8dd03a105574ba704f\"},\n-    {file = \"MarkupSafe-1.1.1-cp35-cp35m-manylinux1_x86_64.whl\", hash = \"sha256:7c1699dfe0cf8ff607dbdcc1e9b9af1755371f92a68f706051cc8c37d447c905\"},\n-    {file = \"MarkupSafe-1.1.1-cp35-cp35m-win32.whl\", hash = \"sha256:6dd73240d2af64df90aa7c4e7481e23825ea70af4b4922f8ede5b9e35f78a3b1\"},\n-    {file = \"MarkupSafe-1.1.1-cp35-cp35m-win_amd64.whl\", hash = \"sha256:9add70b36c5666a2ed02b43b335fe19002ee5235efd4b8a89bfcf9005bebac0d\"},\n-    {file = \"MarkupSafe-1.1.1-cp36-cp36m-macosx_10_6_intel.whl\", hash = \"sha256:24982cc2533820871eba85ba648cd53d8623687ff11cbb805be4ff7b4c971aff\"},\n-    {file = \"MarkupSafe-1.1.1-cp36-cp36m-manylinux1_i686.whl\", hash = \"sha256:00bc623926325b26bb9605ae9eae8a215691f33cae5df11ca5424f06f2d1f473\"},\n-    {file = \"MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\", hash = \"sha256:717ba8fe3ae9cc0006d7c451f0bb265ee07739daf76355d06366154ee68d221e\"},\n-    {file = \"MarkupSafe-1.1.1-cp36-cp36m-win32.whl\", hash = \"sha256:535f6fc4d397c1563d08b88e485c3496cf5784e927af890fb3c3aac7f933ec66\"},\n-    {file = \"MarkupSafe-1.1.1-cp36-cp36m-win_amd64.whl\", hash = \"sha256:b1282f8c00509d99fef04d8ba936b156d419be841854fe901d8ae224c59f0be5\"},\n-    {file = \"MarkupSafe-1.1.1-cp37-cp37m-macosx_10_6_intel.whl\", hash = \"sha256:8defac2f2ccd6805ebf65f5eeb132adcf2ab57aa11fdf4c0dd5169a004710e7d\"},\n-    {file = \"MarkupSafe-1.1.1-cp37-cp37m-manylinux1_i686.whl\", hash = \"sha256:46c99d2de99945ec5cb54f23c8cd5689f6d7177305ebff350a58ce5f8de1669e\"},\n-    {file = \"MarkupSafe-1.1.1-cp37-cp37m-manylinux1_x86_64.whl\", hash = \"sha256:ba59edeaa2fc6114428f1637ffff42da1e311e29382d81b339c1817d37ec93c6\"},\n-    {file = \"MarkupSafe-1.1.1-cp37-cp37m-win32.whl\", hash = \"sha256:b00c1de48212e4cc9603895652c5c410df699856a2853135b3967591e4beebc2\"},\n-    {file = \"MarkupSafe-1.1.1-cp37-cp37m-win_amd64.whl\", hash = \"sha256:9bf40443012702a1d2070043cb6291650a0841ece432556f784f004937f0f32c\"},\n-    {file = \"MarkupSafe-1.1.1-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:6788b695d50a51edb699cb55e35487e430fa21f1ed838122d722e0ff0ac5ba15\"},\n-    {file = \"MarkupSafe-1.1.1-cp38-cp38-manylinux1_i686.whl\", hash = \"sha256:cdb132fc825c38e1aeec2c8aa9338310d29d337bebbd7baa06889d09a60a1fa2\"},\n-    {file = \"MarkupSafe-1.1.1-cp38-cp38-manylinux1_x86_64.whl\", hash = \"sha256:13d3144e1e340870b25e7b10b98d779608c02016d5184cfb9927a9f10c689f42\"},\n-    {file = \"MarkupSafe-1.1.1-cp38-cp38-win32.whl\", hash = \"sha256:596510de112c685489095da617b5bcbbac7dd6384aeebeda4df6025d0256a81b\"},\n-    {file = \"MarkupSafe-1.1.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:e8313f01ba26fbbe36c7be1966a7b7424942f670f38e666995b88d012765b9be\"},\n-    {file = \"MarkupSafe-1.1.1.tar.gz\", hash = \"sha256:29872e92839765e546828bb7754a68c418d927cd064fd4708fab9fe9c8bb116b\"},\n+    {file = \"MarkupSafe-2.0.1-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:d8446c54dc28c01e5a2dbac5a25f071f6653e6e40f3a8818e8b45d790fe6ef53\"},\n+    {file = \"MarkupSafe-2.0.1-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:36bc903cbb393720fad60fc28c10de6acf10dc6cc883f3e24ee4012371399a38\"},\n+    {file = \"MarkupSafe-2.0.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:2d7d807855b419fc2ed3e631034685db6079889a1f01d5d9dac950f764da3dad\"},\n+    {file = \"MarkupSafe-2.0.1-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:add36cb2dbb8b736611303cd3bfcee00afd96471b09cda130da3581cbdc56a6d\"},\n+    {file = \"MarkupSafe-2.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:168cd0a3642de83558a5153c8bd34f175a9a6e7f6dc6384b9655d2697312a646\"},\n+    {file = \"MarkupSafe-2.0.1-cp310-cp310-win32.whl\", hash = \"sha256:99df47edb6bda1249d3e80fdabb1dab8c08ef3975f69aed437cb69d0a5de1e28\"},\n+    {file = \"MarkupSafe-2.0.1-cp310-cp310-win_amd64.whl\", hash = \"sha256:e0f138900af21926a02425cf736db95be9f4af72ba1bb21453432a07f6082134\"},\n+    {file = \"MarkupSafe-2.0.1-cp36-cp36m-macosx_10_9_x86_64.whl\", hash = \"sha256:f9081981fe268bd86831e5c75f7de206ef275defcb82bc70740ae6dc507aee51\"},\n+    {file = \"MarkupSafe-2.0.1-cp36-cp36m-manylinux1_i686.whl\", hash = \"sha256:0955295dd5eec6cb6cc2fe1698f4c6d84af2e92de33fbcac4111913cd100a6ff\"},\n+    {file = \"MarkupSafe-2.0.1-cp36-cp36m-manylinux1_x86_64.whl\", hash = \"sha256:0446679737af14f45767963a1a9ef7620189912317d095f2d9ffa183a4d25d2b\"},\n+    {file = \"MarkupSafe-2.0.1-cp36-cp36m-manylinux2010_i686.whl\", hash = \"sha256:f826e31d18b516f653fe296d967d700fddad5901ae07c622bb3705955e1faa94\"},\n+    {file = \"MarkupSafe-2.0.1-cp36-cp36m-manylinux2010_x86_64.whl\", hash = \"sha256:fa130dd50c57d53368c9d59395cb5526eda596d3ffe36666cd81a44d56e48872\"},\n+    {file = \"MarkupSafe-2.0.1-cp36-cp36m-manylinux2014_aarch64.whl\", hash = \"sha256:905fec760bd2fa1388bb5b489ee8ee5f7291d692638ea5f67982d968366bef9f\"},\n+    {file = \"MarkupSafe-2.0.1-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:bf5d821ffabf0ef3533c39c518f3357b171a1651c1ff6827325e4489b0e46c3c\"},\n+    {file = \"MarkupSafe-2.0.1-cp36-cp36m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:0d4b31cc67ab36e3392bbf3862cfbadac3db12bdd8b02a2731f509ed5b829724\"},\n+    {file = \"MarkupSafe-2.0.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:baa1a4e8f868845af802979fcdbf0bb11f94f1cb7ced4c4b8a351bb60d108145\"},\n+    {file = \"MarkupSafe-2.0.1-cp36-cp36m-win32.whl\", hash = \"sha256:6c4ca60fa24e85fe25b912b01e62cb969d69a23a5d5867682dd3e80b5b02581d\"},\n+    {file = \"MarkupSafe-2.0.1-cp36-cp36m-win_amd64.whl\", hash = \"sha256:b2f4bf27480f5e5e8ce285a8c8fd176c0b03e93dcc6646477d4630e83440c6a9\"},\n+    {file = \"MarkupSafe-2.0.1-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:0717a7390a68be14b8c793ba258e075c6f4ca819f15edfc2a3a027c823718567\"},\n+    {file = \"MarkupSafe-2.0.1-cp37-cp37m-manylinux1_i686.whl\", hash = \"sha256:6557b31b5e2c9ddf0de32a691f2312a32f77cd7681d8af66c2692efdbef84c18\"},\n+    {file = \"MarkupSafe-2.0.1-cp37-cp37m-manylinux1_x86_64.whl\", hash = \"sha256:49e3ceeabbfb9d66c3aef5af3a60cc43b85c33df25ce03d0031a608b0a8b2e3f\"},\n+    {file = \"MarkupSafe-2.0.1-cp37-cp37m-manylinux2010_i686.whl\", hash = \"sha256:d7f9850398e85aba693bb640262d3611788b1f29a79f0c93c565694658f4071f\"},\n+    {file = \"MarkupSafe-2.0.1-cp37-cp37m-manylinux2010_x86_64.whl\", hash = \"sha256:6a7fae0dd14cf60ad5ff42baa2e95727c3d81ded453457771d02b7d2b3f9c0c2\"},\n+    {file = \"MarkupSafe-2.0.1-cp37-cp37m-manylinux2014_aarch64.whl\", hash = \"sha256:b7f2d075102dc8c794cbde1947378051c4e5180d52d276987b8d28a3bd58c17d\"},\n+    {file = \"MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:e9936f0b261d4df76ad22f8fee3ae83b60d7c3e871292cd42f40b81b70afae85\"},\n+    {file = \"MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:2a7d351cbd8cfeb19ca00de495e224dea7e7d919659c2841bbb7f420ad03e2d6\"},\n+    {file = \"MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:60bf42e36abfaf9aff1f50f52644b336d4f0a3fd6d8a60ca0d054ac9f713a864\"},\n+    {file = \"MarkupSafe-2.0.1-cp37-cp37m-win32.whl\", hash = \"sha256:a30e67a65b53ea0a5e62fe23682cfe22712e01f453b95233b25502f7c61cb415\"},\n+    {file = \"MarkupSafe-2.0.1-cp37-cp37m-win_amd64.whl\", hash = \"sha256:611d1ad9a4288cf3e3c16014564df047fe08410e628f89805e475368bd304914\"},\n+    {file = \"MarkupSafe-2.0.1-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:5bb28c636d87e840583ee3adeb78172efc47c8b26127267f54a9c0ec251d41a9\"},\n+    {file = \"MarkupSafe-2.0.1-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:be98f628055368795d818ebf93da628541e10b75b41c559fdf36d104c5787066\"},\n+    {file = \"MarkupSafe-2.0.1-cp38-cp38-manylinux1_i686.whl\", hash = \"sha256:1d609f577dc6e1aa17d746f8bd3c31aa4d258f4070d61b2aa5c4166c1539de35\"},\n+    {file = \"MarkupSafe-2.0.1-cp38-cp38-manylinux1_x86_64.whl\", hash = \"sha256:7d91275b0245b1da4d4cfa07e0faedd5b0812efc15b702576d103293e252af1b\"},\n+    {file = \"MarkupSafe-2.0.1-cp38-cp38-manylinux2010_i686.whl\", hash = \"sha256:01a9b8ea66f1658938f65b93a85ebe8bc016e6769611be228d797c9d998dd298\"},\n+    {file = \"MarkupSafe-2.0.1-cp38-cp38-manylinux2010_x86_64.whl\", hash = \"sha256:47ab1e7b91c098ab893b828deafa1203de86d0bc6ab587b160f78fe6c4011f75\"},\n+    {file = \"MarkupSafe-2.0.1-cp38-cp38-manylinux2014_aarch64.whl\", hash = \"sha256:97383d78eb34da7e1fa37dd273c20ad4320929af65d156e35a5e2d89566d9dfb\"},\n+    {file = \"MarkupSafe-2.0.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:6fcf051089389abe060c9cd7caa212c707e58153afa2c649f00346ce6d260f1b\"},\n+    {file = \"MarkupSafe-2.0.1-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:5855f8438a7d1d458206a2466bf82b0f104a3724bf96a1c781ab731e4201731a\"},\n+    {file = \"MarkupSafe-2.0.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:3dd007d54ee88b46be476e293f48c85048603f5f516008bee124ddd891398ed6\"},\n+    {file = \"MarkupSafe-2.0.1-cp38-cp38-win32.whl\", hash = \"sha256:023cb26ec21ece8dc3907c0e8320058b2e0cb3c55cf9564da612bc325bed5e64\"},\n+    {file = \"MarkupSafe-2.0.1-cp38-cp38-win_amd64.whl\", hash = \"sha256:984d76483eb32f1bcb536dc27e4ad56bba4baa70be32fa87152832cdd9db0833\"},\n+    {file = \"MarkupSafe-2.0.1-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:2ef54abee730b502252bcdf31b10dacb0a416229b72c18b19e24a4509f273d26\"},\n+    {file = \"MarkupSafe-2.0.1-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:3c112550557578c26af18a1ccc9e090bfe03832ae994343cfdacd287db6a6ae7\"},\n+    {file = \"MarkupSafe-2.0.1-cp39-cp39-manylinux1_i686.whl\", hash = \"sha256:53edb4da6925ad13c07b6d26c2a852bd81e364f95301c66e930ab2aef5b5ddd8\"},\n+    {file = \"MarkupSafe-2.0.1-cp39-cp39-manylinux1_x86_64.whl\", hash = \"sha256:f5653a225f31e113b152e56f154ccbe59eeb1c7487b39b9d9f9cdb58e6c79dc5\"},\n+    {file = \"MarkupSafe-2.0.1-cp39-cp39-manylinux2010_i686.whl\", hash = \"sha256:4efca8f86c54b22348a5467704e3fec767b2db12fc39c6d963168ab1d3fc9135\"},\n+    {file = \"MarkupSafe-2.0.1-cp39-cp39-manylinux2010_x86_64.whl\", hash = \"sha256:ab3ef638ace319fa26553db0624c4699e31a28bb2a835c5faca8f8acf6a5a902\"},\n+    {file = \"MarkupSafe-2.0.1-cp39-cp39-manylinux2014_aarch64.whl\", hash = \"sha256:f8ba0e8349a38d3001fae7eadded3f6606f0da5d748ee53cc1dab1d6527b9509\"},\n+    {file = \"MarkupSafe-2.0.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:c47adbc92fc1bb2b3274c4b3a43ae0e4573d9fbff4f54cd484555edbf030baf1\"},\n+    {file = \"MarkupSafe-2.0.1-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:37205cac2a79194e3750b0af2a5720d95f786a55ce7df90c3af697bfa100eaac\"},\n+    {file = \"MarkupSafe-2.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:1f2ade76b9903f39aa442b4aadd2177decb66525062db244b35d71d0ee8599b6\"},\n+    {file = \"MarkupSafe-2.0.1-cp39-cp39-win32.whl\", hash = \"sha256:10f82115e21dc0dfec9ab5c0223652f7197feb168c940f3ef61563fc2d6beb74\"},\n+    {file = \"MarkupSafe-2.0.1-cp39-cp39-win_amd64.whl\", hash = \"sha256:693ce3f9e70a6cf7d2fb9e6c9d8b204b6b39897a2c4a1aa65728d5ac97dcc1d8\"},\n+    {file = \"MarkupSafe-2.0.1.tar.gz\", hash = \"sha256:594c67807fb16238b30c44bdf74f36c02cdf22d1c8cda91ef8a0ed8dabf5620a\"},\n ]\n mccabe = [\n     {file = \"mccabe-0.6.1-py2.py3-none-any.whl\", hash = \"sha256:ab8a6258860da4b6677da4bd2fe5dc2c659cff31b3ee4f7f5d64e79735b80d42\"},\n@@ -1007,62 +1045,97 @@ msgpack = [\n     {file = \"msgpack-1.0.2.tar.gz\", hash = \"sha256:fae04496f5bc150eefad4e9571d1a76c55d021325dcd484ce45065ebbdd00984\"},\n ]\n multidict = [\n-    {file = \"multidict-5.1.0-cp36-cp36m-macosx_10_14_x86_64.whl\", hash = \"sha256:b7993704f1a4b204e71debe6095150d43b2ee6150fa4f44d6d966ec356a8d61f\"},\n-    {file = \"multidict-5.1.0-cp36-cp36m-manylinux1_i686.whl\", hash = \"sha256:9dd6e9b1a913d096ac95d0399bd737e00f2af1e1594a787e00f7975778c8b2bf\"},\n-    {file = \"multidict-5.1.0-cp36-cp36m-manylinux2014_aarch64.whl\", hash = \"sha256:f21756997ad8ef815d8ef3d34edd98804ab5ea337feedcd62fb52d22bf531281\"},\n-    {file = \"multidict-5.1.0-cp36-cp36m-manylinux2014_i686.whl\", hash = \"sha256:1ab820665e67373de5802acae069a6a05567ae234ddb129f31d290fc3d1aa56d\"},\n-    {file = \"multidict-5.1.0-cp36-cp36m-manylinux2014_ppc64le.whl\", hash = \"sha256:9436dc58c123f07b230383083855593550c4d301d2532045a17ccf6eca505f6d\"},\n-    {file = \"multidict-5.1.0-cp36-cp36m-manylinux2014_s390x.whl\", hash = \"sha256:830f57206cc96ed0ccf68304141fec9481a096c4d2e2831f311bde1c404401da\"},\n-    {file = \"multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl\", hash = \"sha256:2e68965192c4ea61fff1b81c14ff712fc7dc15d2bd120602e4a3494ea6584224\"},\n-    {file = \"multidict-5.1.0-cp36-cp36m-win32.whl\", hash = \"sha256:2f1a132f1c88724674271d636e6b7351477c27722f2ed789f719f9e3545a3d26\"},\n-    {file = \"multidict-5.1.0-cp36-cp36m-win_amd64.whl\", hash = \"sha256:3a4f32116f8f72ecf2a29dabfb27b23ab7cdc0ba807e8459e59a93a9be9506f6\"},\n-    {file = \"multidict-5.1.0-cp37-cp37m-macosx_10_14_x86_64.whl\", hash = \"sha256:46c73e09ad374a6d876c599f2328161bcd95e280f84d2060cf57991dec5cfe76\"},\n-    {file = \"multidict-5.1.0-cp37-cp37m-manylinux1_i686.whl\", hash = \"sha256:018132dbd8688c7a69ad89c4a3f39ea2f9f33302ebe567a879da8f4ca73f0d0a\"},\n-    {file = \"multidict-5.1.0-cp37-cp37m-manylinux2014_aarch64.whl\", hash = \"sha256:4b186eb7d6ae7c06eb4392411189469e6a820da81447f46c0072a41c748ab73f\"},\n-    {file = \"multidict-5.1.0-cp37-cp37m-manylinux2014_i686.whl\", hash = \"sha256:3a041b76d13706b7fff23b9fc83117c7b8fe8d5fe9e6be45eee72b9baa75f348\"},\n-    {file = \"multidict-5.1.0-cp37-cp37m-manylinux2014_ppc64le.whl\", hash = \"sha256:051012ccee979b2b06be928a6150d237aec75dd6bf2d1eeeb190baf2b05abc93\"},\n-    {file = \"multidict-5.1.0-cp37-cp37m-manylinux2014_s390x.whl\", hash = \"sha256:6a4d5ce640e37b0efcc8441caeea8f43a06addace2335bd11151bc02d2ee31f9\"},\n-    {file = \"multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl\", hash = \"sha256:5cf3443199b83ed9e955f511b5b241fd3ae004e3cb81c58ec10f4fe47c7dce37\"},\n-    {file = \"multidict-5.1.0-cp37-cp37m-win32.whl\", hash = \"sha256:f200755768dc19c6f4e2b672421e0ebb3dd54c38d5a4f262b872d8cfcc9e93b5\"},\n-    {file = \"multidict-5.1.0-cp37-cp37m-win_amd64.whl\", hash = \"sha256:05c20b68e512166fddba59a918773ba002fdd77800cad9f55b59790030bab632\"},\n-    {file = \"multidict-5.1.0-cp38-cp38-macosx_10_14_x86_64.whl\", hash = \"sha256:54fd1e83a184e19c598d5e70ba508196fd0bbdd676ce159feb412a4a6664f952\"},\n-    {file = \"multidict-5.1.0-cp38-cp38-manylinux1_i686.whl\", hash = \"sha256:0e3c84e6c67eba89c2dbcee08504ba8644ab4284863452450520dad8f1e89b79\"},\n-    {file = \"multidict-5.1.0-cp38-cp38-manylinux2014_aarch64.whl\", hash = \"sha256:dc862056f76443a0db4509116c5cd480fe1b6a2d45512a653f9a855cc0517456\"},\n-    {file = \"multidict-5.1.0-cp38-cp38-manylinux2014_i686.whl\", hash = \"sha256:0e929169f9c090dae0646a011c8b058e5e5fb391466016b39d21745b48817fd7\"},\n-    {file = \"multidict-5.1.0-cp38-cp38-manylinux2014_ppc64le.whl\", hash = \"sha256:d81eddcb12d608cc08081fa88d046c78afb1bf8107e6feab5d43503fea74a635\"},\n-    {file = \"multidict-5.1.0-cp38-cp38-manylinux2014_s390x.whl\", hash = \"sha256:585fd452dd7782130d112f7ddf3473ffdd521414674c33876187e101b588738a\"},\n-    {file = \"multidict-5.1.0-cp38-cp38-manylinux2014_x86_64.whl\", hash = \"sha256:37e5438e1c78931df5d3c0c78ae049092877e5e9c02dd1ff5abb9cf27a5914ea\"},\n-    {file = \"multidict-5.1.0-cp38-cp38-win32.whl\", hash = \"sha256:07b42215124aedecc6083f1ce6b7e5ec5b50047afa701f3442054373a6deb656\"},\n-    {file = \"multidict-5.1.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:929006d3c2d923788ba153ad0de8ed2e5ed39fdbe8e7be21e2f22ed06c6783d3\"},\n-    {file = \"multidict-5.1.0-cp39-cp39-macosx_10_14_x86_64.whl\", hash = \"sha256:b797515be8743b771aa868f83563f789bbd4b236659ba52243b735d80b29ed93\"},\n-    {file = \"multidict-5.1.0-cp39-cp39-manylinux1_i686.whl\", hash = \"sha256:d5c65bdf4484872c4af3150aeebe101ba560dcfb34488d9a8ff8dbcd21079647\"},\n-    {file = \"multidict-5.1.0-cp39-cp39-manylinux2014_aarch64.whl\", hash = \"sha256:b47a43177a5e65b771b80db71e7be76c0ba23cc8aa73eeeb089ed5219cdbe27d\"},\n-    {file = \"multidict-5.1.0-cp39-cp39-manylinux2014_i686.whl\", hash = \"sha256:806068d4f86cb06af37cd65821554f98240a19ce646d3cd24e1c33587f313eb8\"},\n-    {file = \"multidict-5.1.0-cp39-cp39-manylinux2014_ppc64le.whl\", hash = \"sha256:46dd362c2f045095c920162e9307de5ffd0a1bfbba0a6e990b344366f55a30c1\"},\n-    {file = \"multidict-5.1.0-cp39-cp39-manylinux2014_s390x.whl\", hash = \"sha256:ace010325c787c378afd7f7c1ac66b26313b3344628652eacd149bdd23c68841\"},\n-    {file = \"multidict-5.1.0-cp39-cp39-manylinux2014_x86_64.whl\", hash = \"sha256:ecc771ab628ea281517e24fd2c52e8f31c41e66652d07599ad8818abaad38cda\"},\n-    {file = \"multidict-5.1.0-cp39-cp39-win32.whl\", hash = \"sha256:fc13a9524bc18b6fb6e0dbec3533ba0496bbed167c56d0aabefd965584557d80\"},\n-    {file = \"multidict-5.1.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:7df80d07818b385f3129180369079bd6934cf70469f99daaebfac89dca288359\"},\n-    {file = \"multidict-5.1.0.tar.gz\", hash = \"sha256:25b4e5f22d3a37ddf3effc0710ba692cfc792c2b9edfb9c05aefe823256e84d5\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:3822c5894c72e3b35aae9909bef66ec83e44522faf767c0ad39e0e2de11d3b55\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:28e6d883acd8674887d7edc896b91751dc2d8e87fbdca8359591a13872799e4e\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:b61f85101ef08cbbc37846ac0e43f027f7844f3fade9b7f6dd087178caedeee7\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:d9b668c065968c5979fe6b6fa6760bb6ab9aeb94b75b73c0a9c1acf6393ac3bf\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:517d75522b7b18a3385726b54a081afd425d4f41144a5399e5abd97ccafdf36b\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:1b4ac3ba7a97b35a5ccf34f41b5a8642a01d1e55454b699e5e8e7a99b5a3acf5\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:df23c83398715b26ab09574217ca21e14694917a0c857e356fd39e1c64f8283f\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:e58a9b5cc96e014ddf93c2227cbdeca94b56a7eb77300205d6e4001805391747\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:f76440e480c3b2ca7f843ff8a48dc82446b86ed4930552d736c0bac507498a52\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:cfde464ca4af42a629648c0b0d79b8f295cf5b695412451716531d6916461628\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:0fed465af2e0eb6357ba95795d003ac0bdb546305cc2366b1fc8f0ad67cc3fda\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:b70913cbf2e14275013be98a06ef4b412329fe7b4f83d64eb70dce8269ed1e1a\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:a5635bcf1b75f0f6ef3c8a1ad07b500104a971e38d3683167b9454cb6465ac86\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-win32.whl\", hash = \"sha256:77f0fb7200cc7dedda7a60912f2059086e29ff67cefbc58d2506638c1a9132d7\"},\n+    {file = \"multidict-5.2.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:9416cf11bcd73c861267e88aea71e9fcc35302b3943e45e1dbb4317f91a4b34f\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-macosx_10_9_x86_64.whl\", hash = \"sha256:fd77c8f3cba815aa69cb97ee2b2ef385c7c12ada9c734b0f3b32e26bb88bbf1d\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:98ec9aea6223adf46999f22e2c0ab6cf33f5914be604a404f658386a8f1fba37\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:e5283c0a00f48e8cafcecadebfa0ed1dac8b39e295c7248c44c665c16dc1138b\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:5f79c19c6420962eb17c7e48878a03053b7ccd7b69f389d5831c0a4a7f1ac0a1\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:e4a67f1080123de76e4e97a18d10350df6a7182e243312426d508712e99988d4\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:94b117e27efd8e08b4046c57461d5a114d26b40824995a2eb58372b94f9fca02\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-musllinux_1_1_aarch64.whl\", hash = \"sha256:2e77282fd1d677c313ffcaddfec236bf23f273c4fba7cdf198108f5940ae10f5\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-musllinux_1_1_i686.whl\", hash = \"sha256:116347c63ba049c1ea56e157fa8aa6edaf5e92925c9b64f3da7769bdfa012858\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:dc3a866cf6c13d59a01878cd806f219340f3e82eed514485e094321f24900677\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-musllinux_1_1_s390x.whl\", hash = \"sha256:ac42181292099d91217a82e3fa3ce0e0ddf3a74fd891b7c2b347a7f5aa0edded\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-musllinux_1_1_x86_64.whl\", hash = \"sha256:f0bb0973f42ffcb5e3537548e0767079420aefd94ba990b61cf7bb8d47f4916d\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-win32.whl\", hash = \"sha256:ea21d4d5104b4f840b91d9dc8cbc832aba9612121eaba503e54eaab1ad140eb9\"},\n+    {file = \"multidict-5.2.0-cp36-cp36m-win_amd64.whl\", hash = \"sha256:e6453f3cbeb78440747096f239d282cc57a2997a16b5197c9bc839099e1633d0\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:d3def943bfd5f1c47d51fd324df1e806d8da1f8e105cc7f1c76a1daf0f7e17b0\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:35591729668a303a02b06e8dba0eb8140c4a1bfd4c4b3209a436a02a5ac1de11\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:ce8cacda0b679ebc25624d5de66c705bc53dcc7c6f02a7fb0f3ca5e227d80422\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:baf1856fab8212bf35230c019cde7c641887e3fc08cadd39d32a421a30151ea3\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:a43616aec0f0d53c411582c451f5d3e1123a68cc7b3475d6f7d97a626f8ff90d\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:25cbd39a9029b409167aa0a20d8a17f502d43f2efebfe9e3ac019fe6796c59ac\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:0a2cbcfbea6dc776782a444db819c8b78afe4db597211298dd8b2222f73e9cd0\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:3d2d7d1fff8e09d99354c04c3fd5b560fb04639fd45926b34e27cfdec678a704\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:a37e9a68349f6abe24130846e2f1d2e38f7ddab30b81b754e5a1fde32f782b23\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-musllinux_1_1_s390x.whl\", hash = \"sha256:637c1896497ff19e1ee27c1c2c2ddaa9f2d134bbb5e0c52254361ea20486418d\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:9815765f9dcda04921ba467957be543423e5ec6a1136135d84f2ae092c50d87b\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-win32.whl\", hash = \"sha256:8b911d74acdc1fe2941e59b4f1a278a330e9c34c6c8ca1ee21264c51ec9b67ef\"},\n+    {file = \"multidict-5.2.0-cp37-cp37m-win_amd64.whl\", hash = \"sha256:380b868f55f63d048a25931a1632818f90e4be71d2081c2338fcf656d299949a\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:e7d81ce5744757d2f05fc41896e3b2ae0458464b14b5a2c1e87a6a9d69aefaa8\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:2d1d55cdf706ddc62822d394d1df53573d32a7a07d4f099470d3cb9323b721b6\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:a4771d0d0ac9d9fe9e24e33bed482a13dfc1256d008d101485fe460359476065\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:da7d57ea65744d249427793c042094c4016789eb2562576fb831870f9c878d9e\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:cdd68778f96216596218b4e8882944d24a634d984ee1a5a049b300377878fa7c\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:ecc99bce8ee42dcad15848c7885197d26841cb24fa2ee6e89d23b8993c871c64\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:067150fad08e6f2dd91a650c7a49ba65085303fcc3decbd64a57dc13a2733031\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:78c106b2b506b4d895ddc801ff509f941119394b89c9115580014127414e6c2d\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:e6c4fa1ec16e01e292315ba76eb1d012c025b99d22896bd14a66628b245e3e01\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:b227345e4186809d31f22087d0265655114af7cda442ecaf72246275865bebe4\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:06560fbdcf22c9387100979e65b26fba0816c162b888cb65b845d3def7a54c9b\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:7878b61c867fb2df7a95e44b316f88d5a3742390c99dfba6c557a21b30180cac\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:246145bff76cc4b19310f0ad28bd0769b940c2a49fc601b86bfd150cbd72bb22\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-win32.whl\", hash = \"sha256:c30ac9f562106cd9e8071c23949a067b10211917fdcb75b4718cf5775356a940\"},\n+    {file = \"multidict-5.2.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:f19001e790013ed580abfde2a4465388950728861b52f0da73e8e8a9418533c0\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:c1ff762e2ee126e6f1258650ac641e2b8e1f3d927a925aafcfde943b77a36d24\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:bd6c9c50bf2ad3f0448edaa1a3b55b2e6866ef8feca5d8dbec10ec7c94371d21\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:fc66d4016f6e50ed36fb39cd287a3878ffcebfa90008535c62e0e90a7ab713ae\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:a9acb76d5f3dd9421874923da2ed1e76041cb51b9337fd7f507edde1d86535d6\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:dfc924a7e946dd3c6360e50e8f750d51e3ef5395c95dc054bc9eab0f70df4f9c\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:32fdba7333eb2351fee2596b756d730d62b5827d5e1ab2f84e6cbb287cc67fe0\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:b9aad49466b8d828b96b9e3630006234879c8d3e2b0a9d99219b3121bc5cdb17\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:93de39267c4c676c9ebb2057e98a8138bade0d806aad4d864322eee0803140a0\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:f9bef5cff994ca3026fcc90680e326d1a19df9841c5e3d224076407cc21471a1\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:5f841c4f14331fd1e36cbf3336ed7be2cb2a8f110ce40ea253e5573387db7621\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:38ba256ee9b310da6a1a0f013ef4e422fca30a685bcbec86a969bd520504e341\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:3bc3b1621b979621cee9f7b09f024ec76ec03cc365e638126a056317470bde1b\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:6ee908c070020d682e9b42c8f621e8bb10c767d04416e2ebe44e37d0f44d9ad5\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-win32.whl\", hash = \"sha256:1c7976cd1c157fa7ba5456ae5d31ccdf1479680dc9b8d8aa28afabc370df42b8\"},\n+    {file = \"multidict-5.2.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:c9631c642e08b9fff1c6255487e62971d8b8e821808ddd013d8ac058087591ac\"},\n+    {file = \"multidict-5.2.0.tar.gz\", hash = \"sha256:0dd1c93edb444b33ba2274b66f63def8a327d607c6c790772f448a53b6ea59ce\"},\n ]\n nest-asyncio = [\n     {file = \"nest_asyncio-1.5.1-py3-none-any.whl\", hash = \"sha256:76d6e972265063fe92a90b9cc4fb82616e07d586b346ed9d2c89a4187acea39c\"},\n     {file = \"nest_asyncio-1.5.1.tar.gz\", hash = \"sha256:afc5a1c515210a23c461932765691ad39e8eba6551c055ac8d5546e69250d0aa\"},\n ]\n oauthlib = [\n-    {file = \"oauthlib-3.1.0-py2.py3-none-any.whl\", hash = \"sha256:df884cd6cbe20e32633f1db1072e9356f53638e4361bef4e8b03c9127c9328ea\"},\n-    {file = \"oauthlib-3.1.0.tar.gz\", hash = \"sha256:bee41cc35fcca6e988463cacc3bcb8a96224f470ca547e697b604cc697b2f889\"},\n+    {file = \"oauthlib-3.1.1-py2.py3-none-any.whl\", hash = \"sha256:42bf6354c2ed8c6acb54d971fce6f88193d97297e18602a3a886603f9d7730cc\"},\n+    {file = \"oauthlib-3.1.1.tar.gz\", hash = \"sha256:8f0215fcc533dd8dd1bee6f4c412d4f0cd7297307d43ac61666389e3bc3198a3\"},\n ]\n packaging = [\n-    {file = \"packaging-20.9-py2.py3-none-any.whl\", hash = \"sha256:67714da7f7bc052e064859c05c595155bd1ee9f69f76557e21f051443c20947a\"},\n-    {file = \"packaging-20.9.tar.gz\", hash = \"sha256:5b327ac1320dc863dca72f4514ecc086f31186744b84a230374cc1fd776feae5\"},\n+    {file = \"packaging-21.2-py3-none-any.whl\", hash = \"sha256:14317396d1e8cdb122989b916fa2c7e9ca8e2be9e8060a6eff75b6b7b4d8a7e0\"},\n+    {file = \"packaging-21.2.tar.gz\", hash = \"sha256:096d689d78ca690e4cd8a89568ba06d07ca097e3306a4381635073ca91479966\"},\n ]\n pdoc3 = [\n     {file = \"pdoc3-0.9.2.tar.gz\", hash = \"sha256:9df5d931f25f353c69c46819a3bd03ef96dd286f2a70bb1b93a23a781f91faa1\"},\n ]\n pluggy = [\n-    {file = \"pluggy-0.13.1-py2.py3-none-any.whl\", hash = \"sha256:966c145cd83c96502c3c3868f50408687b38434af77734af1e9ca461a4081d2d\"},\n-    {file = \"pluggy-0.13.1.tar.gz\", hash = \"sha256:15b2acde666561e1298d71b523007ed7364de07029219b604cf808bfa1c765b0\"},\n+    {file = \"pluggy-1.0.0-py2.py3-none-any.whl\", hash = \"sha256:74134bbf457f031a36d68416e1509f34bd5ccc019f0bcc952c7b909d06b37bd3\"},\n+    {file = \"pluggy-1.0.0.tar.gz\", hash = \"sha256:4224373bacce55f955a878bf9cfa763c1e360858e330072059e10bad68531159\"},\n ]\n psutil = [\n     {file = \"psutil-5.8.0-cp27-cp27m-macosx_10_9_x86_64.whl\", hash = \"sha256:0066a82f7b1b37d334e68697faba68e5ad5e858279fd6351c8ca6024e8d6ba64\"},\n@@ -1129,31 +1202,31 @@ pyasn1-modules = [\n     {file = \"pyasn1_modules-0.2.8-py3.7.egg\", hash = \"sha256:c29a5e5cc7a3f05926aff34e097e84f8589cd790ce0ed41b67aed6857b26aafd\"},\n ]\n pycodestyle = [\n-    {file = \"pycodestyle-2.6.0-py2.py3-none-any.whl\", hash = \"sha256:2295e7b2f6b5bd100585ebcb1f616591b652db8a741695b3d8f5d28bdc934367\"},\n-    {file = \"pycodestyle-2.6.0.tar.gz\", hash = \"sha256:c58a7d2815e0e8d7972bf1803331fb0152f867bd89adf8a01dfd55085434192e\"},\n+    {file = \"pycodestyle-2.7.0-py2.py3-none-any.whl\", hash = \"sha256:514f76d918fcc0b55c6680472f0a37970994e07bbb80725808c17089be302068\"},\n+    {file = \"pycodestyle-2.7.0.tar.gz\", hash = \"sha256:c389c1d06bf7904078ca03399a4816f974a1d590090fecea0c63ec26ebaf1cef\"},\n ]\n pyflakes = [\n-    {file = \"pyflakes-2.2.0-py2.py3-none-any.whl\", hash = \"sha256:0d94e0e05a19e57a99444b6ddcf9a6eb2e5c68d3ca1e98e90707af8152c90a92\"},\n-    {file = \"pyflakes-2.2.0.tar.gz\", hash = \"sha256:35b2d75ee967ea93b55750aa9edbbf72813e06a66ba54438df2cfac9e3c27fc8\"},\n+    {file = \"pyflakes-2.3.1-py2.py3-none-any.whl\", hash = \"sha256:7893783d01b8a89811dd72d7dfd4d84ff098e5eed95cfa8905b22bbffe52efc3\"},\n+    {file = \"pyflakes-2.3.1.tar.gz\", hash = \"sha256:f5bc8ecabc05bb9d291eb5203d6810b49040f6ff446a756326104746cc00c1db\"},\n ]\n pyparsing = [\n     {file = \"pyparsing-2.4.7-py2.py3-none-any.whl\", hash = \"sha256:ef9d7589ef3c200abe66653d3f1ab1033c3c419ae9b9bdb1240a85b024efc88b\"},\n     {file = \"pyparsing-2.4.7.tar.gz\", hash = \"sha256:c203ec8783bf771a155b207279b9bccb8dea02d8f0c9e5f8ead507bc3246ecc1\"},\n ]\n pytest = [\n-    {file = \"pytest-6.2.2-py3-none-any.whl\", hash = \"sha256:b574b57423e818210672e07ca1fa90aaf194a4f63f3ab909a2c67ebb22913839\"},\n-    {file = \"pytest-6.2.2.tar.gz\", hash = \"sha256:9d1edf9e7d0b84d72ea3dbcdfd22b35fb543a5e8f2a60092dd578936bf63d7f9\"},\n+    {file = \"pytest-6.2.5-py3-none-any.whl\", hash = \"sha256:7310f8d27bc79ced999e760ca304d69f6ba6c6649c0b60fb0e04a4a77cacc134\"},\n+    {file = \"pytest-6.2.5.tar.gz\", hash = \"sha256:131b36680866a76e6781d13f101efb86cf674ebb9762eb70d3082b6f29889e89\"},\n ]\n pytest-cov = [\n-    {file = \"pytest-cov-2.11.1.tar.gz\", hash = \"sha256:359952d9d39b9f822d9d29324483e7ba04a3a17dd7d05aa6beb7ea01e359e5f7\"},\n-    {file = \"pytest_cov-2.11.1-py2.py3-none-any.whl\", hash = \"sha256:bdb9fdb0b85a7cc825269a4c56b48ccaa5c7e365054b6038772c32ddcdc969da\"},\n+    {file = \"pytest-cov-2.12.1.tar.gz\", hash = \"sha256:261ceeb8c227b726249b376b8526b600f38667ee314f910353fa318caa01f4d7\"},\n+    {file = \"pytest_cov-2.12.1-py2.py3-none-any.whl\", hash = \"sha256:261bb9e47e65bd099c89c3edf92972865210c36813f80ede5277dceb77a4a62a\"},\n ]\n pytest-sugar = [\n     {file = \"pytest-sugar-0.9.4.tar.gz\", hash = \"sha256:b1b2186b0a72aada6859bea2a5764145e3aaa2c1cfbb23c3a19b5f7b697563d3\"},\n ]\n python-dateutil = [\n-    {file = \"python-dateutil-2.8.1.tar.gz\", hash = \"sha256:73ebfe9dbf22e832286dafa60473e4cd239f8592f699aa5adaf10050e6e1823c\"},\n-    {file = \"python_dateutil-2.8.1-py2.py3-none-any.whl\", hash = \"sha256:75bb3f31ea686f1197762692a9ee6a7550b59fc6ca3a1f4b5d7e32fb98e2da2a\"},\n+    {file = \"python-dateutil-2.8.2.tar.gz\", hash = \"sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86\"},\n+    {file = \"python_dateutil-2.8.2-py2.py3-none-any.whl\", hash = \"sha256:961d03dc3453ebbc59dbdea9e4e11c5651520a876d0f4db161e8674aae935da9\"},\n ]\n python-dotenv = [\n     {file = \"python-dotenv-0.15.0.tar.gz\", hash = \"sha256:587825ed60b1711daea4832cf37524dfd404325b7db5e25ebe88c495c9f807a0\"},\n@@ -1197,8 +1270,8 @@ pyyaml = [\n     {file = \"PyYAML-5.4.1.tar.gz\", hash = \"sha256:607774cbba28732bfa802b54baa7484215f530991055bb562efbed5b2f20a45e\"},\n ]\n requests = [\n-    {file = \"requests-2.25.1-py2.py3-none-any.whl\", hash = \"sha256:c210084e36a42ae6b9219e00e48287def368a26d03a048ddad7bfee44f75871e\"},\n-    {file = \"requests-2.25.1.tar.gz\", hash = \"sha256:27973dd4a904a4f13b263a19c866c13b92a39ed1c964655f025f3f8d3d75b804\"},\n+    {file = \"requests-2.26.0-py2.py3-none-any.whl\", hash = \"sha256:6c1246513ecd5ecd4528a0906f910e8f0f9c6b8ec72030dc9fd154dc1a6efd24\"},\n+    {file = \"requests-2.26.0.tar.gz\", hash = \"sha256:b8aa58f8cf793ffd8782d3d8cb19e66ef36f7aba4353eec859e74678b01b07a7\"},\n ]\n requests-oauthlib = [\n     {file = \"requests-oauthlib-1.3.0.tar.gz\", hash = \"sha256:b4261601a71fd721a8bd6d7aa1cc1d6a8a93b4a9f5e96626f8e4d91e8beeaa6a\"},\n@@ -1210,15 +1283,15 @@ rsa = [\n     {file = \"rsa-4.7.2.tar.gz\", hash = \"sha256:9d689e6ca1b3038bc82bf8d23e944b6b6037bc02301a574935b2dd946e0353b9\"},\n ]\n six = [\n-    {file = \"six-1.15.0-py2.py3-none-any.whl\", hash = \"sha256:8b74bedcbbbaca38ff6d7491d76f2b06b3592611af620f8426e82dddb04a5ced\"},\n-    {file = \"six-1.15.0.tar.gz\", hash = \"sha256:30639c035cdb23534cd4aa2dd52c3bf48f06e5f4a941509c8bafd8ce11080259\"},\n+    {file = \"six-1.16.0-py2.py3-none-any.whl\", hash = \"sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254\"},\n+    {file = \"six-1.16.0.tar.gz\", hash = \"sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926\"},\n ]\n sortedcontainers = [\n-    {file = \"sortedcontainers-2.3.0-py2.py3-none-any.whl\", hash = \"sha256:37257a32add0a3ee490bb170b599e93095eed89a55da91fa9f48753ea12fd73f\"},\n-    {file = \"sortedcontainers-2.3.0.tar.gz\", hash = \"sha256:59cc937650cf60d677c16775597c89a960658a09cf7c1a668f86e1e4464b10a1\"},\n+    {file = \"sortedcontainers-2.4.0-py2.py3-none-any.whl\", hash = \"sha256:a163dcaede0f1c021485e957a39245190e74249897e2ae4b2aa38595db237ee0\"},\n+    {file = \"sortedcontainers-2.4.0.tar.gz\", hash = \"sha256:25caa5a06cc30b6b83d11423433f65d1f9d76c4c6a0c90e3379eaa43b9bfdb88\"},\n ]\n sty = [\n-    {file = \"sty-1.0.0rc1-py3-none-any.whl\", hash = \"sha256:52bc299ba08a7e2683ed4500a13222d62811ad616e621c9d3e3da2d8175ae46c\"},\n+    {file = \"sty-1.0.0rc2-py3-none-any.whl\", hash = \"sha256:d811703ff450170b08fe4410caf052d6139df8d9e1d8f35694c212e14ffe5851\"},\n ]\n tblib = [\n     {file = \"tblib-1.7.0-py2.py3-none-any.whl\", hash = \"sha256:289fa7359e580950e7d9743eab36b0691f0310fce64dee7d9c31065b8f723e23\"},\n@@ -1279,62 +1352,97 @@ tornado = [\n     {file = \"tornado-6.1.tar.gz\", hash = \"sha256:33c6e81d7bd55b468d2e793517c909b139960b6c790a60b7991b9b6b76fb9791\"},\n ]\n typing-extensions = [\n-    {file = \"typing_extensions-3.7.4.3-py2-none-any.whl\", hash = \"sha256:dafc7639cde7f1b6e1acc0f457842a83e722ccca8eef5270af2d74792619a89f\"},\n-    {file = \"typing_extensions-3.7.4.3-py3-none-any.whl\", hash = \"sha256:7cb407020f00f7bfc3cb3e7881628838e69d8f3fcab2f64742a5e76b2f841918\"},\n-    {file = \"typing_extensions-3.7.4.3.tar.gz\", hash = \"sha256:99d4073b617d30288f569d3f13d2bd7548c3a7e4c8de87db09a9d29bb3a4a60c\"},\n+    {file = \"typing_extensions-3.10.0.2-py2-none-any.whl\", hash = \"sha256:d8226d10bc02a29bcc81df19a26e56a9647f8b0a6d4a83924139f4a8b01f17b7\"},\n+    {file = \"typing_extensions-3.10.0.2-py3-none-any.whl\", hash = \"sha256:f1d25edafde516b146ecd0613dabcc61409817af4766fbbcfb8d1ad4ec441a34\"},\n+    {file = \"typing_extensions-3.10.0.2.tar.gz\", hash = \"sha256:49f75d16ff11f1cd258e1b988ccff82a3ca5570217d7ad8c5f48205dd99a677e\"},\n ]\n urllib3 = [\n-    {file = \"urllib3-1.26.3-py2.py3-none-any.whl\", hash = \"sha256:1b465e494e3e0d8939b50680403e3aedaa2bc434b7d5af64dfd3c958d7f5ae80\"},\n-    {file = \"urllib3-1.26.3.tar.gz\", hash = \"sha256:de3eedaad74a2683334e282005cd8d7f22f4d55fa690a2a1020a416cb0a47e73\"},\n+    {file = \"urllib3-1.26.7-py2.py3-none-any.whl\", hash = \"sha256:c4fdf4019605b6e5423637e01bc9fe4daef873709a7973e195ceba0a62bbc844\"},\n+    {file = \"urllib3-1.26.7.tar.gz\", hash = \"sha256:4987c65554f7a2dbf30c18fd48778ef124af6fab771a377103da0585e2336ece\"},\n ]\n websocket-client = [\n-    {file = \"websocket_client-0.57.0-py2.py3-none-any.whl\", hash = \"sha256:0fc45c961324d79c781bab301359d5a1b00b13ad1b10415a4780229ef71a5549\"},\n-    {file = \"websocket_client-0.57.0.tar.gz\", hash = \"sha256:d735b91d6d1692a6a181f2a8c9e0238e5f6373356f561bb9dc4c7af36f452010\"},\n+    {file = \"websocket-client-1.2.1.tar.gz\", hash = \"sha256:8dfb715d8a992f5712fff8c843adae94e22b22a99b2c5e6b0ec4a1a981cc4e0d\"},\n+    {file = \"websocket_client-1.2.1-py2.py3-none-any.whl\", hash = \"sha256:0133d2f784858e59959ce82ddac316634229da55b498aac311f1620567a710ec\"},\n ]\n yarl = [\n-    {file = \"yarl-1.6.3-cp36-cp36m-macosx_10_14_x86_64.whl\", hash = \"sha256:0355a701b3998dcd832d0dc47cc5dedf3874f966ac7f870e0f3a6788d802d434\"},\n-    {file = \"yarl-1.6.3-cp36-cp36m-manylinux1_i686.whl\", hash = \"sha256:bafb450deef6861815ed579c7a6113a879a6ef58aed4c3a4be54400ae8871478\"},\n-    {file = \"yarl-1.6.3-cp36-cp36m-manylinux2014_aarch64.whl\", hash = \"sha256:547f7665ad50fa8563150ed079f8e805e63dd85def6674c97efd78eed6c224a6\"},\n-    {file = \"yarl-1.6.3-cp36-cp36m-manylinux2014_i686.whl\", hash = \"sha256:63f90b20ca654b3ecc7a8d62c03ffa46999595f0167d6450fa8383bab252987e\"},\n-    {file = \"yarl-1.6.3-cp36-cp36m-manylinux2014_ppc64le.whl\", hash = \"sha256:97b5bdc450d63c3ba30a127d018b866ea94e65655efaf889ebeabc20f7d12406\"},\n-    {file = \"yarl-1.6.3-cp36-cp36m-manylinux2014_s390x.whl\", hash = \"sha256:d8d07d102f17b68966e2de0e07bfd6e139c7c02ef06d3a0f8d2f0f055e13bb76\"},\n-    {file = \"yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl\", hash = \"sha256:15263c3b0b47968c1d90daa89f21fcc889bb4b1aac5555580d74565de6836366\"},\n-    {file = \"yarl-1.6.3-cp36-cp36m-win32.whl\", hash = \"sha256:b5dfc9a40c198334f4f3f55880ecf910adebdcb2a0b9a9c23c9345faa9185721\"},\n-    {file = \"yarl-1.6.3-cp36-cp36m-win_amd64.whl\", hash = \"sha256:b2e9a456c121e26d13c29251f8267541bd75e6a1ccf9e859179701c36a078643\"},\n-    {file = \"yarl-1.6.3-cp37-cp37m-macosx_10_14_x86_64.whl\", hash = \"sha256:ce3beb46a72d9f2190f9e1027886bfc513702d748047b548b05dab7dfb584d2e\"},\n-    {file = \"yarl-1.6.3-cp37-cp37m-manylinux1_i686.whl\", hash = \"sha256:2ce4c621d21326a4a5500c25031e102af589edb50c09b321049e388b3934eec3\"},\n-    {file = \"yarl-1.6.3-cp37-cp37m-manylinux2014_aarch64.whl\", hash = \"sha256:d26608cf178efb8faa5ff0f2d2e77c208f471c5a3709e577a7b3fd0445703ac8\"},\n-    {file = \"yarl-1.6.3-cp37-cp37m-manylinux2014_i686.whl\", hash = \"sha256:4c5bcfc3ed226bf6419f7a33982fb4b8ec2e45785a0561eb99274ebbf09fdd6a\"},\n-    {file = \"yarl-1.6.3-cp37-cp37m-manylinux2014_ppc64le.whl\", hash = \"sha256:4736eaee5626db8d9cda9eb5282028cc834e2aeb194e0d8b50217d707e98bb5c\"},\n-    {file = \"yarl-1.6.3-cp37-cp37m-manylinux2014_s390x.whl\", hash = \"sha256:68dc568889b1c13f1e4745c96b931cc94fdd0defe92a72c2b8ce01091b22e35f\"},\n-    {file = \"yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl\", hash = \"sha256:7356644cbed76119d0b6bd32ffba704d30d747e0c217109d7979a7bc36c4d970\"},\n-    {file = \"yarl-1.6.3-cp37-cp37m-win32.whl\", hash = \"sha256:00d7ad91b6583602eb9c1d085a2cf281ada267e9a197e8b7cae487dadbfa293e\"},\n-    {file = \"yarl-1.6.3-cp37-cp37m-win_amd64.whl\", hash = \"sha256:69ee97c71fee1f63d04c945f56d5d726483c4762845400a6795a3b75d56b6c50\"},\n-    {file = \"yarl-1.6.3-cp38-cp38-macosx_10_14_x86_64.whl\", hash = \"sha256:e46fba844f4895b36f4c398c5af062a9808d1f26b2999c58909517384d5deda2\"},\n-    {file = \"yarl-1.6.3-cp38-cp38-manylinux1_i686.whl\", hash = \"sha256:31ede6e8c4329fb81c86706ba8f6bf661a924b53ba191b27aa5fcee5714d18ec\"},\n-    {file = \"yarl-1.6.3-cp38-cp38-manylinux2014_aarch64.whl\", hash = \"sha256:fcbb48a93e8699eae920f8d92f7160c03567b421bc17362a9ffbbd706a816f71\"},\n-    {file = \"yarl-1.6.3-cp38-cp38-manylinux2014_i686.whl\", hash = \"sha256:72a660bdd24497e3e84f5519e57a9ee9220b6f3ac4d45056961bf22838ce20cc\"},\n-    {file = \"yarl-1.6.3-cp38-cp38-manylinux2014_ppc64le.whl\", hash = \"sha256:324ba3d3c6fee56e2e0b0d09bf5c73824b9f08234339d2b788af65e60040c959\"},\n-    {file = \"yarl-1.6.3-cp38-cp38-manylinux2014_s390x.whl\", hash = \"sha256:e6b5460dc5ad42ad2b36cca524491dfcaffbfd9c8df50508bddc354e787b8dc2\"},\n-    {file = \"yarl-1.6.3-cp38-cp38-manylinux2014_x86_64.whl\", hash = \"sha256:6d6283d8e0631b617edf0fd726353cb76630b83a089a40933043894e7f6721e2\"},\n-    {file = \"yarl-1.6.3-cp38-cp38-win32.whl\", hash = \"sha256:9ede61b0854e267fd565e7527e2f2eb3ef8858b301319be0604177690e1a3896\"},\n-    {file = \"yarl-1.6.3-cp38-cp38-win_amd64.whl\", hash = \"sha256:f0b059678fd549c66b89bed03efcabb009075bd131c248ecdf087bdb6faba24a\"},\n-    {file = \"yarl-1.6.3-cp39-cp39-macosx_10_14_x86_64.whl\", hash = \"sha256:329412812ecfc94a57cd37c9d547579510a9e83c516bc069470db5f75684629e\"},\n-    {file = \"yarl-1.6.3-cp39-cp39-manylinux1_i686.whl\", hash = \"sha256:c49ff66d479d38ab863c50f7bb27dee97c6627c5fe60697de15529da9c3de724\"},\n-    {file = \"yarl-1.6.3-cp39-cp39-manylinux2014_aarch64.whl\", hash = \"sha256:f040bcc6725c821a4c0665f3aa96a4d0805a7aaf2caf266d256b8ed71b9f041c\"},\n-    {file = \"yarl-1.6.3-cp39-cp39-manylinux2014_i686.whl\", hash = \"sha256:d5c32c82990e4ac4d8150fd7652b972216b204de4e83a122546dce571c1bdf25\"},\n-    {file = \"yarl-1.6.3-cp39-cp39-manylinux2014_ppc64le.whl\", hash = \"sha256:d597767fcd2c3dc49d6eea360c458b65643d1e4dbed91361cf5e36e53c1f8c96\"},\n-    {file = \"yarl-1.6.3-cp39-cp39-manylinux2014_s390x.whl\", hash = \"sha256:8aa3decd5e0e852dc68335abf5478a518b41bf2ab2f330fe44916399efedfae0\"},\n-    {file = \"yarl-1.6.3-cp39-cp39-manylinux2014_x86_64.whl\", hash = \"sha256:73494d5b71099ae8cb8754f1df131c11d433b387efab7b51849e7e1e851f07a4\"},\n-    {file = \"yarl-1.6.3-cp39-cp39-win32.whl\", hash = \"sha256:5b883e458058f8d6099e4420f0cc2567989032b5f34b271c0827de9f1079a424\"},\n-    {file = \"yarl-1.6.3-cp39-cp39-win_amd64.whl\", hash = \"sha256:4953fb0b4fdb7e08b2f3b3be80a00d28c5c8a2056bb066169de00e6501b986b6\"},\n-    {file = \"yarl-1.6.3.tar.gz\", hash = \"sha256:8a9066529240171b68893d60dca86a763eae2139dd42f42106b03cf4b426bf10\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-macosx_10_9_universal2.whl\", hash = \"sha256:e35d8230e4b08d86ea65c32450533b906a8267a87b873f2954adeaecede85169\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-macosx_10_9_x86_64.whl\", hash = \"sha256:eb4b3f277880c314e47720b4b6bb2c85114ab3c04c5442c9bc7006b3787904d8\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-macosx_11_0_arm64.whl\", hash = \"sha256:c7015dcedb91d90a138eebdc7e432aec8966e0147ab2a55f2df27b1904fa7291\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:bb3e478175e15e00d659fb0354a6a8db71a7811a2a5052aed98048bc972e5d2b\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:8b8c409aa3a7966647e7c1c524846b362a6bcbbe120bf8a176431f940d2b9a2e\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b22ea41c7e98170474a01e3eded1377d46b2dfaef45888a0005c683eaaa49285\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:a7dfc46add4cfe5578013dbc4127893edc69fe19132d2836ff2f6e49edc5ecd6\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:82ff6f85f67500a4f74885d81659cd270eb24dfe692fe44e622b8a2fd57e7279\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-musllinux_1_1_aarch64.whl\", hash = \"sha256:f3cd2158b2ed0fb25c6811adfdcc47224efe075f2d68a750071dacc03a7a66e4\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-musllinux_1_1_i686.whl\", hash = \"sha256:59c0f13f9592820c51280d1cf811294d753e4a18baf90f0139d1dc93d4b6fc5f\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-musllinux_1_1_ppc64le.whl\", hash = \"sha256:7f7655ad83d1a8afa48435a449bf2f3009293da1604f5dd95b5ddcf5f673bd69\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-musllinux_1_1_s390x.whl\", hash = \"sha256:aa9f0d9b62d15182341b3e9816582f46182cab91c1a57b2d308b9a3c4e2c4f78\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-musllinux_1_1_x86_64.whl\", hash = \"sha256:fdd1b90c225a653b1bd1c0cae8edf1957892b9a09c8bf7ee6321eeb8208eac0f\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-win32.whl\", hash = \"sha256:7c8d0bb76eabc5299db203e952ec55f8f4c53f08e0df4285aac8c92bd9e12675\"},\n+    {file = \"yarl-1.7.0-cp310-cp310-win_amd64.whl\", hash = \"sha256:622a36fa779efb4ff9eff5fe52730ff17521431379851a31e040958fc251670c\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-macosx_10_9_x86_64.whl\", hash = \"sha256:3d461b7a8e139b9e4b41f62eb417ffa0b98d1c46d4caf14c845e6a3b349c0bb1\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:81cfacdd1e40bc931b5519499342efa388d24d262c30a3d31187bfa04f4a7001\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:821b978f2152be7695d4331ef0621d207aedf9bbd591ba23a63412a3efc29a01\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:b64bd24c8c9a487f4a12260dc26732bf41028816dbf0c458f17864fbebdb3131\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:98c9ddb92b60a83c21be42c776d3d9d5ec632a762a094c41bda37b7dfbd2cd83\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:a532d75ca74431c053a88a802e161fb3d651b8bf5821a3440bc3616e38754583\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-musllinux_1_1_aarch64.whl\", hash = \"sha256:053e09817eafb892e94e172d05406c1b3a22a93bc68f6eff5198363a3d764459\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-musllinux_1_1_i686.whl\", hash = \"sha256:98c51f02d542945d306c8e934aa2c1e66ba5e9c1c86b5bf37f3a51c8a747067e\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:15ec41a5a5fdb7bace6d7b16701f9440007a82734f69127c0fbf6d87e10f4a1e\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-musllinux_1_1_s390x.whl\", hash = \"sha256:a7f08819dba1e1255d6991ed37448a1bf4b1352c004bcd899b9da0c47958513d\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-musllinux_1_1_x86_64.whl\", hash = \"sha256:8e3ffab21db0542ffd1887f3b9575ddd58961f2cf61429cb6458afc00c4581e0\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-win32.whl\", hash = \"sha256:50127634f519b2956005891507e3aa4ac345f66a7ea7bbc2d7dcba7401f41898\"},\n+    {file = \"yarl-1.7.0-cp36-cp36m-win_amd64.whl\", hash = \"sha256:36ec44f15193f6d5288d42ebb8e751b967ebdfb72d6830983838d45ab18edb4f\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-macosx_10_9_x86_64.whl\", hash = \"sha256:ec1b5a25a25c880c976d0bb3d107def085bb08dbb3db7f4442e0a2b980359d24\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:b36f5a63c891f813c6f04ef19675b382efc190fd5ce7e10ab19386d2548bca06\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:38173b8c3a29945e7ecade9a3f6ff39581eee8201338ee6a2c8882db5df3e806\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:8ba402f32184f0b405fb281b93bd0d8ab7e3257735b57b62a6ed2e94cdf4fe50\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:be52bc5208d767cdd8308a9e93059b3b36d1e048fecbea0e0346d0d24a76adc0\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:08c2044a956f4ef30405f2f433ce77f1f57c2c773bf81ae43201917831044d5a\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-musllinux_1_1_aarch64.whl\", hash = \"sha256:484d61c047c45670ef5967653a1d0783e232c54bf9dd786a7737036828fa8d54\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-musllinux_1_1_i686.whl\", hash = \"sha256:b7de92a4af85cfcaf4081f8aa6165b1d63ee5de150af3ee85f954145f93105a7\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-musllinux_1_1_ppc64le.whl\", hash = \"sha256:376e41775aab79c5575534924a386c8e0f1a5d91db69fc6133fd27a489bcaf10\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-musllinux_1_1_s390x.whl\", hash = \"sha256:8a8b10d0e7bac154f959b709fcea593cda527b234119311eb950096653816a86\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-musllinux_1_1_x86_64.whl\", hash = \"sha256:f46cd4c43e6175030e2a56def8f1d83b64e6706eeb2bb9ab0ef4756f65eab23f\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-win32.whl\", hash = \"sha256:b28cfb46140efe1a6092b8c5c4994a1fe70dc83c38fbcea4992401e0c6fb9cce\"},\n+    {file = \"yarl-1.7.0-cp37-cp37m-win_amd64.whl\", hash = \"sha256:9624154ec9c02a776802da1086eed7f5034bd1971977f5146233869c2ac80297\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-macosx_10_9_universal2.whl\", hash = \"sha256:69945d13e1bbf81784a9bc48824feb9cd66491e6a503d4e83f6cd7c7cc861361\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-macosx_10_9_x86_64.whl\", hash = \"sha256:46a742ed9e363bd01be64160ce7520e92e11989bd4cb224403cfd31c101cc83d\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-macosx_11_0_arm64.whl\", hash = \"sha256:cb4ff1ac7cb4500f43581b3f4cbd627d702143aa6be1fdc1fa3ebffaf4dc1be5\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:3ad51e17cd65ea3debb0e10f0120cf8dd987c741fe423ed2285087368090b33d\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:7e37786ea89a5d3ffbbf318ea9790926f8dfda83858544f128553c347ad143c6\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:c63c1e208f800daad71715786bfeb1cecdc595d87e2e9b1cd234fd6e597fd71d\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:91cbe24300c11835ef186436363352b3257db7af165e0a767f4f17aa25761388\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:e510dbec7c59d32eaa61ffa48173d5e3d7170a67f4a03e8f5e2e9e3971aca622\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-musllinux_1_1_aarch64.whl\", hash = \"sha256:3def6e681cc02397e5d8141ee97b41d02932b2bcf0fb34532ad62855eab7c60e\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-musllinux_1_1_i686.whl\", hash = \"sha256:263c81b94e6431942b27f6f671fa62f430a0a5c14bb255f2ab69eeb9b2b66ff7\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-musllinux_1_1_ppc64le.whl\", hash = \"sha256:e78c91faefe88d601ddd16e3882918dbde20577a2438e2320f8239c8b7507b8f\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-musllinux_1_1_s390x.whl\", hash = \"sha256:22b2430c49713bfb2f0a0dd4a8d7aab218b28476ba86fd1c78ad8899462cbcf2\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-musllinux_1_1_x86_64.whl\", hash = \"sha256:2e7ad9db939082f5d0b9269cfd92c025cb8f2fbbb1f1b9dc5a393c639db5bd92\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-win32.whl\", hash = \"sha256:3a31e4a8dcb1beaf167b7e7af61b88cb961b220db8d3ba1c839723630e57eef7\"},\n+    {file = \"yarl-1.7.0-cp38-cp38-win_amd64.whl\", hash = \"sha256:d579957439933d752358c6a300c93110f84aae67b63dd0c19dde6ecbf4056f6b\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-macosx_10_9_universal2.whl\", hash = \"sha256:87721b549505a546eb003252185103b5ec8147de6d3ad3714d148a5a67b6fe53\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-macosx_10_9_x86_64.whl\", hash = \"sha256:a1fa866fa24d9f4108f9e58ea8a2135655419885cdb443e36b39a346e1181532\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-macosx_11_0_arm64.whl\", hash = \"sha256:1d3b8449dfedfe94eaff2b77954258b09b24949f6818dfa444b05dbb05ae1b7e\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\", hash = \"sha256:db2372e350794ce8b9f810feb094c606b7e0e4aa6807141ac4fadfe5ddd75bb0\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl\", hash = \"sha256:a06d9d0b9a97fa99b84fee71d9dd11e69e21ac8a27229089f07b5e5e50e8d63c\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl\", hash = \"sha256:a3455c2456d6307bcfa80bc1157b8603f7d93573291f5bdc7144489ca0df4628\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl\", hash = \"sha256:d30d67e3486aea61bb2cbf7cf81385364c2e4f7ce7469a76ed72af76a5cdfe6b\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\", hash = \"sha256:c18a4b286e8d780c3a40c31d7b79836aa93b720f71d5743f20c08b7e049ca073\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-musllinux_1_1_aarch64.whl\", hash = \"sha256:d54c925396e7891666cabc0199366ca55b27d003393465acef63fd29b8b7aa92\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-musllinux_1_1_i686.whl\", hash = \"sha256:64773840952de17851a1c7346ad7f71688c77e74248d1f0bc230e96680f84028\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-musllinux_1_1_ppc64le.whl\", hash = \"sha256:acbf1756d9dc7cd0ae943d883be72e84e04396f6c2ff93a6ddeca929d562039f\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-musllinux_1_1_s390x.whl\", hash = \"sha256:2e48f27936aa838939c798f466c851ba4ae79e347e8dfce43b009c64b930df12\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-musllinux_1_1_x86_64.whl\", hash = \"sha256:1beef4734ca1ad40a9d8c6b20a76ab46e3a2ed09f38561f01e4aa2ea82cafcef\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-win32.whl\", hash = \"sha256:8ee78c9a5f3c642219d4607680a4693b59239c27a3aa608b64ef79ddc9698039\"},\n+    {file = \"yarl-1.7.0-cp39-cp39-win_amd64.whl\", hash = \"sha256:d750503682605088a14d29a4701548c15c510da4f13c8b17409c4097d5b04c52\"},\n+    {file = \"yarl-1.7.0.tar.gz\", hash = \"sha256:8e7ebaf62e19c2feb097ffb7c94deb0f0c9fab52590784c8cd679d30ab009162\"},\n ]\n zict = [\n     {file = \"zict-2.0.0-py3-none-any.whl\", hash = \"sha256:26aa1adda8250a78dfc6a78d200bfb2ea43a34752cf58980bca75dde0ba0c6e9\"},\n     {file = \"zict-2.0.0.tar.gz\", hash = \"sha256:8e2969797627c8a663575c2fc6fcb53a05e37cdb83ee65f341fc6e0c3d0ced16\"},\n ]\n zipp = [\n-    {file = \"zipp-3.4.0-py3-none-any.whl\", hash = \"sha256:102c24ef8f171fd729d46599845e95c7ab894a4cf45f5de11a44cc7444fb1108\"},\n-    {file = \"zipp-3.4.0.tar.gz\", hash = \"sha256:ed5eee1974372595f9e416cc7bbeeb12335201d8081ca8a0743c954d4446e5cb\"},\n+    {file = \"zipp-3.6.0-py3-none-any.whl\", hash = \"sha256:9fe5ea21568a0a70e50f273397638d39b03353731e6cbbb3fd8502a33fec40bc\"},\n+    {file = \"zipp-3.6.0.tar.gz\", hash = \"sha256:71c644c5369f4a6e07636f0aa966270449561fcea2e3d6747b8d23efaa9d7832\"},\n ]"}, {"sha": "2f077d126e5fa5726762118d808e2ae270dbe381", "filename": "pyproject.toml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/d975bad402c02c8a45c0e0e9964b4a9138095b37/pyproject.toml", "raw_url": "https://github.com/backtick-se/cowait/raw/d975bad402c02c8a45c0e0e9964b4a9138095b37/pyproject.toml", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/pyproject.toml?ref=d975bad402c02c8a45c0e0e9964b4a9138095b37", "patch": "@@ -14,7 +14,7 @@ python = \"^3.7\"\n docker = \">=4\"\n kubernetes = \">=10\"\n nest-asyncio = \"^1.4.1\"\n-aiohttp = \"^3\"\n+aiohttp = \"^3.7\"\n aiohttp-middlewares = \"^1\"\n pytest = \"^6\"\n alt-pytest-asyncio = \"^0.5.4\" # would be nice to move to the widely used pytest-asyncio"}], "stats": {"total": 684, "additions": 396, "deletions": 288}}, {"sha": "56de0909e270e59b1f52ed15859459dfb554cd57", "html_url": "https://github.com/backtick-se/cowait/commit/56de0909e270e59b1f52ed15859459dfb554cd57", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-11-01T09:42:17Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-11-01T09:42:17Z"}, "message": "bump version to 0.4.30", "tree": {"sha": "e0d20a3e5dd2abdb93a656111034eecad5c6fd33", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/e0d20a3e5dd2abdb93a656111034eecad5c6fd33"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/56de0909e270e59b1f52ed15859459dfb554cd57", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "879764113dbfb6b817906bd98aaf93902b9567cc", "filename": "cowait/version.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/56de0909e270e59b1f52ed15859459dfb554cd57/cowait/version.py", "raw_url": "https://github.com/backtick-se/cowait/raw/56de0909e270e59b1f52ed15859459dfb554cd57/cowait/version.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/version.py?ref=56de0909e270e59b1f52ed15859459dfb554cd57", "patch": "@@ -1 +1 @@\n-version = \"0.4.29\"\n+version = \"0.4.30\""}, {"sha": "3b500f4bda64f9563e1f70e3cf300639903a5b0d", "filename": "pyproject.toml", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/backtick-se/cowait/blob/56de0909e270e59b1f52ed15859459dfb554cd57/pyproject.toml", "raw_url": "https://github.com/backtick-se/cowait/raw/56de0909e270e59b1f52ed15859459dfb554cd57/pyproject.toml", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/pyproject.toml?ref=56de0909e270e59b1f52ed15859459dfb554cd57", "patch": "@@ -1,6 +1,6 @@\n [tool.poetry]\n name = \"cowait\"\n-version = \"0.4.29\"\n+version = \"0.4.30\"\n description = \"\"\n authors = [\"Backtick Technologies <johan@backtick.se>\"]\n license = \"Apache License v2.0\"\n@@ -14,7 +14,7 @@ python = \"^3.7\"\n docker = \">=4\"\n kubernetes = \">=10\"\n nest-asyncio = \"^1.4.1\"\n-aiohttp = \"^3.7\"\n+aiohttp = \"3.7.4\"\n aiohttp-middlewares = \"^1\"\n pytest = \"^6\"\n alt-pytest-asyncio = \"^0.5.4\" # would be nice to move to the widely used pytest-asyncio"}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 333, "title": "Bump url-parse from 1.5.1 to 1.5.3 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/333", "html_url": "https://github.com/backtick-se/cowait/pull/333", "diff_url": "https://github.com/backtick-se/cowait/pull/333.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/333.patch", "merged_at": null}, "body": "Bumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.1 to 1.5.3.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/ad444931666a30bad11472d89a216461cf16cae2\"><code>ad44493</code></a> [dist] 1.5.3</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/c7984617e235892cc22e0f47bb5ff1c012e6e39f\"><code>c798461</code></a> [fix] Fix host parsing for file URLs (<a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/210\">#210</a>)</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/201034b8670c2aa382d7ec410ee750ac6f2f9c38\"><code>201034b</code></a> [dist] 1.5.2</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/2d9ac2c94067742b2116332c1e03be9f37371dff\"><code>2d9ac2c</code></a> [fix] Sanitize only special URLs (<a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/209\">#209</a>)</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/fb128af4f43fa17f351d50cf615c7598c751f50a\"><code>fb128af</code></a> [fix] Use <code>'null'</code> as <code>origin</code> for non special URLs</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/fed6d9e338ea39de2d68bb66607066d71328c62f\"><code>fed6d9e</code></a> [fix] Add a leading slash only if the URL is special</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/94872e7ab9103ee69b958959baa14c9e682a7f10\"><code>94872e7</code></a> [fix] Do not incorrectly set the <code>slashes</code> property to <code>true</code></li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/81ab967889b08112d3356e451bf03e6aa0cbb7e0\"><code>81ab967</code></a> [fix] Ignore slashes after the protocol for special URLs</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/ee22050a48a67409aa5f7c87947284156d615bd1\"><code>ee22050</code></a> [ci] Use GitHub Actions</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/d2979b586d8c7751e0c77f127d9ce1b2143cc0c9\"><code>d2979b5</code></a> [fix] Special case the <code>file:</code> protocol (<a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/204\">#204</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/unshiftio/url-parse/compare/1.5.1...1.5.3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=url-parse&package-manager=npm_and_yarn&previous-version=1.5.1&new-version=1.5.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "4784ecda0930433e8ce47531cdd4bbd1d85fa93e", "html_url": "https://github.com/backtick-se/cowait/commit/4784ecda0930433e8ce47531cdd4bbd1d85fa93e", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-10-06T18:28:37Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-10-06T18:28:37Z"}, "message": "Bump url-parse from 1.5.1 to 1.5.3 in /cloud\n\nBumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.1 to 1.5.3.\n- [Release notes](https://github.com/unshiftio/url-parse/releases)\n- [Commits](https://github.com/unshiftio/url-parse/compare/1.5.1...1.5.3)\n\n---\nupdated-dependencies:\n- dependency-name: url-parse\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "c3a740be0ad156edab6a9c1eeeacb919d279afa5", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/c3a740be0ad156edab6a9c1eeeacb919d279afa5"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/4784ecda0930433e8ce47531cdd4bbd1d85fa93e", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJhXerVCRBK7hj4Ov3rIwAAlKUIACAaT7E5/ifdxU58DYPXYnXz\nVXElmhPxAlJWSO5h6ZaXpsEVWvyjsBA1my8CpxZQQ1gcTBjtLfgnyIwiuYbPuuns\nKEJGZleKask6cfucXzKvhxprj+7OCSN3udGLdDcF/mIwiKe/JUwvhG0hXH1CB3zN\n03XpivTvxyx7dS0C+vkAQ6daXah7Nwd8iiK0hYCDNUxcCGkALigOypEcd4+vy/5m\nOJ6+8YjbapTimNkBZMglFBqZ7mdbafNzXybziy0TV3XnTUVrK3OeG1ChtO9yIpby\nkOzDOEsrb8tILUz31XtnQwcVq6Ab62Ak4ux4JgVtZAc3vw3cNvOiPNNddFpimX8=\n=a00e\n-----END PGP SIGNATURE-----\n", "payload": "tree c3a740be0ad156edab6a9c1eeeacb919d279afa5\nparent e3c9b9ad1a32fa6b9bd7289798e91775d2f33dd4\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1633544917 +0000\ncommitter GitHub <noreply@github.com> 1633544917 +0000\n\nBump url-parse from 1.5.1 to 1.5.3 in /cloud\n\nBumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.1 to 1.5.3.\n- [Release notes](https://github.com/unshiftio/url-parse/releases)\n- [Commits](https://github.com/unshiftio/url-parse/compare/1.5.1...1.5.3)\n\n---\nupdated-dependencies:\n- dependency-name: url-parse\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "116256ac32fd5735e14273fbcd7844ab3744df46", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/4784ecda0930433e8ce47531cdd4bbd1d85fa93e/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/4784ecda0930433e8ce47531cdd4bbd1d85fa93e/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=4784ecda0930433e8ce47531cdd4bbd1d85fa93e", "patch": "@@ -10907,9 +10907,9 @@ url-loader@2.1.0:\n     schema-utils \"^2.0.0\"\n \n url-parse@^1.4.3:\n-  version \"1.5.1\"\n-  resolved \"https://registry.yarnpkg.com/url-parse/-/url-parse-1.5.1.tgz#d5fa9890af8a5e1f274a2c98376510f6425f6e3b\"\n-  integrity sha512-HOfCOUJt7iSYzEx/UqgtwKRMC6EU91NFhsCHMv9oM03VJcVo2Qrp8T8kI9D7amFf1cu+/3CEhgb3rF9zL7k85Q==\n+  version \"1.5.3\"\n+  resolved \"https://registry.yarnpkg.com/url-parse/-/url-parse-1.5.3.tgz#71c1303d38fb6639ade183c2992c8cc0686df862\"\n+  integrity sha512-IIORyIQD9rvj0A4CLWsHkBBJuNqWpFQe224b6j9t/ABmquIS0qDU2pY6kl6AuOrL5OkCXHMCFNe1jBcuAggjvQ==\n   dependencies:\n     querystringify \"^2.1.1\"\n     requires-port \"^1.0.0\""}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 332, "title": "Bump tmpl from 1.0.4 to 1.0.5 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/332", "html_url": "https://github.com/backtick-se/cowait/pull/332", "diff_url": "https://github.com/backtick-se/cowait/pull/332.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/332.patch", "merged_at": "2022-02-21T09:51:06Z"}, "body": "Bumps [tmpl](https://github.com/daaku/nodejs-tmpl) from 1.0.4 to 1.0.5.\n<details>\n<summary>Commits</summary>\n<ul>\n<li>See full diff in <a href=\"https://github.com/daaku/nodejs-tmpl/commits/v1.0.5\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tmpl&package-manager=npm_and_yarn&previous-version=1.0.4&new-version=1.0.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "9822f96c408627c6c1e76152f732c3abeac73090", "html_url": "https://github.com/backtick-se/cowait/commit/9822f96c408627c6c1e76152f732c3abeac73090", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-09-21T22:58:26Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-09-21T22:58:26Z"}, "message": "Bump tmpl from 1.0.4 to 1.0.5 in /cloud\n\nBumps [tmpl](https://github.com/daaku/nodejs-tmpl) from 1.0.4 to 1.0.5.\n- [Release notes](https://github.com/daaku/nodejs-tmpl/releases)\n- [Commits](https://github.com/daaku/nodejs-tmpl/commits/v1.0.5)\n\n---\nupdated-dependencies:\n- dependency-name: tmpl\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "d8066fcc132899960d51f37cb281895c2b64f163", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/d8066fcc132899960d51f37cb281895c2b64f163"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/9822f96c408627c6c1e76152f732c3abeac73090", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJhSmOSCRBK7hj4Ov3rIwAAvXoIAHXxk2j2I94qpHZf6UFav0IB\n+S8O4MZnLpSNSiIuYI3GEQydug1B5RldpPWoiP0hoKIW9PCWVG3Z2929LUgyYtdo\ntf7frM+kz0pHsswN9KaTJ0CLnM0bKMspMATe2c6JVW9Fe5DVRzQkfP1W/Ru3/GU3\nRQOj/RRAvCItSK4XWnnf9TkHpy06lPHRqFgDcLkWN1CQQNHwL7g8mp+fz2v4ujlf\nR+6V6N4Nw9Haoo0FZohVLia1/mocqn0dWIKfDfvrE6JMW5jHGZNqq1itU2ojBMHI\nLDXzoQ+zwvumEPTufRlkMJ26YuvA/FZx/rGBLpqeZHUQSTO26xidFCg2VLsO5bk=\n=6ei/\n-----END PGP SIGNATURE-----\n", "payload": "tree d8066fcc132899960d51f37cb281895c2b64f163\nparent e3c9b9ad1a32fa6b9bd7289798e91775d2f33dd4\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1632265106 +0000\ncommitter GitHub <noreply@github.com> 1632265106 +0000\n\nBump tmpl from 1.0.4 to 1.0.5 in /cloud\n\nBumps [tmpl](https://github.com/daaku/nodejs-tmpl) from 1.0.4 to 1.0.5.\n- [Release notes](https://github.com/daaku/nodejs-tmpl/releases)\n- [Commits](https://github.com/daaku/nodejs-tmpl/commits/v1.0.5)\n\n---\nupdated-dependencies:\n- dependency-name: tmpl\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "932b555479939971a069404cdd6e8fea1a6a0f7c", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/9822f96c408627c6c1e76152f732c3abeac73090/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/9822f96c408627c6c1e76152f732c3abeac73090/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=9822f96c408627c6c1e76152f732c3abeac73090", "patch": "@@ -10599,9 +10599,9 @@ tmp@^0.0.33:\n     os-tmpdir \"~1.0.2\"\n \n tmpl@1.0.x:\n-  version \"1.0.4\"\n-  resolved \"https://registry.yarnpkg.com/tmpl/-/tmpl-1.0.4.tgz#23640dd7b42d00433911140820e5cf440e521dd1\"\n-  integrity sha1-I2QN17QtAEM5ERQIIOXPRA5SHdE=\n+  version \"1.0.5\"\n+  resolved \"https://registry.yarnpkg.com/tmpl/-/tmpl-1.0.5.tgz#8683e0b902bb9c20c4f726e3c0b69f36518c07cc\"\n+  integrity sha512-3f0uOEAQwIqGuWW2MVzYg8fV/QNnc/IpuJNG837rLuczAaLVHslWHZQj4IGiEl5Hs3kkbhwL9Ab7Hrsmuj+Smw==\n \n to-arraybuffer@^1.0.0:\n   version \"1.0.1\""}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 331, "title": "Bump tensorflow-cpu from 2.4.0 to 2.5.1 in /examples/10-imdb", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2588709704, "node_id": "MDU6TGFiZWwyNTg4NzA5NzA0", "url": "https://api.github.com/repos/backtick-se/cowait/labels/python", "name": "python", "color": "2b67c6", "default": false, "description": "Pull requests that update Python code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/331", "html_url": "https://github.com/backtick-se/cowait/pull/331", "diff_url": "https://github.com/backtick-se/cowait/pull/331.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/331.patch", "merged_at": null}, "body": "Bumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/releases\">tensorflow-cpu's releases</a>.</em></p>\n<blockquote>\n<h2>TensorFlow 2.5.1</h2>\n<h1>Release 2.5.1</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a heap out of bounds access in sparse reduction operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635\">CVE-2021-37635</a>)</li>\n<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636\">CVE-2021-37636</a>)</li>\n<li>Fixes a null pointer dereference in <code>CompressElement</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637\">CVE-2021-37637</a>)</li>\n<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638\">CVE-2021-37638</a>)</li>\n<li>Fixes a null pointer dereference and a heap OOB read arising from operations restoring tensors (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639\">CVE-2021-37639</a>)</li>\n<li>Fixes an integer division by 0 in sparse reshaping (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640\">CVE-2021-37640</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceScatterDiv</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37642\">CVE-2021-37642</a>)</li>\n<li>Fixes a heap OOB in <code>RaggedGather</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37641\">CVE-2021-37641</a>)</li>\n<li>Fixes a <code>std::abort</code> raised from <code>TensorListReserve</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37644\">CVE-2021-37644</a>)</li>\n<li>Fixes a null pointer dereference in <code>MatrixDiagPartOp</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37643\">CVE-2021-37643</a>)</li>\n<li>Fixes an integer overflow due to conversion to unsigned (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37645\">CVE-2021-37645</a>)</li>\n<li>Fixes a bad allocation error in <code>StringNGrams</code> caused by integer conversion (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37646\">CVE-2021-37646</a>)</li>\n<li>Fixes a null pointer dereference in <code>SparseTensorSliceDataset</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37647\">CVE-2021-37647</a>)</li>\n<li>Fixes an incorrect validation of <code>SaveV2</code> inputs (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37648\">CVE-2021-37648</a>)</li>\n<li>Fixes a null pointer dereference in <code>UncompressElement</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37649\">CVE-2021-37649</a>)</li>\n<li>Fixes a segfault and a heap buffer overflow in <code>{Experimental,}DatasetToTFRecord</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37650\">CVE-2021-37650</a>)</li>\n<li>Fixes a heap buffer overflow in <code>FractionalAvgPoolGrad</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37651\">CVE-2021-37651</a>)</li>\n<li>Fixes a use after free in boosted trees creation (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37652\">CVE-2021-37652</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceGather</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37653\">CVE-2021-37653</a>)</li>\n<li>Fixes a heap OOB and a <code>CHECK</code> fail in <code>ResourceGather</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37654\">CVE-2021-37654</a>)</li>\n<li>Fixes a heap OOB in <code>ResourceScatterUpdate</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37655\">CVE-2021-37655</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>RaggedTensorToSparse</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37656\">CVE-2021-37656</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>MatrixDiagV*</code> ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37657\">CVE-2021-37657</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>MatrixSetDiagV*</code> ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37658\">CVE-2021-37658</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr and heap OOB in binary cwise ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37659\">CVE-2021-37659</a>)</li>\n<li>Fixes a division by 0 in inplace operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37660\">CVE-2021-37660</a>)</li>\n<li>Fixes a crash caused by integer conversion to unsigned (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37661\">CVE-2021-37661</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in boosted trees (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37662\">CVE-2021-37662</a>)</li>\n<li>Fixes a heap OOB in boosted trees (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37664\">CVE-2021-37664</a>)</li>\n<li>Fixes vulnerabilities arising from incomplete validation in <code>QuantizeV2</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37663\">CVE-2021-37663</a>)</li>\n<li>Fixes vulnerabilities arising from incomplete validation in MKL requantization (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37665\">CVE-2021-37665</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>RaggedTensorToVariant</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37666\">CVE-2021-37666</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in unicode encoding (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37667\">CVE-2021-37667</a>)</li>\n<li>Fixes an FPE in <code>tf.raw_ops.UnravelIndex</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37668\">CVE-2021-37668</a>)</li>\n<li>Fixes a crash in NMS ops caused by integer conversion to unsigned (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37669\">CVE-2021-37669</a>)</li>\n<li>Fixes a heap OOB in <code>UpperBound</code> and <code>LowerBound</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37670\">CVE-2021-37670</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in map operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37671\">CVE-2021-37671</a>)</li>\n<li>Fixes a heap OOB in <code>SdcaOptimizerV2</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37672\">CVE-2021-37672</a>)</li>\n<li>Fixes a <code>CHECK</code>-fail in <code>MapStage</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37673\">CVE-2021-37673</a>)</li>\n<li>Fixes a vulnerability arising from incomplete validation in <code>MaxPoolGrad</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37674\">CVE-2021-37674</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in shape inference (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37676\">CVE-2021-37676</a>)</li>\n<li>Fixes a division by 0 in most convolution operators (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37675\">CVE-2021-37675</a>)</li>\n<li>Fixes vulnerabilities arising from missing validation in shape inference for <code>Dequantize</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37677\">CVE-2021-37677</a>)</li>\n<li>Fixes an arbitrary code execution due to YAML deserialization (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37678\">CVE-2021-37678</a>)</li>\n<li>Fixes a heap OOB in nested <code>tf.map_fn</code> with <code>RaggedTensor</code>s (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37679\">CVE-2021-37679</a>)</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\">tensorflow-cpu's changelog</a>.</em></p>\n<blockquote>\n<h1>Release 2.5.1</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a heap out of bounds access in sparse reduction operations\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635\">CVE-2021-37635</a>)</li>\n<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636\">CVE-2021-37636</a>)</li>\n<li>Fixes a null pointer dereference in <code>CompressElement</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637\">CVE-2021-37637</a>)</li>\n<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638\">CVE-2021-37638</a>)</li>\n<li>Fixes a null pointer dereference and a heap OOB read arising from operations\nrestoring tensors\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639\">CVE-2021-37639</a>)</li>\n<li>Fixes an integer division by 0 in sparse reshaping\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640\">CVE-2021-37640</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceScatterDiv</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37642\">CVE-2021-37642</a>)</li>\n<li>Fixes a heap OOB in <code>RaggedGather</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37641\">CVE-2021-37641</a>)</li>\n<li>Fixes a <code>std::abort</code> raised from <code>TensorListReserve</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37644\">CVE-2021-37644</a>)</li>\n<li>Fixes a null pointer dereference in <code>MatrixDiagPartOp</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37643\">CVE-2021-37643</a>)</li>\n<li>Fixes an integer overflow due to conversion to unsigned\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37645\">CVE-2021-37645</a>)</li>\n<li>Fixes a bad allocation error in <code>StringNGrams</code> caused by integer conversion\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37646\">CVE-2021-37646</a>)</li>\n<li>Fixes a null pointer dereference in <code>SparseTensorSliceDataset</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37647\">CVE-2021-37647</a>)</li>\n<li>Fixes an incorrect validation of <code>SaveV2</code> inputs\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37648\">CVE-2021-37648</a>)</li>\n<li>Fixes a null pointer dereference in <code>UncompressElement</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37649\">CVE-2021-37649</a>)</li>\n<li>Fixes a segfault and a heap buffer overflow in\n<code>{Experimental,}DatasetToTFRecord</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37650\">CVE-2021-37650</a>)</li>\n<li>Fixes a heap buffer overflow in <code>FractionalAvgPoolGrad</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37651\">CVE-2021-37651</a>)</li>\n<li>Fixes a use after free in boosted trees creation\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37652\">CVE-2021-37652</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceGather</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37653\">CVE-2021-37653</a>)</li>\n<li>Fixes a heap OOB and a <code>CHECK</code> fail in <code>ResourceGather</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37654\">CVE-2021-37654</a>)</li>\n<li>Fixes a heap OOB in <code>ResourceScatterUpdate</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37655\">CVE-2021-37655</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in\n<code>RaggedTensorToSparse</code></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/8222c1cfc866126111f23bd9872998480cebf2c1\"><code>8222c1c</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51381\">#51381</a> from tensorflow/mm-fix-r2.5-build</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/d5842603e03504d8ed30b0622e03869899c9f41d\"><code>d584260</code></a> Disable broken/flaky test</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/f6c6ce30bab35320e5da6e25fbdd8c369de75ab7\"><code>f6c6ce3</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51367\">#51367</a> from tensorflow-jenkins/version-numbers-2.5.1-17468</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/3ca781272c60959f3a24a2b440f2f275aab71a76\"><code>3ca7812</code></a> Update version numbers to 2.5.1</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/4fdf683c878574bc2c39fe8ac152ffc26183efb6\"><code>4fdf683</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51361\">#51361</a> from tensorflow/mm-update-relnotes-on-r2.5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/05fc01aa0ffe973a2b1517bd92479e38f5d2c72a\"><code>05fc01a</code></a> Put CVE numbers for fixes in parentheses</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/bee1dc4a6116b53101fc8773f43662a89514847d\"><code>bee1dc4</code></a> Update release notes for the new patch release</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/47beb4c1987293659784d6aa1dfaacc86bc07d84\"><code>47beb4c</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/50597\">#50597</a> from kruglov-dmitry/v2.5.0-sync-abseil-cmake-bazel</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/6f39597952e230d2a782547380cdf8143bdcdc5d\"><code>6f39597</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49383\">#49383</a> from ashahab/abin-load-segfault-r2.5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/0539b34641ee0773f07d859fe69dc0dfc71069d3\"><code>0539b34</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/48979\">#48979</a> from liufengdb/r2.5-cherrypick</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-cpu&package-manager=pip&previous-version=2.4.0&new-version=2.5.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "d6e30bd494398c439fbe42ccf9b8d54d4271b327", "html_url": "https://github.com/backtick-se/cowait/commit/d6e30bd494398c439fbe42ccf9b8d54d4271b327", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-08-25T14:40:05Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-08-25T14:40:05Z"}, "message": "Bump tensorflow-cpu from 2.4.0 to 2.5.1 in /examples/10-imdb\n\nBumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.1.\n- [Release notes](https://github.com/tensorflow/tensorflow/releases)\n- [Changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md)\n- [Commits](https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.1)\n\n---\nupdated-dependencies:\n- dependency-name: tensorflow-cpu\n  dependency-type: direct:production\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "517150c2c769a06248d09c7e8ef49f0bc0ffd2dc", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/517150c2c769a06248d09c7e8ef49f0bc0ffd2dc"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/d6e30bd494398c439fbe42ccf9b8d54d4271b327", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJhJlZFCRBK7hj4Ov3rIwAAd10IADK4ouILHb7Imt2j8sZScLVB\n+ixbRm+N2GrfHuZnbefLcpW6iqDme46oEPviuU/nmVpQ/wHlmVND/YwIOcS+pcBF\nBRfEUzEU2iGfv6O4azMZQdqU86EUMq6iRGJaLXXU1rF0X6noihawiXNb1zmUjvCK\nFTcKIwyHJN4I2tKQh68XfqvwU8OXLQRoE03Y+2jppKdKmfAP4EW0laSsbsJGNWTy\nstBh3oGqygAnIvE46RJR1CNDuhzwB8isD6z8KkSF58574WVB+hWIRwHlTAoBYep8\nod7fm1j4W7PR0Lvlp7J6GAoUs0p6ROJgFv3cSPflO3v9rdokbIz56ECa0wqGx+s=\n=J670\n-----END PGP SIGNATURE-----\n", "payload": "tree 517150c2c769a06248d09c7e8ef49f0bc0ffd2dc\nparent e3c9b9ad1a32fa6b9bd7289798e91775d2f33dd4\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1629902405 +0000\ncommitter GitHub <noreply@github.com> 1629902405 +0000\n\nBump tensorflow-cpu from 2.4.0 to 2.5.1 in /examples/10-imdb\n\nBumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.1.\n- [Release notes](https://github.com/tensorflow/tensorflow/releases)\n- [Changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md)\n- [Commits](https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.1)\n\n---\nupdated-dependencies:\n- dependency-name: tensorflow-cpu\n  dependency-type: direct:production\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "a962cf0b0d03d03ba128d9ff0636a524d6de8df8", "filename": "examples/10-imdb/requirements.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/d6e30bd494398c439fbe42ccf9b8d54d4271b327/examples/10-imdb/requirements.txt", "raw_url": "https://github.com/backtick-se/cowait/raw/d6e30bd494398c439fbe42ccf9b8d54d4271b327/examples/10-imdb/requirements.txt", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/examples/10-imdb/requirements.txt?ref=d6e30bd494398c439fbe42ccf9b8d54d4271b327", "patch": "@@ -1,2 +1,2 @@\n-tensorflow-cpu==2.4.0\n+tensorflow-cpu==2.5.1\n keras==2.4.0"}], "stats": {"total": 2, "additions": 1, "deletions": 1}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 330, "title": "Bump tensorflow-cpu from 2.4.0 to 2.5.1 in /examples/06-tensorflow", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2588709704, "node_id": "MDU6TGFiZWwyNTg4NzA5NzA0", "url": "https://api.github.com/repos/backtick-se/cowait/labels/python", "name": "python", "color": "2b67c6", "default": false, "description": "Pull requests that update Python code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/330", "html_url": "https://github.com/backtick-se/cowait/pull/330", "diff_url": "https://github.com/backtick-se/cowait/pull/330.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/330.patch", "merged_at": null}, "body": "Bumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/releases\">tensorflow-cpu's releases</a>.</em></p>\n<blockquote>\n<h2>TensorFlow 2.5.1</h2>\n<h1>Release 2.5.1</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a heap out of bounds access in sparse reduction operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635\">CVE-2021-37635</a>)</li>\n<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636\">CVE-2021-37636</a>)</li>\n<li>Fixes a null pointer dereference in <code>CompressElement</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637\">CVE-2021-37637</a>)</li>\n<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638\">CVE-2021-37638</a>)</li>\n<li>Fixes a null pointer dereference and a heap OOB read arising from operations restoring tensors (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639\">CVE-2021-37639</a>)</li>\n<li>Fixes an integer division by 0 in sparse reshaping (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640\">CVE-2021-37640</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceScatterDiv</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37642\">CVE-2021-37642</a>)</li>\n<li>Fixes a heap OOB in <code>RaggedGather</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37641\">CVE-2021-37641</a>)</li>\n<li>Fixes a <code>std::abort</code> raised from <code>TensorListReserve</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37644\">CVE-2021-37644</a>)</li>\n<li>Fixes a null pointer dereference in <code>MatrixDiagPartOp</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37643\">CVE-2021-37643</a>)</li>\n<li>Fixes an integer overflow due to conversion to unsigned (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37645\">CVE-2021-37645</a>)</li>\n<li>Fixes a bad allocation error in <code>StringNGrams</code> caused by integer conversion (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37646\">CVE-2021-37646</a>)</li>\n<li>Fixes a null pointer dereference in <code>SparseTensorSliceDataset</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37647\">CVE-2021-37647</a>)</li>\n<li>Fixes an incorrect validation of <code>SaveV2</code> inputs (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37648\">CVE-2021-37648</a>)</li>\n<li>Fixes a null pointer dereference in <code>UncompressElement</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37649\">CVE-2021-37649</a>)</li>\n<li>Fixes a segfault and a heap buffer overflow in <code>{Experimental,}DatasetToTFRecord</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37650\">CVE-2021-37650</a>)</li>\n<li>Fixes a heap buffer overflow in <code>FractionalAvgPoolGrad</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37651\">CVE-2021-37651</a>)</li>\n<li>Fixes a use after free in boosted trees creation (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37652\">CVE-2021-37652</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceGather</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37653\">CVE-2021-37653</a>)</li>\n<li>Fixes a heap OOB and a <code>CHECK</code> fail in <code>ResourceGather</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37654\">CVE-2021-37654</a>)</li>\n<li>Fixes a heap OOB in <code>ResourceScatterUpdate</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37655\">CVE-2021-37655</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>RaggedTensorToSparse</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37656\">CVE-2021-37656</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>MatrixDiagV*</code> ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37657\">CVE-2021-37657</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>MatrixSetDiagV*</code> ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37658\">CVE-2021-37658</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr and heap OOB in binary cwise ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37659\">CVE-2021-37659</a>)</li>\n<li>Fixes a division by 0 in inplace operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37660\">CVE-2021-37660</a>)</li>\n<li>Fixes a crash caused by integer conversion to unsigned (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37661\">CVE-2021-37661</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in boosted trees (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37662\">CVE-2021-37662</a>)</li>\n<li>Fixes a heap OOB in boosted trees (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37664\">CVE-2021-37664</a>)</li>\n<li>Fixes vulnerabilities arising from incomplete validation in <code>QuantizeV2</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37663\">CVE-2021-37663</a>)</li>\n<li>Fixes vulnerabilities arising from incomplete validation in MKL requantization (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37665\">CVE-2021-37665</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>RaggedTensorToVariant</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37666\">CVE-2021-37666</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in unicode encoding (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37667\">CVE-2021-37667</a>)</li>\n<li>Fixes an FPE in <code>tf.raw_ops.UnravelIndex</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37668\">CVE-2021-37668</a>)</li>\n<li>Fixes a crash in NMS ops caused by integer conversion to unsigned (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37669\">CVE-2021-37669</a>)</li>\n<li>Fixes a heap OOB in <code>UpperBound</code> and <code>LowerBound</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37670\">CVE-2021-37670</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in map operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37671\">CVE-2021-37671</a>)</li>\n<li>Fixes a heap OOB in <code>SdcaOptimizerV2</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37672\">CVE-2021-37672</a>)</li>\n<li>Fixes a <code>CHECK</code>-fail in <code>MapStage</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37673\">CVE-2021-37673</a>)</li>\n<li>Fixes a vulnerability arising from incomplete validation in <code>MaxPoolGrad</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37674\">CVE-2021-37674</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in shape inference (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37676\">CVE-2021-37676</a>)</li>\n<li>Fixes a division by 0 in most convolution operators (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37675\">CVE-2021-37675</a>)</li>\n<li>Fixes vulnerabilities arising from missing validation in shape inference for <code>Dequantize</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37677\">CVE-2021-37677</a>)</li>\n<li>Fixes an arbitrary code execution due to YAML deserialization (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37678\">CVE-2021-37678</a>)</li>\n<li>Fixes a heap OOB in nested <code>tf.map_fn</code> with <code>RaggedTensor</code>s (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37679\">CVE-2021-37679</a>)</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\">tensorflow-cpu's changelog</a>.</em></p>\n<blockquote>\n<h1>Release 2.5.1</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a heap out of bounds access in sparse reduction operations\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635\">CVE-2021-37635</a>)</li>\n<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636\">CVE-2021-37636</a>)</li>\n<li>Fixes a null pointer dereference in <code>CompressElement</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637\">CVE-2021-37637</a>)</li>\n<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638\">CVE-2021-37638</a>)</li>\n<li>Fixes a null pointer dereference and a heap OOB read arising from operations\nrestoring tensors\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639\">CVE-2021-37639</a>)</li>\n<li>Fixes an integer division by 0 in sparse reshaping\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640\">CVE-2021-37640</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceScatterDiv</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37642\">CVE-2021-37642</a>)</li>\n<li>Fixes a heap OOB in <code>RaggedGather</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37641\">CVE-2021-37641</a>)</li>\n<li>Fixes a <code>std::abort</code> raised from <code>TensorListReserve</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37644\">CVE-2021-37644</a>)</li>\n<li>Fixes a null pointer dereference in <code>MatrixDiagPartOp</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37643\">CVE-2021-37643</a>)</li>\n<li>Fixes an integer overflow due to conversion to unsigned\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37645\">CVE-2021-37645</a>)</li>\n<li>Fixes a bad allocation error in <code>StringNGrams</code> caused by integer conversion\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37646\">CVE-2021-37646</a>)</li>\n<li>Fixes a null pointer dereference in <code>SparseTensorSliceDataset</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37647\">CVE-2021-37647</a>)</li>\n<li>Fixes an incorrect validation of <code>SaveV2</code> inputs\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37648\">CVE-2021-37648</a>)</li>\n<li>Fixes a null pointer dereference in <code>UncompressElement</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37649\">CVE-2021-37649</a>)</li>\n<li>Fixes a segfault and a heap buffer overflow in\n<code>{Experimental,}DatasetToTFRecord</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37650\">CVE-2021-37650</a>)</li>\n<li>Fixes a heap buffer overflow in <code>FractionalAvgPoolGrad</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37651\">CVE-2021-37651</a>)</li>\n<li>Fixes a use after free in boosted trees creation\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37652\">CVE-2021-37652</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceGather</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37653\">CVE-2021-37653</a>)</li>\n<li>Fixes a heap OOB and a <code>CHECK</code> fail in <code>ResourceGather</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37654\">CVE-2021-37654</a>)</li>\n<li>Fixes a heap OOB in <code>ResourceScatterUpdate</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37655\">CVE-2021-37655</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in\n<code>RaggedTensorToSparse</code></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/8222c1cfc866126111f23bd9872998480cebf2c1\"><code>8222c1c</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51381\">#51381</a> from tensorflow/mm-fix-r2.5-build</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/d5842603e03504d8ed30b0622e03869899c9f41d\"><code>d584260</code></a> Disable broken/flaky test</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/f6c6ce30bab35320e5da6e25fbdd8c369de75ab7\"><code>f6c6ce3</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51367\">#51367</a> from tensorflow-jenkins/version-numbers-2.5.1-17468</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/3ca781272c60959f3a24a2b440f2f275aab71a76\"><code>3ca7812</code></a> Update version numbers to 2.5.1</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/4fdf683c878574bc2c39fe8ac152ffc26183efb6\"><code>4fdf683</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51361\">#51361</a> from tensorflow/mm-update-relnotes-on-r2.5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/05fc01aa0ffe973a2b1517bd92479e38f5d2c72a\"><code>05fc01a</code></a> Put CVE numbers for fixes in parentheses</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/bee1dc4a6116b53101fc8773f43662a89514847d\"><code>bee1dc4</code></a> Update release notes for the new patch release</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/47beb4c1987293659784d6aa1dfaacc86bc07d84\"><code>47beb4c</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/50597\">#50597</a> from kruglov-dmitry/v2.5.0-sync-abseil-cmake-bazel</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/6f39597952e230d2a782547380cdf8143bdcdc5d\"><code>6f39597</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49383\">#49383</a> from ashahab/abin-load-segfault-r2.5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/0539b34641ee0773f07d859fe69dc0dfc71069d3\"><code>0539b34</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/48979\">#48979</a> from liufengdb/r2.5-cherrypick</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-cpu&package-manager=pip&previous-version=2.4.0&new-version=2.5.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "1ea36b29d7415604718ba939f7beca889609b983", "html_url": "https://github.com/backtick-se/cowait/commit/1ea36b29d7415604718ba939f7beca889609b983", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-08-25T14:39:39Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-08-25T14:39:39Z"}, "message": "Bump tensorflow-cpu from 2.4.0 to 2.5.1 in /examples/06-tensorflow\n\nBumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.1.\n- [Release notes](https://github.com/tensorflow/tensorflow/releases)\n- [Changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md)\n- [Commits](https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.1)\n\n---\nupdated-dependencies:\n- dependency-name: tensorflow-cpu\n  dependency-type: direct:production\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "0c05b4e4f484e57639e4f68de3388107afa91c4c", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/0c05b4e4f484e57639e4f68de3388107afa91c4c"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/1ea36b29d7415604718ba939f7beca889609b983", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJhJlYrCRBK7hj4Ov3rIwAAHd8IAF3BrB+JF+inOiiV++1l1bgO\n7ZLLQZzwKJMLP4bGzZZDEEMu/eim1pv/K7NwaS7fgyNG6HWOQAoFM8kbnPJAf393\nBUW2K3/mSnfRuNqJ+EgVFQhuxxdY7Bt5xBHTVBRY1sihwIocqlPzilwZLSu3ubch\nRzMEf/1LgZv3kLuF0/qiNf2jnLT0Ziu1WPfCdWLpJJxzVjwNRf4zBjfvAcmaSZXr\ndqPJc3I35cAvcOLuzBRZiKmjqAwxZWoK5W3gfUnNIqtdWxnoC+GKxAqA0p/Z2HnG\nFj+L+rM/K3R7QRmmAcAZlpqXgH5LBuRozENgAv70ro4vxMsYqsX3clXiT25gpgU=\n=ggsU\n-----END PGP SIGNATURE-----\n", "payload": "tree 0c05b4e4f484e57639e4f68de3388107afa91c4c\nparent e3c9b9ad1a32fa6b9bd7289798e91775d2f33dd4\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1629902379 +0000\ncommitter GitHub <noreply@github.com> 1629902379 +0000\n\nBump tensorflow-cpu from 2.4.0 to 2.5.1 in /examples/06-tensorflow\n\nBumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.1.\n- [Release notes](https://github.com/tensorflow/tensorflow/releases)\n- [Changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md)\n- [Commits](https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.1)\n\n---\nupdated-dependencies:\n- dependency-name: tensorflow-cpu\n  dependency-type: direct:production\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "a962cf0b0d03d03ba128d9ff0636a524d6de8df8", "filename": "examples/06-tensorflow/requirements.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/1ea36b29d7415604718ba939f7beca889609b983/examples/06-tensorflow/requirements.txt", "raw_url": "https://github.com/backtick-se/cowait/raw/1ea36b29d7415604718ba939f7beca889609b983/examples/06-tensorflow/requirements.txt", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/examples/06-tensorflow/requirements.txt?ref=1ea36b29d7415604718ba939f7beca889609b983", "patch": "@@ -1,2 +1,2 @@\n-tensorflow-cpu==2.4.0\n+tensorflow-cpu==2.5.1\n keras==2.4.0"}], "stats": {"total": 2, "additions": 1, "deletions": 1}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 329, "title": "Bump path-parse from 1.0.6 to 1.0.7 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/329", "html_url": "https://github.com/backtick-se/cowait/pull/329", "diff_url": "https://github.com/backtick-se/cowait/pull/329.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/329.patch", "merged_at": "2022-02-21T09:50:58Z"}, "body": "Bumps [path-parse](https://github.com/jbgutierrez/path-parse) from 1.0.6 to 1.0.7.\n<details>\n<summary>Commits</summary>\n<ul>\n<li>See full diff in <a href=\"https://github.com/jbgutierrez/path-parse/commits/v1.0.7\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=path-parse&package-manager=npm_and_yarn&previous-version=1.0.6&new-version=1.0.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "d5b3380ff78ceaa2c3a1446e15a638e070907c01", "html_url": "https://github.com/backtick-se/cowait/commit/d5b3380ff78ceaa2c3a1446e15a638e070907c01", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-08-12T14:01:52Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-08-12T14:01:52Z"}, "message": "Bump path-parse from 1.0.6 to 1.0.7 in /cloud\n\nBumps [path-parse](https://github.com/jbgutierrez/path-parse) from 1.0.6 to 1.0.7.\n- [Release notes](https://github.com/jbgutierrez/path-parse/releases)\n- [Commits](https://github.com/jbgutierrez/path-parse/commits/v1.0.7)\n\n---\nupdated-dependencies:\n- dependency-name: path-parse\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "78e3f76039d19e2f5bac7aa3a2589d540bac5944", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/78e3f76039d19e2f5bac7aa3a2589d540bac5944"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/d5b3380ff78ceaa2c3a1446e15a638e070907c01", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJhFSnQCRBK7hj4Ov3rIwAArDAIAFoxBIhG3xfOzs01++mhYsQ/\n370vs15B8vAdCkJqwGZc7rtsdNxSgjCAPh98UOjXdDJp6A9510p8oS8S/F9Z7eNv\nypOYGwsysIhf6v5aZN9eLtyBhkzVBZcWe20AnZE/8ErDndtlANnYTAnRTb++JeSv\nmVVJh0wrGyE8oKVoFzmX+msvWj+Ml7pfwhX8O3ryhjBzC9zvxUmdOOxFiINLf2Y1\n7xBCP5cxQ8scSYHJcwOTQcWD2kBDIC7qK2nDdanFuTprMyDHJAYMvf/LYN9RE0ET\nbRwEtMOwZvazq8D8zRbngN3RwUFUEVQmdzciaXHApUyIi6wjdMdgOXdI4KJwoe8=\n=LQDm\n-----END PGP SIGNATURE-----\n", "payload": "tree 78e3f76039d19e2f5bac7aa3a2589d540bac5944\nparent e3c9b9ad1a32fa6b9bd7289798e91775d2f33dd4\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1628776912 +0000\ncommitter GitHub <noreply@github.com> 1628776912 +0000\n\nBump path-parse from 1.0.6 to 1.0.7 in /cloud\n\nBumps [path-parse](https://github.com/jbgutierrez/path-parse) from 1.0.6 to 1.0.7.\n- [Release notes](https://github.com/jbgutierrez/path-parse/releases)\n- [Commits](https://github.com/jbgutierrez/path-parse/commits/v1.0.7)\n\n---\nupdated-dependencies:\n- dependency-name: path-parse\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "55d24f2384b4cdef8be8a451cd15d404945aed76", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/d5b3380ff78ceaa2c3a1446e15a638e070907c01/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/d5b3380ff78ceaa2c3a1446e15a638e070907c01/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=d5b3380ff78ceaa2c3a1446e15a638e070907c01", "patch": "@@ -7746,9 +7746,9 @@ path-key@^2.0.0, path-key@^2.0.1:\n   integrity sha1-QRyttXTFoUDTpLGRDUDYDMn0C0A=\n \n path-parse@^1.0.6:\n-  version \"1.0.6\"\n-  resolved \"https://registry.yarnpkg.com/path-parse/-/path-parse-1.0.6.tgz#d62dbb5679405d72c4737ec58600e9ddcf06d24c\"\n-  integrity sha512-GSmOT2EbHrINBf9SR7CDELwlJ8AENk3Qn7OikK4nFYAu3Ote2+JYNVvkpAEQm3/TLNEJFD/xZJjzyxg3KBWOzw==\n+  version \"1.0.7\"\n+  resolved \"https://registry.yarnpkg.com/path-parse/-/path-parse-1.0.7.tgz#fbc114b60ca42b30d9daf5858e4bd68bbedb6735\"\n+  integrity sha512-LDJzPVEEEPR+y48z93A0Ed0yXb8pAByGWo/k5YYdYgpY2/2EsOsksJrq7lOHxryrVOn1ejG6oAp8ahvOIQD8sw==\n \n path-to-regexp@0.1.7:\n   version \"0.1.7\""}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 327, "title": "Improve logs command", "labels": [{"id": 1562464620, "node_id": "MDU6TGFiZWwxNTYyNDY0NjIw", "url": "https://api.github.com/repos/backtick-se/cowait/labels/enhancement", "name": "enhancement", "color": "9cdd56", "default": true, "description": "Improve existing features"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/327", "html_url": "https://github.com/backtick-se/cowait/pull/327", "diff_url": "https://github.com/backtick-se/cowait/pull/327.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/327.patch", "merged_at": "2021-07-27T12:26:44Z"}, "body": "- `logs` command now allows reading logs from terminated Kubernetes pods\r\n- Logs show an error if the task appears to have been lost during execution (i.e. OOM killed or manually deleted)", "commits": [{"sha": "9977d05cb82814e7928f3ab6f7a90b5c23374e18", "html_url": "https://github.com/backtick-se/cowait/commit/9977d05cb82814e7928f3ab6f7a90b5c23374e18", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-07-23T09:43:25Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-07-23T09:43:25Z"}, "message": "read logs from terminated pods", "tree": {"sha": "1b54b7105879292dac6e69a04e3b371fcb4da818", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/1b54b7105879292dac6e69a04e3b371fcb4da818"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/9977d05cb82814e7928f3ab6f7a90b5c23374e18", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "ecb1bbaf839cd15a7f35e30e477ee1ec9dc66239", "filename": "cowait/engine/kubernetes/kubernetes.py", "status": "modified", "additions": 15, "deletions": 1, "changes": 16, "blob_url": "https://github.com/backtick-se/cowait/blob/9977d05cb82814e7928f3ab6f7a90b5c23374e18/cowait/engine/kubernetes/kubernetes.py", "raw_url": "https://github.com/backtick-se/cowait/raw/9977d05cb82814e7928f3ab6f7a90b5c23374e18/cowait/engine/kubernetes/kubernetes.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/kubernetes/kubernetes.py?ref=9977d05cb82814e7928f3ab6f7a90b5c23374e18", "patch": "@@ -186,7 +186,20 @@ def wait_until_deleted(self, task_id: str, poll_interval: float = 1):\n             time.sleep(poll_interval)\n \n     def logs(self, task_id: str):\n-        # wait for pod to become ready\n+        # if the pod has already terminated attempt to retrieve all the logs at once\n+        try:\n+            pod = self.get_task_pod(task_id)\n+            pod_is_ready(pod)\n+\n+        except PodTerminatedError:\n+            log = self.core.read_namespaced_pod_log(name=task_id, namespace=self.namespace)\n+            return json_stream(log.splitlines(True))\n+\n+        except Exception:\n+            # other errors will be caught later in wait_until_ready\n+            pass\n+\n+        # otherwise, wait for the pod to become ready and stream logs\n         self.wait_until_ready(task_id)\n \n         try:\n@@ -204,6 +217,7 @@ def add_newline_stream():\n             return json_stream(add_newline_stream())\n \n         except Exception:\n+            # this looks like a terrible idea\n             return self.logs(task_id)\n \n     def destroy_all(self) -> list:"}, {"sha": "9c64f64cce705cb6478a19d7a8f24b009e9bd9a9", "filename": "cowait/engine/kubernetes/pod.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/backtick-se/cowait/blob/9977d05cb82814e7928f3ab6f7a90b5c23374e18/cowait/engine/kubernetes/pod.py", "raw_url": "https://github.com/backtick-se/cowait/raw/9977d05cb82814e7928f3ab6f7a90b5c23374e18/cowait/engine/kubernetes/pod.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/kubernetes/pod.py?ref=9977d05cb82814e7928f3ab6f7a90b5c23374e18", "patch": "@@ -1,4 +1,3 @@\n-import json\n from cowait.tasks import TaskDefinition\n from cowait.engine.const import ENV_TASK_DEFINITION\n from cowait.engine.utils import env_unpack"}], "stats": {"total": 17, "additions": 15, "deletions": 2}}, {"sha": "6f67103d5854bd1b3056425b03ff9df56df12941", "html_url": "https://github.com/backtick-se/cowait/commit/6f67103d5854bd1b3056425b03ff9df56df12941", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-07-27T12:02:56Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-07-27T12:20:34Z"}, "message": "show an error in log output if the log stream ends without a return or an error", "tree": {"sha": "6caee7b4e31e235b88c6bcb7e2758bcc3dc281b7", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/6caee7b4e31e235b88c6bcb7e2758bcc3dc281b7"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/6f67103d5854bd1b3056425b03ff9df56df12941", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "d8ff302aafaaaf4650478eb85cbf7e5b11c2f4fc", "filename": "cowait/cli/commands/run.py", "status": "modified", "additions": 26, "deletions": 1, "changes": 27, "blob_url": "https://github.com/backtick-se/cowait/blob/6f67103d5854bd1b3056425b03ff9df56df12941/cowait/cli/commands/run.py", "raw_url": "https://github.com/backtick-se/cowait/raw/6f67103d5854bd1b3056425b03ff9df56df12941/cowait/cli/commands/run.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/run.py?ref=6f67103d5854bd1b3056425b03ff9df56df12941", "patch": "@@ -2,6 +2,7 @@\n import json\n import getpass\n import docker.errors\n+from datetime import datetime\n from cowait.tasks.definition import TaskDefinition, generate_task_id\n from cowait.engine.errors import TaskCreationError, ProviderError\n from cowait.tasks.messages import TASK_INIT, TASK_STATUS, TASK_FAIL, TASK_RETURN, TASK_LOG\n@@ -123,6 +124,7 @@ def destroy(*args):\n             logger.header('task output')\n             for msg in logs:\n                 logger.handle(msg)\n+            logger.finalize()\n \n         logger.header()\n \n@@ -147,6 +149,8 @@ def __init__(self, raw: bool = False, quiet: bool = False, time: bool = True):\n         super().__init__(quiet, time)\n         self.raw = raw\n         self.idlen = 0\n+        self.returned = False\n+        self.failed = False\n \n     @property\n     def newline_indent(self):\n@@ -168,8 +172,14 @@ def handle(self, msg):\n             if type == TASK_INIT:\n                 self.on_init(**msg)\n             elif type == TASK_RETURN:\n+                if msg['id'] == self.id:\n+                    # mark task as returned\n+                    self.returned = True\n                 self.on_return(**msg)\n             elif type == TASK_FAIL:\n+                if msg['id'] == self.id:\n+                    # mark task as failed\n+                    self.failed = True\n                 self.on_fail(**msg)\n             elif type == TASK_STATUS:\n                 pass\n@@ -204,8 +214,10 @@ def print(self, *args):\n             return\n         super().print(*args)\n \n-    def print_id(self, id, ts=None, short=True, pad=True):\n+    def print_id(self, id, short=True, pad=True):\n         color = fg(hash(id) % 214 + 17)\n+        if id == 'system':\n+            color = fg('red')\n         if short and '-' in id:\n             id = id[:id.rfind('-')]\n             self.idlen = max(self.idlen, len(id))\n@@ -246,3 +258,16 @@ def on_log(self, id: str, file: str, data: str, ts: str = None, **msg):\n         self.print_time(ts)\n         self.print_id(id)\n         self.println('  ', data)\n+\n+    def finalize(self):\n+        if self.returned or self.failed:\n+            return\n+        ts = datetime.now().isoformat()\n+        error = (f'Reached end of log stream without return or failure. '\n+                 f'Task {self.id} appears to have been lost.')\n+        if self.raw:\n+            # print a raw error\n+            if not self.quiet:\n+                print(json.dumps({'id': 'system', 'type': TASK_FAIL, 'error': error, 'ts': ts}))\n+        else:\n+            self.on_fail(id='system', ts=ts, error=error)"}, {"sha": "5a6c1c184954f9bce53e5422d98a1f111893fb97", "filename": "cowait/cli/commands/task.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/6f67103d5854bd1b3056425b03ff9df56df12941/cowait/cli/commands/task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/6f67103d5854bd1b3056425b03ff9df56df12941/cowait/cli/commands/task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/task.py?ref=6f67103d5854bd1b3056425b03ff9df56df12941", "patch": "@@ -1,6 +1,6 @@\n+from cowait.engine import ProviderError\n from ..config import Config\n from ..context import Context\n-from cowait.engine import ProviderError\n from .run import RunLogger\n \n \n@@ -45,15 +45,16 @@ def logs(config: Config, task_id: str, cluster_name: str, raw: bool = False):\n     try:\n         context = Context.open(config)\n         cluster = context.get_cluster(cluster_name)\n-    \n+\n         logs = cluster.logs(task_id)\n         logger = RunLogger(raw)\n         logger.id = task_id\n         logger.header('task output')\n         for msg in logs:\n             logger.handle(msg)\n+        logger.finalize()\n+\n         logger.header()\n \n     except ProviderError as e:\n         print('Provider error:', str(e))\n-"}], "stats": {"total": 34, "additions": 30, "deletions": 4}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 326, "title": "File descriptor based output capturing", "labels": [{"id": 1562464620, "node_id": "MDU6TGFiZWwxNTYyNDY0NjIw", "url": "https://api.github.com/repos/backtick-se/cowait/labels/enhancement", "name": "enhancement", "color": "9cdd56", "default": true, "description": "Improve existing features"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/326", "html_url": "https://github.com/backtick-se/cowait/pull/326", "diff_url": "https://github.com/backtick-se/cowait/pull/326.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/326.patch", "merged_at": "2021-07-23T09:19:06Z"}, "body": "Allows capturing of subprocess output.\r\n\r\nImplementation based on pytest's `FDCapture`", "commits": [{"sha": "002751088342d0af6af805b93014c678b4e8274e", "html_url": "https://github.com/backtick-se/cowait/commit/002751088342d0af6af805b93014c678b4e8274e", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-07-22T14:55:55Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-07-23T09:18:35Z"}, "message": "reduce stats spam interval", "tree": {"sha": "1fdf0e4d50ca4baef2853138416ac6dbb6293624", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/1fdf0e4d50ca4baef2853138416ac6dbb6293624"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/002751088342d0af6af805b93014c678b4e8274e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "37819451beb4172f50feb8fe27f503151e0e47ef", "filename": "cowait/worker/executor.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/002751088342d0af6af805b93014c678b4e8274e/cowait/worker/executor.py", "raw_url": "https://github.com/backtick-se/cowait/raw/002751088342d0af6af805b93014c678b4e8274e/cowait/worker/executor.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/worker/executor.py?ref=002751088342d0af6af805b93014c678b4e8274e", "patch": "@@ -33,7 +33,7 @@ async def execute(cluster: ClusterProvider, taskdef: TaskDefinition) -> None:\n             )\n \n             # monitor system resources\n-            node.monitor_system(interval=2)\n+            node.monitor_system(interval=10)\n \n             # initialize task\n             task.init()"}], "stats": {"total": 2, "additions": 1, "deletions": 1}}, {"sha": "1552e25dc3d42d7ab69b00705719c7c3acd0283c", "html_url": "https://github.com/backtick-se/cowait/commit/1552e25dc3d42d7ab69b00705719c7c3acd0283c", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-07-22T14:56:31Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-07-23T09:18:35Z"}, "message": "add file descriptor based output capturing utils", "tree": {"sha": "7e558bb06add26dd96a8615039ff07e5181b2f9a", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/7e558bb06add26dd96a8615039ff07e5181b2f9a"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/1552e25dc3d42d7ab69b00705719c7c3acd0283c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "d8d6bfc7ac2e9909a1cc44c31e68b8e3848dfe1c", "filename": "cowait/utils/file_capture.py", "status": "added", "additions": 99, "deletions": 0, "changes": 99, "blob_url": "https://github.com/backtick-se/cowait/blob/1552e25dc3d42d7ab69b00705719c7c3acd0283c/cowait/utils/file_capture.py", "raw_url": "https://github.com/backtick-se/cowait/raw/1552e25dc3d42d7ab69b00705719c7c3acd0283c/cowait/utils/file_capture.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/utils/file_capture.py?ref=1552e25dc3d42d7ab69b00705719c7c3acd0283c", "patch": "@@ -0,0 +1,99 @@\n+import os\n+import io\n+import sys\n+from tempfile import TemporaryFile\n+\n+SYS_FDS = {\n+    1: 'stdout',\n+    2: 'stderr',\n+}\n+\n+\n+class CallbackFile(io.TextIOWrapper):\n+    def __init__(self, callback: callable = None):\n+        super().__init__(\n+            TemporaryFile(buffering=0),\n+            encoding=\"utf-8\",\n+            errors=\"replace\",\n+            newline=\"\",\n+            write_through=True,\n+        )\n+        self.callback = callback\n+\n+    def write(self, data):\n+        super().write(data)\n+        if '\\n' in data:\n+            self.flush()\n+\n+    def getvalue(self):\n+        self.buffer.seek(0)\n+        res = self.buffer.read()\n+        self.buffer.seek(0)\n+        self.buffer.truncate()\n+        return res.decode('utf-8')\n+\n+    def flush(self):\n+        if self.callback is not None:\n+            text = self.getvalue()\n+            if len(text) > 0:\n+                self.callback(text)\n+        else:\n+            super().flush()\n+\n+\n+class FDCapture:\n+    def __init__(self, targetfd: int, callback: callable = None) -> None:\n+        self.targetfd = targetfd\n+        self.targetfd_save = os.dup(targetfd)\n+        self.tmpfile = CallbackFile(callback)\n+        self._capturing = False\n+\n+    def start(self) -> None:\n+        \"\"\"Start capturing on targetfd using memorized tmpfile.\"\"\"\n+        assert not self._capturing\n+\n+        # redirect the target fd to our temporary file\n+        os.dup2(self.tmpfile.fileno(), self.targetfd)\n+\n+        if self.targetfd in SYS_FDS:\n+            # for system file descriptors (stdout, stderr) we also need to replace the sys.stdout/sys.stderr streams.\n+            # this allows us to capture output from the local python process\n+            self.sysfile = getattr(sys, SYS_FDS[self.targetfd])\n+            setattr(sys, SYS_FDS[self.targetfd], self.tmpfile)\n+\n+        self._capturing = True\n+\n+    def stop(self) -> None:\n+        \"\"\"Stop capturing, restore streams & return original capture file\"\"\"\n+        assert self._capturing\n+\n+        # disable output redirection\n+        os.dup2(self.targetfd_save, self.targetfd)\n+\n+        # close temporary file descriptor\n+        os.close(self.targetfd_save)\n+\n+        if self.targetfd in SYS_FDS:\n+            # restore the original sys stream\n+            setattr(sys, SYS_FDS[self.targetfd], self.sysfile)\n+\n+        self._capturing = False\n+\n+    def writeorg(self, data):\n+        \"\"\"Write to original file descriptor.\"\"\"\n+        assert self._capturing\n+        os.write(self.targetfd_save, data.encode('utf-8'))\n+\n+    def snap(self):\n+        return self.tmpfile.getvalue()\n+\n+    def __del__(self):\n+        self.tmpfile.close()\n+\n+\n+class FDTransform(FDCapture):\n+    def __init__(self, targetfd: int, transformer: callable):\n+        def callback(data):\n+            self.writeorg(transformer(data))\n+\n+        super().__init__(targetfd, callback)"}, {"sha": "055bc1f3481649b1d21f43c2306eeda063faadf4", "filename": "cowait/utils/stream_capture.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/backtick-se/cowait/blob/1552e25dc3d42d7ab69b00705719c7c3acd0283c/cowait/utils/stream_capture.py", "raw_url": "https://github.com/backtick-se/cowait/raw/1552e25dc3d42d7ab69b00705719c7c3acd0283c/cowait/utils/stream_capture.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/utils/stream_capture.py?ref=1552e25dc3d42d7ab69b00705719c7c3acd0283c", "patch": "@@ -24,9 +24,9 @@ def write(self, data):\n             self.stream.write(data)\n \n         if '\\n' in data:\n-            self.flush(auto=True)\n+            self.flush()\n \n-    def flush(self, auto: bool = False):\n+    def flush(self):\n         if not self.silence:\n             self.stream.flush()\n "}], "stats": {"total": 103, "additions": 101, "deletions": 2}}, {"sha": "ee0f838cda3ba6c53c85a2b7d7e4fe1c6b78af37", "html_url": "https://github.com/backtick-se/cowait/commit/ee0f838cda3ba6c53c85a2b7d7e4fe1c6b78af37", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-07-22T15:15:24Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-07-23T09:18:35Z"}, "message": "file descriptor capture context", "tree": {"sha": "43219d6859bcb319569cf6efa768d4f6ef9a8a10", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/43219d6859bcb319569cf6efa768d4f6ef9a8a10"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/ee0f838cda3ba6c53c85a2b7d7e4fe1c6b78af37", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "6772f4397fc896b2f4a94da50e0cdf446c6fdae4", "filename": "cowait/utils/file_capture.py", "status": "modified", "additions": 33, "deletions": 16, "changes": 49, "blob_url": "https://github.com/backtick-se/cowait/blob/ee0f838cda3ba6c53c85a2b7d7e4fe1c6b78af37/cowait/utils/file_capture.py", "raw_url": "https://github.com/backtick-se/cowait/raw/ee0f838cda3ba6c53c85a2b7d7e4fe1c6b78af37/cowait/utils/file_capture.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/utils/file_capture.py?ref=ee0f838cda3ba6c53c85a2b7d7e4fe1c6b78af37", "patch": "@@ -3,36 +3,38 @@\n import sys\n from tempfile import TemporaryFile\n \n+STDOUT = 1\n+STDERR = 2\n SYS_FDS = {\n-    1: 'stdout',\n-    2: 'stderr',\n+    STDOUT: 'stdout',\n+    STDERR: 'stderr',\n }\n \n \n class CallbackFile(io.TextIOWrapper):\n     def __init__(self, callback: callable = None):\n         super().__init__(\n             TemporaryFile(buffering=0),\n-            encoding=\"utf-8\",\n-            errors=\"replace\",\n-            newline=\"\",\n+            encoding='utf-8',\n+            errors='replace',\n             write_through=True,\n+            newline='',\n         )\n         self.callback = callback\n \n-    def write(self, data):\n+    def write(self, data) -> None:\n         super().write(data)\n         if '\\n' in data:\n             self.flush()\n \n-    def getvalue(self):\n+    def getvalue(self) -> str:\n         self.buffer.seek(0)\n         res = self.buffer.read()\n         self.buffer.seek(0)\n         self.buffer.truncate()\n         return res.decode('utf-8')\n \n-    def flush(self):\n+    def flush(self) -> None:\n         if self.callback is not None:\n             text = self.getvalue()\n             if len(text) > 0:\n@@ -79,21 +81,36 @@ def stop(self) -> None:\n \n         self._capturing = False\n \n-    def writeorg(self, data):\n+    def writeorg(self, data) -> None:\n         \"\"\"Write to original file descriptor.\"\"\"\n         assert self._capturing\n         os.write(self.targetfd_save, data.encode('utf-8'))\n \n-    def snap(self):\n+    def getvalue(self) -> str:\n         return self.tmpfile.getvalue()\n \n-    def __del__(self):\n+    def __del__(self) -> None:\n         self.tmpfile.close()\n \n \n-class FDTransform(FDCapture):\n-    def __init__(self, targetfd: int, transformer: callable):\n-        def callback(data):\n-            self.writeorg(transformer(data))\n+class FileCapturing(object):\n+    def __init__(\n+        self,\n+        on_stdout: callable = None,\n+        on_stderr: callable = None,\n+    ):\n+        self.stdout = FDCapture(STDOUT, on_stdout)\n+        self.stderr = FDCapture(STDERR, on_stderr)\n \n-        super().__init__(targetfd, callback)\n+    def __enter__(self) -> None:\n+        self.stdout.start()\n+        self.stderr.start()\n+        return self\n+\n+    def __exit__(self, *args) -> None:\n+        self.stdout.stop()\n+        self.stderr.stop()\n+\n+    def __del__(self) -> None:\n+        del self.stdout\n+        del self.stderr"}], "stats": {"total": 49, "additions": 33, "deletions": 16}}, {"sha": "4bf90533ccd5886a094a5369741b1393ff2995a5", "html_url": "https://github.com/backtick-se/cowait/commit/4bf90533ccd5886a094a5369741b1393ff2995a5", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-07-23T08:46:40Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-07-23T09:18:35Z"}, "message": "use file descriptor-level output capturing", "tree": {"sha": "0d754008054417dcd6b1c383fd5fad8d4b8b4d29", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/0d754008054417dcd6b1c383fd5fad8d4b8b4d29"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/4bf90533ccd5886a094a5369741b1393ff2995a5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "882a1488840a22c113ab14410e2bb5e0e9e1fcf8", "filename": "cowait/utils/file_capture.py", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "blob_url": "https://github.com/backtick-se/cowait/blob/4bf90533ccd5886a094a5369741b1393ff2995a5/cowait/utils/file_capture.py", "raw_url": "https://github.com/backtick-se/cowait/raw/4bf90533ccd5886a094a5369741b1393ff2995a5/cowait/utils/file_capture.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/utils/file_capture.py?ref=4bf90533ccd5886a094a5369741b1393ff2995a5", "patch": "@@ -47,6 +47,7 @@ class FDCapture:\n     def __init__(self, targetfd: int, callback: callable = None) -> None:\n         self.targetfd = targetfd\n         self.targetfd_save = os.dup(targetfd)\n+        self.callback = callback\n         self.tmpfile = CallbackFile(callback)\n         self._capturing = False\n \n@@ -72,6 +73,12 @@ def stop(self) -> None:\n         # disable output redirection\n         os.dup2(self.targetfd_save, self.targetfd)\n \n+        # if we have a callback, flush any remaining data\n+        if self.callback:\n+            remainder = self.getvalue()\n+            if len(remainder) > 0:\n+                self.callback(remainder)\n+\n         # close temporary file descriptor\n         os.close(self.targetfd_save)\n \n@@ -83,8 +90,10 @@ def stop(self) -> None:\n \n     def writeorg(self, data) -> None:\n         \"\"\"Write to original file descriptor.\"\"\"\n-        assert self._capturing\n-        os.write(self.targetfd_save, data.encode('utf-8'))\n+        if self._capturing:\n+            os.write(self.targetfd_save, data.encode('utf-8'))\n+        else:\n+            os.write(self.targetfd, data.encode('utf-8'))\n \n     def getvalue(self) -> str:\n         return self.tmpfile.getvalue()"}, {"sha": "c503f628ba81afede4f4192385d06bd35e460e40", "filename": "cowait/worker/executor.py", "status": "modified", "additions": 29, "deletions": 30, "changes": 59, "blob_url": "https://github.com/backtick-se/cowait/blob/4bf90533ccd5886a094a5369741b1393ff2995a5/cowait/worker/executor.py", "raw_url": "https://github.com/backtick-se/cowait/raw/4bf90533ccd5886a094a5369741b1393ff2995a5/cowait/worker/executor.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/worker/executor.py?ref=4bf90533ccd5886a094a5369741b1393ff2995a5", "patch": "@@ -21,20 +21,31 @@ async def execute(cluster: ClusterProvider, taskdef: TaskDefinition) -> None:\n         # init should always be the first command sent\n         await node.parent.send_init(taskdef)\n \n+        # instantiate\n+        TaskClass = load_task_class(taskdef.name)\n+        task = TaskClass(\n+            taskdef=taskdef,\n+            cluster=cluster,\n+            node=node,\n+        )\n+\n+        # monitor system resources\n+        node.monitor_system(interval=10)\n+\n+        # unpack wrapped function if defined.\n+        # allows typechecking of functional tasks\n+        taskfunc = task.run\n+        if hasattr(TaskClass, '__wraps__'):\n+            taskfunc = TaskClass.__wraps__\n+\n+        # prepare arguments\n+        inputs = {\n+            **get_parameter_defaults(taskfunc),\n+            **taskdef.inputs,\n+        }\n+\n         # run task within a log capture context\n         with node.capture_logs():\n-\n-            # instantiate\n-            TaskClass = load_task_class(taskdef.name)\n-            task = TaskClass(\n-                taskdef=taskdef,\n-                cluster=cluster,\n-                node=node,\n-            )\n-\n-            # monitor system resources\n-            node.monitor_system(interval=10)\n-\n             # initialize task\n             task.init()\n \n@@ -43,18 +54,6 @@ async def execute(cluster: ClusterProvider, taskdef: TaskDefinition) -> None:\n             # to register extra http routes.\n             node.serve()\n \n-            # unpack wrapped function if defined.\n-            # allows typechecking of functional tasks\n-            taskfunc = task.run\n-            if hasattr(TaskClass, '__wraps__'):\n-                taskfunc = TaskClass.__wraps__\n-\n-            # prepare arguments\n-            inputs = {\n-                **get_parameter_defaults(taskfunc),\n-                **taskdef.inputs,\n-            }\n-\n             # before hook\n             inputs = await task.before(inputs)\n             if inputs is None:\n@@ -77,14 +76,14 @@ async def execute(cluster: ClusterProvider, taskdef: TaskDefinition) -> None:\n             # after hook\n             await task.after(inputs)\n \n-            # wait for dangling tasks\n-            await handle_orphans(task)\n+        # wait for dangling tasks\n+        await handle_orphans(task)\n \n-            # prepare & typecheck result\n-            result, result_type = typed_return(taskfunc, result)\n+        # prepare & typecheck result\n+        result, result_type = typed_return(taskfunc, result)\n \n-            # submit result\n-            await node.parent.send_done(result, result_type.describe())\n+        # submit result\n+        await node.parent.send_done(result, result_type.describe())\n \n     except TaskError as e:\n         # pass subtask errors upstream"}, {"sha": "bea3c30a13a76307b6baed5685fcd5c141a8d02d", "filename": "cowait/worker/logger/json_logger.py", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "blob_url": "https://github.com/backtick-se/cowait/blob/4bf90533ccd5886a094a5369741b1393ff2995a5/cowait/worker/logger/json_logger.py", "raw_url": "https://github.com/backtick-se/cowait/raw/4bf90533ccd5886a094a5369741b1393ff2995a5/cowait/worker/logger/json_logger.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/worker/logger/json_logger.py?ref=4bf90533ccd5886a094a5369741b1393ff2995a5", "patch": "@@ -1,9 +1,19 @@\n+import os\n import json\n from datetime import datetime\n from .logger import Logger\n \n \n class JSONLogger(Logger):\n+    def __init__(self):\n+        # create a duplicate of stdout early on, so that output from the logger\n+        # won't be captured and cause a infinite recursion\n+        self.stdout = os.dup(1)\n+\n+    def __del__(self):\n+        os.close(self.stdout)\n+\n     def log(self, type: str, msg: dict) -> None:\n         msg['ts'] = datetime.now().isoformat()\n-        self.print(json.dumps(msg), flush=True)\n+        jsonstr = json.dumps(msg) + '\\n'\n+        os.write(self.stdout, jsonstr.encode('utf-8'))"}, {"sha": "7f4177fd60f6de20b289c03e00918ad999ae2ab7", "filename": "cowait/worker/worker_node.py", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/4bf90533ccd5886a094a5369741b1393ff2995a5/cowait/worker/worker_node.py", "raw_url": "https://github.com/backtick-se/cowait/raw/4bf90533ccd5886a094a5369741b1393ff2995a5/cowait/worker/worker_node.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/worker/worker_node.py?ref=4bf90533ccd5886a094a5369741b1393ff2995a5", "patch": "@@ -1,6 +1,6 @@\n import asyncio\n from cowait.network import Server, get_local_ip, WS_PATH\n-from cowait.utils import StreamCapturing\n+from cowait.utils.file_capture import FileCapturing\n from .io_thread import IOThread\n from .logger import Logger\n from .parent_client import ParentClient\n@@ -36,18 +36,17 @@ async def _close():\n         await asyncio.sleep(0.2)\n         self.io.create_task(_close())\n \n-    def capture_logs(self) -> StreamCapturing:\n+    def capture_logs(self) -> FileCapturing:\n         \"\"\" Sets up a stream capturing context, forwarding logs to the node \"\"\"\n         def logger(file):\n             def callback(x):\n                 nonlocal file\n                 self.io.create_task(self.parent.send_log(file, x))\n             return callback\n \n-        return StreamCapturing(\n+        return FileCapturing(\n             on_stdout=logger('stdout'),\n             on_stderr=logger('stderr'),\n-            silence=True,\n         )\n \n     def monitor_system(self, interval: float = 1.0):"}], "stats": {"total": 91, "additions": 54, "deletions": 37}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 325, "title": "Improve cowait test", "labels": [{"id": 1562464620, "node_id": "MDU6TGFiZWwxNTYyNDY0NjIw", "url": "https://api.github.com/repos/backtick-se/cowait/labels/enhancement", "name": "enhancement", "color": "9cdd56", "default": true, "description": "Improve existing features"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/325", "html_url": "https://github.com/backtick-se/cowait/pull/325", "diff_url": "https://github.com/backtick-se/cowait/pull/325.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/325.patch", "merged_at": "2021-07-23T09:18:11Z"}, "body": "Adds two new flags:\r\n- `--verbose` enables verbose output from pytest (false by default)\r\n- `--capture` toggles output capturing (true by default)\r\n\r\nImproved the pytest argument generation code", "commits": [{"sha": "45d8cb9b362b7e16f9cefb28dee7e2bb134a6fb9", "html_url": "https://github.com/backtick-se/cowait/commit/45d8cb9b362b7e16f9cefb28dee7e2bb134a6fb9", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-06-30T11:58:03Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-06-30T11:58:03Z"}, "message": "add verbosity, output capture arguments to cowait test", "tree": {"sha": "75080d32f284d7937a67fde01a72b80f9f312e05", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/75080d32f284d7937a67fde01a72b80f9f312e05"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/45d8cb9b362b7e16f9cefb28dee7e2bb134a6fb9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "ec2d9aabfffdf50523703584671d46b8cec58528", "filename": "cowait/cli/app/task.py", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "blob_url": "https://github.com/backtick-se/cowait/blob/45d8cb9b362b7e16f9cefb28dee7e2bb134a6fb9/cowait/cli/app/task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/45d8cb9b362b7e16f9cefb28dee7e2bb134a6fb9/cowait/cli/app/task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/app/task.py?ref=45d8cb9b362b7e16f9cefb28dee7e2bb134a6fb9", "patch": "@@ -157,12 +157,23 @@ def run(\n               help='pytest marks',\n               type=str,\n               default=None)\n+@click.option('-v', '--verbose',\n+              help='display verbose output',\n+              type=bool,\n+              default=False)\n+@click.option('--capture',\n+              help='toggle pytest output capture',\n+              type=bool,\n+              default=True)\n @click.pass_context\n-def test(ctx, cluster: str, mount: bool, cpu: str, cpu_limit: str, memory: str, memory_limit: str, marks: str):\n+def test(\n+    ctx, cluster: str, mount: bool, cpu: str, cpu_limit: str, memory: str, memory_limit: str,\n+    marks: str, verbose: bool, capture: bool,\n+):\n     cowait.cli.test(\n         ctx.obj, cluster_name=cluster, mount=mount,\n         cpu=cpu, cpu_limit=cpu_limit, memory=memory, memory_limit=memory_limit,\n-        marks=marks,\n+        marks=marks, verbose=verbose, capture=capture,\n     )\n \n "}, {"sha": "9f04f40b7767b32fe9c3a4e53aa17b28c75df04a", "filename": "cowait/cli/commands/test.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/backtick-se/cowait/blob/45d8cb9b362b7e16f9cefb28dee7e2bb134a6fb9/cowait/cli/commands/test.py", "raw_url": "https://github.com/backtick-se/cowait/raw/45d8cb9b362b7e16f9cefb28dee7e2bb134a6fb9/cowait/cli/commands/test.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/test.py?ref=45d8cb9b362b7e16f9cefb28dee7e2bb134a6fb9", "patch": "@@ -17,6 +17,8 @@ def test(\n     memory: str = None,\n     memory_limit: str = None,\n     marks: str = None,\n+    verbose: bool = None,\n+    capture: bool = None,\n ):\n     logger = TestLogger()\n     try:\n@@ -51,6 +53,8 @@ def test(\n             },\n             inputs={\n                 'marks': marks,\n+                'verbose': verbose,\n+                'capture': capture,\n             },\n             cpu=context.override('cpu', cpu),\n             cpu_limit=context.override('cpu_limit', cpu_limit),"}, {"sha": "2fa0823a1d082f0a2e8057cf9c217d614f17e2b6", "filename": "cowait/test/test_task.py", "status": "modified", "additions": 41, "deletions": 9, "changes": 50, "blob_url": "https://github.com/backtick-se/cowait/blob/45d8cb9b362b7e16f9cefb28dee7e2bb134a6fb9/cowait/test/test_task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/45d8cb9b362b7e16f9cefb28dee7e2bb134a6fb9/cowait/test/test_task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/test/test_task.py?ref=45d8cb9b362b7e16f9cefb28dee7e2bb134a6fb9", "patch": "@@ -1,25 +1,35 @@\n+import os\n import pytest\n import asyncio\n from alt_pytest_asyncio.plugin import AltPytestAsyncioPlugin\n from cowait import Task\n \n \n class PytestTask(Task):\n-    async def run(self, marks=None):\n+    async def run(\n+        self,\n+        marks=None,\n+        verbose: bool = True,\n+        capture: bool = True,\n+    ):\n         plugins = [\n             # make sure pytest uses the existing event loop\n             AltPytestAsyncioPlugin(loop=asyncio.get_event_loop()),\n         ]\n \n-        args = [\n-            # use cowait's bundled pytest settings\n-            \"-c\", \"/var/cowait/pytest.ini\",\n-        ]\n+        # use cowait's bundled pytest config if none is provided in the project root\n+        config = None\n+        if not os.path.exists('pytest.ini'):\n+            config = '/var/cowait/pytest.ini'\n+\n+        args = create_pytest_args(\n+            config=config,\n+            verbose=verbose,\n+            marks=marks,\n+            capture=capture,\n+        )\n \n-        # add pytest marks to args\n-        if marks and len(marks) > 0:\n-            print('Marks:', marks)\n-            args += [\"-m\", marks]\n+        print('Args:', ' '.join(args))\n \n         # run tests\n         code = pytest.main(args, plugins=plugins)\n@@ -28,3 +38,25 @@ async def run(self, marks=None):\n         if code != pytest.ExitCode.OK and \\\n            code != pytest.ExitCode.NO_TESTS_COLLECTED:\n             raise RuntimeError('Tests failed')\n+\n+        return True\n+\n+\n+def create_pytest_args(config=None, marks=None, verbose=True, capture=True):\n+    args = []\n+\n+    if config:\n+        args += ['-c', config]\n+\n+    # verbose output\n+    if verbose:\n+        args.append('-vv')\n+\n+    # output capture settings\n+    args.append('--capture=' + ('fd' if capture else 'no'))\n+\n+    # marks\n+    if marks and len(marks) > 0:\n+        args += [\"-m\", marks]\n+\n+    return args"}], "stats": {"total": 69, "additions": 58, "deletions": 11}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 322, "title": "Bump ws from 5.2.2 to 5.2.3 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/322", "html_url": "https://github.com/backtick-se/cowait/pull/322", "diff_url": "https://github.com/backtick-se/cowait/pull/322.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/322.patch", "merged_at": "2021-06-22T09:57:42Z"}, "body": "Bumps [ws](https://github.com/websockets/ws) from 5.2.2 to 5.2.3.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/websockets/ws/releases\">ws's releases</a>.</em></p>\n<blockquote>\n<h2>5.2.3</h2>\n<h1>Bug fixes</h1>\n<ul>\n<li>Backported 00c425ec to the 5.x release line (76d47c14).</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/websockets/ws/commit/6dd88e7e968ef2416445d8f8620c17d99b15c77c\"><code>6dd88e7</code></a> [dist] 5.2.3</li>\n<li><a href=\"https://github.com/websockets/ws/commit/76d47c1479002022a3e4357b3c9f0e23a68d4cd2\"><code>76d47c1</code></a> [security] Fix ReDoS vulnerability</li>\n<li>See full diff in <a href=\"https://github.com/websockets/ws/compare/5.2.2...5.2.3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=ws&package-manager=npm_and_yarn&previous-version=5.2.2&new-version=5.2.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "00fa7186c77b7aec2b55d18712c3257531d361d7", "html_url": "https://github.com/backtick-se/cowait/commit/00fa7186c77b7aec2b55d18712c3257531d361d7", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-06-22T09:57:15Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-06-22T09:57:15Z"}, "message": "Bump ws from 5.2.2 to 5.2.3 in /cloud\n\nBumps [ws](https://github.com/websockets/ws) from 5.2.2 to 5.2.3.\n- [Release notes](https://github.com/websockets/ws/releases)\n- [Commits](https://github.com/websockets/ws/compare/5.2.2...5.2.3)\n\n---\nupdated-dependencies:\n- dependency-name: ws\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "d7b24db6d3cfef0a9fd5208c6fe7b76b6f52c45a", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/d7b24db6d3cfef0a9fd5208c6fe7b76b6f52c45a"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/00fa7186c77b7aec2b55d18712c3257531d361d7", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJg0bP7CRBK7hj4Ov3rIwAADwYIAHW/NSaKPCEdr4by333iiRd0\nrBgimIa7/TJajwd/jn/hPZbsLAaJ5P7185R3LfPmVUjnSKsqApQnRNLmBPCKhbW2\nE1jkUv19FL9gdP2+bZSM1YOS3Fa5690SrVR6hiHeeyXTB8jxX5zimUFxtVoSrPly\nYAlxElJQlAWJFhXVk6qfoPQKZKKDb1wyYR1fhzVY66LOd+5RQIzr9XtkaWf6XUNV\n1mDVi4LqPhdkd+66M7Scgbt3eClypAjZGdAnUw0fjnk+r2dcKnKcrMJstmKlnuhG\nEzmxQyCd/sJoFKmowQdSTQ24R4ZQdc+5d1PYIFyM8xdo7/to3WShKWcjwBVL+vE=\n=wjfk\n-----END PGP SIGNATURE-----\n", "payload": "tree d7b24db6d3cfef0a9fd5208c6fe7b76b6f52c45a\nparent b87d1cfc3b971b63245aea0a4b8f3db67f98dc67\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1624355835 +0000\ncommitter GitHub <noreply@github.com> 1624355835 +0000\n\nBump ws from 5.2.2 to 5.2.3 in /cloud\n\nBumps [ws](https://github.com/websockets/ws) from 5.2.2 to 5.2.3.\n- [Release notes](https://github.com/websockets/ws/releases)\n- [Commits](https://github.com/websockets/ws/compare/5.2.2...5.2.3)\n\n---\nupdated-dependencies:\n- dependency-name: ws\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "9a096cfdfd2f2813a7dbea656af3c826e2e10abb", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/00fa7186c77b7aec2b55d18712c3257531d361d7/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/00fa7186c77b7aec2b55d18712c3257531d361d7/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=00fa7186c77b7aec2b55d18712c3257531d361d7", "patch": "@@ -11432,9 +11432,9 @@ write@1.0.3:\n     mkdirp \"^0.5.1\"\n \n ws@^5.2.0:\n-  version \"5.2.2\"\n-  resolved \"https://registry.yarnpkg.com/ws/-/ws-5.2.2.tgz#dffef14866b8e8dc9133582514d1befaf96e980f\"\n-  integrity sha512-jaHFD6PFv6UgoIVda6qZllptQsMlDEJkTQcybzzXDYM1XO9Y8em691FGMPmM46WGyLU4z9KMgQN+qrux/nhlHA==\n+  version \"5.2.3\"\n+  resolved \"https://registry.yarnpkg.com/ws/-/ws-5.2.3.tgz#05541053414921bc29c63bee14b8b0dd50b07b3d\"\n+  integrity sha512-jZArVERrMsKUatIdnLzqvcfydI85dvd/Fp1u/VOpfdDWQ4c9qWXe+VIeAbQ5FrDwciAkr+lzofXLz3Kuf26AOA==\n   dependencies:\n     async-limiter \"~1.0.0\"\n "}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 321, "title": "Bump color-string from 1.5.3 to 1.5.5 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/321", "html_url": "https://github.com/backtick-se/cowait/pull/321", "diff_url": "https://github.com/backtick-se/cowait/pull/321.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/321.patch", "merged_at": "2021-06-22T09:57:31Z"}, "body": "Bumps [color-string](https://github.com/Qix-/color-string) from 1.5.3 to 1.5.5.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/Qix-/color-string/releases\">color-string's releases</a>.</em></p>\n<blockquote>\n<h2>1.5.5 (Patch/Security Release) - hwb() ReDos patch (low-severity)</h2>\n<blockquote>\n<p>Release notes copied verbatim from the commit message, which can be found here: 0789e21284c33d89ebc4ab4ca6f759b9375ac9d3</p>\n</blockquote>\n<pre><code>Discovered by Yeting Li, c/o Colin Ife via Snyk.io.\n<p>A ReDos (Regular Expression Denial of Service) vulnerability\nwas responsibly disclosed to me via email by Colin on\nMar 5 2021 regarding an exponential time complexity for\nlinearly increasing input lengths for <code>hwb()</code> color strings.</p>\n<p>Strings reaching more than 5000 characters would see several\nmilliseconds of processing time; strings reaching more than\n50,000 characters began seeing 1500ms (1.5s) of processing time.</p>\n<p>The cause was due to a the regular expression that parses\nhwb() strings - specifically, the hue value - where\nthe integer portion of the hue value used a 0-or-more quantifier\nshortly thereafter followed by a 1-or-more quantifier.</p>\n<p>This caused excessive backtracking and a cartesian scan,\nresulting in exponential time complexity given a linear\nincrease in input length.</p>\n<p>Thank you Yeting Li and Colin Ife for bringing this to my\nattention in a secure, responsible and professional manner.</p>\n<p>A CVE will not be assigned for this vulnerability.\n</code></pre></p>\n<h2>1.5.4 (Patch Release)</h2>\n<ul>\n<li>Removes rounding of alpha values in RGBA hex (<code>#rrggbbaa</code>) and condensed-hex (<code>#rgba</code>) parsers, which caused certain unique inputs to result in identical outputs (see <a href=\"https://github.com/qix-/color/issues/174\">https://github.com/qix-/color/issues/174</a>).</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li>See full diff in <a href=\"https://github.com/Qix-/color-string/commits/1.5.5\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=color-string&package-manager=npm_and_yarn&previous-version=1.5.3&new-version=1.5.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "cb76550f0224eb38b5321bb2f3fabd49c2a082e9", "html_url": "https://github.com/backtick-se/cowait/commit/cb76550f0224eb38b5321bb2f3fabd49c2a082e9", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-06-22T09:57:12Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-06-22T09:57:12Z"}, "message": "Bump color-string from 1.5.3 to 1.5.5 in /cloud\n\nBumps [color-string](https://github.com/Qix-/color-string) from 1.5.3 to 1.5.5.\n- [Release notes](https://github.com/Qix-/color-string/releases)\n- [Changelog](https://github.com/Qix-/color-string/blob/master/CHANGELOG.md)\n- [Commits](https://github.com/Qix-/color-string/commits/1.5.5)\n\n---\nupdated-dependencies:\n- dependency-name: color-string\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "fe27d0c4c6b4c3175b4301584509acb3a9e504d3", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/fe27d0c4c6b4c3175b4301584509acb3a9e504d3"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/cb76550f0224eb38b5321bb2f3fabd49c2a082e9", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJg0bP4CRBK7hj4Ov3rIwAA3IwIAHW4yRgPQvLFINufI02zerOR\n6QfOSsrrGva/4llhovW1lUMjj2fhhZ5xctYA64MHGYadyACkHTyeTxJihJJM/Afh\nGyiNdWD+a8v02wdL/NuA59HUphmFuMBf1h32OqUKJhqwfIrFfQpdrreEU6FMq5N3\nn8YZUwHQpdLAfwtwL46aPrvlyhq3LMsIbPDtN5h8uFyF30zHxJS40FGg4XrMfg42\nLDQGLAS4W+qY4fkz+gvEL6goPIBR96QtBRNXbwdZbWUo1B1pKpenzf0/FsFYYsa1\n8x28YaKu3bRhweYxd5JJRCIX+t17vQcL2O1cotqJ4AqiArLixE6Qw22M4XzlMxA=\n=GR7L\n-----END PGP SIGNATURE-----\n", "payload": "tree fe27d0c4c6b4c3175b4301584509acb3a9e504d3\nparent b87d1cfc3b971b63245aea0a4b8f3db67f98dc67\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1624355832 +0000\ncommitter GitHub <noreply@github.com> 1624355832 +0000\n\nBump color-string from 1.5.3 to 1.5.5 in /cloud\n\nBumps [color-string](https://github.com/Qix-/color-string) from 1.5.3 to 1.5.5.\n- [Release notes](https://github.com/Qix-/color-string/releases)\n- [Changelog](https://github.com/Qix-/color-string/blob/master/CHANGELOG.md)\n- [Commits](https://github.com/Qix-/color-string/commits/1.5.5)\n\n---\nupdated-dependencies:\n- dependency-name: color-string\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "203c0fb3bf90eb6f7ee71e13052845b73b4413f5", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/cb76550f0224eb38b5321bb2f3fabd49c2a082e9/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/cb76550f0224eb38b5321bb2f3fabd49c2a082e9/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=cb76550f0224eb38b5321bb2f3fabd49c2a082e9", "patch": "@@ -3114,9 +3114,9 @@ color-name@^1.0.0, color-name@~1.1.4:\n   integrity sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==\n \n color-string@^1.5.2:\n-  version \"1.5.3\"\n-  resolved \"https://registry.yarnpkg.com/color-string/-/color-string-1.5.3.tgz#c9bbc5f01b58b5492f3d6857459cb6590ce204cc\"\n-  integrity sha512-dC2C5qeWoYkxki5UAXapdjqO672AM4vZuPGRQfO8b5HKuKGBbKWpITyDYN7TOFKvRW7kOgAn3746clDBMDJyQw==\n+  version \"1.5.5\"\n+  resolved \"https://registry.yarnpkg.com/color-string/-/color-string-1.5.5.tgz#65474a8f0e7439625f3d27a6a19d89fc45223014\"\n+  integrity sha512-jgIoum0OfQfq9Whcfc2z/VhCNcmQjWbey6qBX0vqt7YICflUmBCh9E9CiQD5GSJ+Uehixm3NUwHVhqUAWRivZg==\n   dependencies:\n     color-name \"^1.0.0\"\n     simple-swizzle \"^0.2.2\""}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 320, "title": "Pytest marks support for cowait test", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/320", "html_url": "https://github.com/backtick-se/cowait/pull/320", "diff_url": "https://github.com/backtick-se/cowait/pull/320.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/320.patch", "merged_at": "2021-06-22T10:06:13Z"}, "body": "", "commits": [{"sha": "532574edcc0da360a7d2dbb1655350f9bc87f36d", "html_url": "https://github.com/backtick-se/cowait/commit/532574edcc0da360a7d2dbb1655350f9bc87f36d", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-06-22T09:56:10Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-06-22T09:56:10Z"}, "message": "add marks argument to cowait test", "tree": {"sha": "4445e7d012f20b41331f474c611abfacc782df06", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/4445e7d012f20b41331f474c611abfacc782df06"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/532574edcc0da360a7d2dbb1655350f9bc87f36d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "8abe535130ce0cc93de055da9072b7595fd9f955", "filename": "cowait/cli/app/task.py", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "blob_url": "https://github.com/backtick-se/cowait/blob/532574edcc0da360a7d2dbb1655350f9bc87f36d/cowait/cli/app/task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/532574edcc0da360a7d2dbb1655350f9bc87f36d/cowait/cli/app/task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/app/task.py?ref=532574edcc0da360a7d2dbb1655350f9bc87f36d", "patch": "@@ -133,7 +133,7 @@ def run(\n               default=None,\n               type=str,\n               help='cluster name')\n-@click.option('-m', '--mount/--no-mount',\n+@click.option('--mount/--no-mount',\n               default=True,\n               type=bool,\n               help='mount working directory')\n@@ -153,11 +153,16 @@ def run(\n               help='memory limit',\n               type=str,\n               default=None)\n+@click.option('-m', '--marks',\n+              help='pytest marks',\n+              type=str,\n+              default=None)\n @click.pass_context\n-def test(ctx, cluster: str, mount: bool, cpu: str, cpu_limit: str, memory: str, memory_limit: str):\n+def test(ctx, cluster: str, mount: bool, cpu: str, cpu_limit: str, memory: str, memory_limit: str, marks: str):\n     cowait.cli.test(\n         ctx.obj, cluster_name=cluster, mount=mount,\n-        cpu=cpu, cpu_limit=cpu_limit, memory=memory, memory_limit=memory_limit\n+        cpu=cpu, cpu_limit=cpu_limit, memory=memory, memory_limit=memory_limit,\n+        marks=marks,\n     )\n \n "}, {"sha": "b807840c2be171f477f14b714d2911a3ad4f8583", "filename": "cowait/cli/commands/test.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/backtick-se/cowait/blob/532574edcc0da360a7d2dbb1655350f9bc87f36d/cowait/cli/commands/test.py", "raw_url": "https://github.com/backtick-se/cowait/raw/532574edcc0da360a7d2dbb1655350f9bc87f36d/cowait/cli/commands/test.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/test.py?ref=532574edcc0da360a7d2dbb1655350f9bc87f36d", "patch": "@@ -16,6 +16,7 @@ def test(\n     cpu_limit: str = None,\n     memory: str = None,\n     memory_limit: str = None,\n+    marks: str = None,\n ):\n     logger = TestLogger()\n     try:\n@@ -48,6 +49,9 @@ def test(\n                 **context.get('volumes', {}),\n                 **volumes,\n             },\n+            inputs={\n+                'marks': marks,\n+            },\n             cpu=context.override('cpu', cpu),\n             cpu_limit=context.override('cpu_limit', cpu_limit),\n             memory=context.override('memory', memory),"}, {"sha": "e6b8738d7ec9971754c667380a49fb0f4e8e4ee5", "filename": "cowait/test/test_task.py", "status": "modified", "additions": 17, "deletions": 5, "changes": 22, "blob_url": "https://github.com/backtick-se/cowait/blob/532574edcc0da360a7d2dbb1655350f9bc87f36d/cowait/test/test_task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/532574edcc0da360a7d2dbb1655350f9bc87f36d/cowait/test/test_task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/test/test_task.py?ref=532574edcc0da360a7d2dbb1655350f9bc87f36d", "patch": "@@ -5,14 +5,26 @@\n \n \n class PytestTask(Task):\n-    async def run(self):\n-        loop = asyncio.get_event_loop()\n-\n+    async def run(self, marks=None):\n         plugins = [\n-            AltPytestAsyncioPlugin(loop=loop),\n+            # make sure pytest uses the existing event loop\n+            AltPytestAsyncioPlugin(loop=asyncio.get_event_loop()),\n+        ]\n+\n+        args = [\n+            # use cowait's bundled pytest settings\n+            \"-c\", \"/var/cowait/pytest.ini\",\n         ]\n \n-        code = pytest.main([\"-c\", \"/var/cowait/pytest.ini\"], plugins=plugins)\n+        # add pytest marks to args\n+        if marks and len(marks) > 0:\n+            print('Marks:', marks)\n+            args += [\"-m\", marks]\n+\n+        # run tests\n+        code = pytest.main(args, plugins=plugins)\n+\n+        # throw an exception if the tests failed\n         if code != pytest.ExitCode.OK and \\\n            code != pytest.ExitCode.NO_TESTS_COLLECTED:\n             raise RuntimeError('Tests failed')"}], "stats": {"total": 37, "additions": 29, "deletions": 8}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 319, "title": "Bump merge-deep from 3.0.2 to 3.0.3 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/319", "html_url": "https://github.com/backtick-se/cowait/pull/319", "diff_url": "https://github.com/backtick-se/cowait/pull/319.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/319.patch", "merged_at": "2021-06-22T09:57:02Z"}, "body": "Bumps [merge-deep](https://github.com/jonschlinkert/merge-deep) from 3.0.2 to 3.0.3.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/jonschlinkert/merge-deep/commit/628ff47c9d824ccf21adf9a2b7cc6b74632e11a1\"><code>628ff47</code></a> 3.0.3</li>\n<li><a href=\"https://github.com/jonschlinkert/merge-deep/commit/cfbe20ccdb00255b711de57e37ed8ce9f109ef3f\"><code>cfbe20c</code></a> run verb to generate README documentation</li>\n<li><a href=\"https://github.com/jonschlinkert/merge-deep/commit/e370968581413a2e5ffdbbf7c2f5094e0e0b3861\"><code>e370968</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/jonschlinkert/merge-deep/issues/17\">#17</a> from jonschlinkert/key-properties</li>\n<li><a href=\"https://github.com/jonschlinkert/merge-deep/commit/393e2cbaeacf54e77a307c3620a00f0ac057b8d5\"><code>393e2cb</code></a> adding a test to ensure using merge-deep for inheritance still works</li>\n<li><a href=\"https://github.com/jonschlinkert/merge-deep/commit/c39b16134a6a9704be2e661b49b92e8561f10d90\"><code>c39b161</code></a> add test to ensure constructor is not cloned</li>\n<li><a href=\"https://github.com/jonschlinkert/merge-deep/commit/11e5dd56de8a6aed0b1ed022089dbce6968d82a5\"><code>11e5dd5</code></a> add isValidKey function to ensure only valid keys are merged</li>\n<li>See full diff in <a href=\"https://github.com/jonschlinkert/merge-deep/compare/3.0.2...3.0.3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=merge-deep&package-manager=npm_and_yarn&previous-version=3.0.2&new-version=3.0.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "5a0ec5bb7e521dbf301bae10a433979f1177e10b", "html_url": "https://github.com/backtick-se/cowait/commit/5a0ec5bb7e521dbf301bae10a433979f1177e10b", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-06-08T10:54:36Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-06-08T10:54:36Z"}, "message": "Bump merge-deep from 3.0.2 to 3.0.3 in /cloud\n\nBumps [merge-deep](https://github.com/jonschlinkert/merge-deep) from 3.0.2 to 3.0.3.\n- [Release notes](https://github.com/jonschlinkert/merge-deep/releases)\n- [Commits](https://github.com/jonschlinkert/merge-deep/compare/3.0.2...3.0.3)\n\n---\nupdated-dependencies:\n- dependency-name: merge-deep\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "23b605819dc82b1713fe5a6747db309ce8c93af1", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/23b605819dc82b1713fe5a6747db309ce8c93af1"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/5a0ec5bb7e521dbf301bae10a433979f1177e10b", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJgv0xsCRBK7hj4Ov3rIwAAXZ8IADsKyMjq5iNsxNrahQdJl2V0\nytIz0AReYBXfpLOyBW63gZboUt2YQKoUBwPZEPI+3hikVThND4qCE76WJue+Upnt\nghMyfv8kb1/PUK8AtZ4H/IL8jpIG9JgHbvlmLao9P+nOpIN1Nk88eERvcc+X4q80\nrGnnHEu73WYppOi0v86HONKT/ZHlTs4OI2Hla/aCGZDDH0A/hHhjwx3nuWZjWylo\nUHkzAY/pVsQuxVfiuYoOd1SSH4vQykzPoUSr8LdcjgimiMj41PPl2x/PGzC9MsGX\nOWc9WSZ2pMQiS2iPzHMzNZi406Bz+YZDe59G3xts2gU+rGHXORyfwj5ZvS8oui8=\n=0dEX\n-----END PGP SIGNATURE-----\n", "payload": "tree 23b605819dc82b1713fe5a6747db309ce8c93af1\nparent 63f3651af7dae309bbe09f07f245eb3d17f2f1d5\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1623149676 +0000\ncommitter GitHub <noreply@github.com> 1623149676 +0000\n\nBump merge-deep from 3.0.2 to 3.0.3 in /cloud\n\nBumps [merge-deep](https://github.com/jonschlinkert/merge-deep) from 3.0.2 to 3.0.3.\n- [Release notes](https://github.com/jonschlinkert/merge-deep/releases)\n- [Commits](https://github.com/jonschlinkert/merge-deep/compare/3.0.2...3.0.3)\n\n---\nupdated-dependencies:\n- dependency-name: merge-deep\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "ed220ec488a99e973fe3b452c3965f7f8cf5ccf3", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/5a0ec5bb7e521dbf301bae10a433979f1177e10b/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/5a0ec5bb7e521dbf301bae10a433979f1177e10b/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=5a0ec5bb7e521dbf301bae10a433979f1177e10b", "patch": "@@ -6911,9 +6911,9 @@ merge-anything@^2.2.4:\n     is-what \"^3.3.1\"\n \n merge-deep@^3.0.2:\n-  version \"3.0.2\"\n-  resolved \"https://registry.yarnpkg.com/merge-deep/-/merge-deep-3.0.2.tgz#f39fa100a4f1bd34ff29f7d2bf4508fbb8d83ad2\"\n-  integrity sha512-T7qC8kg4Zoti1cFd8Cr0M+qaZfOwjlPDEdZIIPPB2JZctjaPM4fX+i7HOId69tAti2fvO6X5ldfYUONDODsrkA==\n+  version \"3.0.3\"\n+  resolved \"https://registry.yarnpkg.com/merge-deep/-/merge-deep-3.0.3.tgz#1a2b2ae926da8b2ae93a0ac15d90cd1922766003\"\n+  integrity sha512-qtmzAS6t6grwEkNrunqTBdn0qKwFgNWvlxUbAV8es9M7Ot1EbyApytCnvE0jALPa46ZpKDUo527kKiaWplmlFA==\n   dependencies:\n     arr-union \"^3.1.0\"\n     clone-deep \"^0.2.4\""}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 318, "title": "Bump tensorflow-cpu from 2.4.0 to 2.5.0 in /examples/06-tensorflow", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2588709704, "node_id": "MDU6TGFiZWwyNTg4NzA5NzA0", "url": "https://api.github.com/repos/backtick-se/cowait/labels/python", "name": "python", "color": "2b67c6", "default": false, "description": "Pull requests that update Python code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/318", "html_url": "https://github.com/backtick-se/cowait/pull/318", "diff_url": "https://github.com/backtick-se/cowait/pull/318.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/318.patch", "merged_at": null}, "body": "Bumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.0.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/releases\">tensorflow-cpu's releases</a>.</em></p>\n<blockquote>\n<h2>TensorFlow 2.5.0</h2>\n<h1>Release 2.5.0</h1>\n<h2>Major Features and Improvements</h2>\n<ul>\n<li>Support for Python3.9 has been added.</li>\n<li><code>tf.data</code>:\n<ul>\n<li><code>tf.data</code> service now supports strict round-robin reads, which is useful for synchronous training workloads where example sizes vary. With strict round robin reads, users can guarantee that consumers get similar-sized examples in the same step.</li>\n<li>tf.data service now supports optional compression. Previously data would always be compressed, but now you can disable compression by passing <code>compression=None</code> to <code>tf.data.experimental.service.distribute(...)</code>.</li>\n<li><code>tf.data.Dataset.batch()</code> now supports <code>num_parallel_calls</code> and <code>deterministic</code> arguments. <code>num_parallel_calls</code> is used to indicate that multiple input batches should be computed in parallel. With <code>num_parallel_calls</code> set, <code>deterministic</code> is used to indicate that outputs can be obtained in the non-deterministic order.</li>\n<li>Options returned by <code>tf.data.Dataset.options()</code> are no longer mutable.</li>\n<li>tf.data input pipelines can now be executed in debug mode, which disables any asynchrony, parallelism, or non-determinism and forces Python execution (as opposed to trace-compiled graph execution) of user-defined functions passed into transformations such as <code>map</code>. The debug mode can be enabled through <code>tf.data.experimental.enable_debug_mode()</code>.</li>\n</ul>\n</li>\n<li><code>tf.lite</code>\n<ul>\n<li>Enabled the new MLIR-based quantization backend by default\n<ul>\n<li>The new backend is used for 8 bits full integer post-training quantization</li>\n<li>The new backend removes the redundant rescales and fixes some bugs (shared weight/bias, extremely small scales, etc)</li>\n<li>Set <code>experimental_new_quantizer</code> in tf.lite.TFLiteConverter to False to disable this change</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><code>tf.keras</code>\n<ul>\n<li><code>tf.keras.metrics.AUC</code> now support logit predictions.</li>\n<li>Enabled a new supported input type in <code>Model.fit</code>, <code>tf.keras.utils.experimental.DatasetCreator</code>, which takes a callable, <code>dataset_fn</code>. <code>DatasetCreator</code> is intended to work across all <code>tf.distribute</code> strategies, and is the only input type supported for Parameter Server strategy.</li>\n</ul>\n</li>\n<li><code>tf.distribute</code>\n<ul>\n<li><code>tf.distribute.experimental.ParameterServerStrategy</code> now supports training with Keras <code>Model.fit</code> when used with <code>DatasetCreator</code>.</li>\n<li>Creating <code>tf.random.Generator</code> under <code>tf.distribute.Strategy</code> scopes is now allowed (except for <code>tf.distribute.experimental.CentralStorageStrategy</code> and <code>tf.distribute.experimental.ParameterServerStrategy</code>). Different replicas will get different random-number streams.</li>\n</ul>\n</li>\n<li>TPU embedding support\n<ul>\n<li>Added <code>profile_data_directory</code> to <code>EmbeddingConfigSpec</code> in <code>_tpu_estimator_embedding.py</code>. This allows embedding lookup statistics gathered at runtime to be used in embedding layer partitioning decisions.</li>\n</ul>\n</li>\n<li>PluggableDevice\n<ul>\n<li>Third-party devices can now connect to TensorFlow as plug-ins through <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api.md\">StreamExecutor C API</a>.\nand <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.md\">PluggableDevice</a> interface.\n<ul>\n<li>Add custom ops and kernels through <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20190814-kernel-and-op-registration.md\">kernel and op registration C API</a>.</li>\n<li>Register custom graph optimization passes with <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20201027-modular-tensorflow-graph-c-api.md\">graph optimization C API</a>.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"https://github.com/oneapi-src/oneDNN\">oneAPI Deep Neural Network Library (oneDNN)</a> CPU performance optimizations from <a href=\"https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html\">Intel-optimized TensorFlow</a> are now available in the official x86-64 Linux and Windows builds.\n<ul>\n<li>They are off by default. Enable them by setting the environment variable <code>TF_ENABLE_ONEDNN_OPTS=1</code>.</li>\n<li>We do not recommend using them in GPU systems, as they have not been sufficiently tested with GPUs yet.</li>\n</ul>\n</li>\n<li>TensorFlow pip packages are now built with CUDA11.2 and cuDNN 8.1.0</li>\n</ul>\n<h2>Breaking Changes</h2>\n<ul>\n<li>The <code>TF_CPP_MIN_VLOG_LEVEL</code> environment variable has been renamed to to <code>TF_CPP_MAX_VLOG_LEVEL</code> which correctly describes its effect.</li>\n</ul>\n<h2>Bug Fixes and Other Changes</h2>\n<ul>\n<li><code>tf.keras</code>:\n<ul>\n<li>Preprocessing layers API consistency changes:\n<ul>\n<li><code>StringLookup</code> added <code>output_mode</code>, <code>sparse</code>, and <code>pad_to_max_tokens</code> arguments with same semantics as <code>TextVectorization</code>.</li>\n<li><code>IntegerLookup</code> added <code>output_mode</code>, <code>sparse</code>, and <code>pad_to_max_tokens</code> arguments with same semantics as <code>TextVectorization</code>. Renamed <code>max_values</code>, <code>oov_value</code> and <code>mask_value</code> to <code>max_tokens</code>, <code>oov_token</code> and <code>mask_token</code> to align with <code>StringLookup</code> and <code>TextVectorization</code>.</li>\n<li><code>TextVectorization</code> default for <code>pad_to_max_tokens</code> switched to False.</li>\n<li><code>CategoryEncoding</code> no longer supports <code>adapt</code>, <code>IntegerLookup</code> now supports equivalent functionality. <code>max_tokens</code> argument renamed to <code>num_tokens</code>.</li>\n<li><code>Discretization</code> added <code>num_bins</code> argument for learning bins boundaries through calling <code>adapt</code> on a dataset. Renamed <code>bins</code> argument to <code>bin_boundaries</code> for specifying bins without <code>adapt</code>.</li>\n</ul>\n</li>\n<li>Improvements to model saving/loading:\n<ul>\n<li><code>model.load_weights</code> now accepts paths to saved models.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\">tensorflow-cpu's changelog</a>.</em></p>\n<blockquote>\n<h1>Release 2.5.0</h1>\n<!-- raw HTML omitted -->\n<h2>Breaking Changes</h2>\n<ul>\n<li>\n<!-- raw HTML omitted -->\n</li>\n<li>The <code>TF_CPP_MIN_VLOG_LEVEL</code> environment variable has been renamed to to\n<code>TF_CPP_MAX_VLOG_LEVEL</code> which correctly describes its effect.</li>\n</ul>\n<h2>Known Caveats</h2>\n<ul>\n<li><!-- raw HTML omitted --></li>\n<li><!-- raw HTML omitted --></li>\n<li><!-- raw HTML omitted --></li>\n</ul>\n<h2>Major Features and Improvements</h2>\n<ul>\n<li>\n<p><!-- raw HTML omitted --></p>\n</li>\n<li>\n<p><!-- raw HTML omitted --></p>\n</li>\n<li>\n<p>TPU embedding support</p>\n<ul>\n<li>Added <code>profile_data_directory</code> to <code>EmbeddingConfigSpec</code> in\n<code>_tpu_estimator_embedding.py</code>. This allows embedding lookup statistics\ngathered at runtime to be used in embedding layer partitioning decisions.</li>\n</ul>\n</li>\n<li>\n<p><code>tf.keras.metrics.AUC</code> now support logit predictions.</p>\n</li>\n<li>\n<p>Creating <code>tf.random.Generator</code> under <code>tf.distribute.Strategy</code> scopes is now allowed (except for <code>tf.distribute.experimental.CentralStorageStrategy</code> and <code>tf.distribute.experimental.ParameterServerStrategy</code>). Different replicas will get different random-number streams.</p>\n</li>\n<li>\n<p><code>tf.data</code>:</p>\n<ul>\n<li>tf.data service now supports strict round-robin reads, which is useful\nfor synchronous training workloads where example sizes vary. With strict\nround robin reads, users can guarantee that consumers get similar-sized\nexamples in the same step.</li>\n<li>tf.data service now supports optional compression. Previously data would\nalways be compressed, but now you can disable compression by passing\n<code>compression=None</code> to <code>tf.data.experimental.service.distribute(...)</code>.</li>\n<li><code>tf.data.Dataset.batch()</code> now supports <code>num_parallel_calls</code> and\n<code>deterministic</code> arguments. <code>num_parallel_calls</code> is used to indicate that\nmultiple input batches should be computed in parallel. With\n<code>num_parallel_calls</code> set, <code>deterministic</code> is used to indicate that\noutputs can be obtained in the non-deterministic order.</li>\n<li>Options returned by <code>tf.data.Dataset.options()</code> are no longer mutable.</li>\n<li>tf.data input pipelines can now be executed in debug mode, which\ndisables any asynchrony, parallelism, or non-determinism and forces\nPython execution (as opposed to trace-compiled graph execution) of\nuser-defined functions passed into transformations such as <code>map</code>. The\ndebug mode can be enabled through <code>tf.data.experimental.enable_debug_mode()</code>.</li>\n</ul>\n</li>\n<li>\n<p><code>tf.lite</code></p>\n<ul>\n<li>Enabled the new MLIR-based quantization backend by default\n<ul>\n<li>The new backend is used for 8 bits full integer post-training quantization</li>\n<li>The new backend removes the redundant rescales and fixes some bugs (shared weight/bias, extremely small scales, etc)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d\"><code>a4dfb8d</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49124\">#49124</a> from tensorflow/mm-cherrypick-tf-data-segfault-fix-...</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/2107b1dc414edb3fc78e632bca4f4936171093b2\"><code>2107b1d</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49116\">#49116</a> from tensorflow-jenkins/version-numbers-2.5.0-17609</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/16b813906fcb46306aef29a04ddd0cbdb4e77918\"><code>16b8139</code></a> Update snapshot_dataset_op.cc</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/86a0d86cb5da6a28b78ea7f886ec2831d23f6d6b\"><code>86a0d86</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49126\">#49126</a> from geetachavan1/cherrypicks_X9ZNY</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/9436ae693ef66a9efb7e7e7888134173d9a0821d\"><code>9436ae6</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49128\">#49128</a> from geetachavan1/cherrypicks_D73J5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/6b2bf99cd9336026689579b683a709c5efcb4ae9\"><code>6b2bf99</code></a> Validate that a and b are proper sparse tensors</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/c03ad1a46d5b3f23df67dad03185a0ee16020c96\"><code>c03ad1a</code></a> Ensure validation sticks in banded_triangular_solve_op</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/12a6ead7ac968c402feb85ce0a8069ccbc6bf735\"><code>12a6ead</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49120\">#49120</a> from geetachavan1/cherrypicks_KJ5M9</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/b67f5b8a0a098c34c71c679aa46480035c46886e\"><code>b67f5b8</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49118\">#49118</a> from geetachavan1/cherrypicks_BIDTR</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/a13c0ade86295bd3a8356b4b8cc980cf0c5e70e0\"><code>a13c0ad</code></a> [tf.data][cherrypick] Fix snapshot segfault when using repeat and prefecth</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.0\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-cpu&package-manager=pip&previous-version=2.4.0&new-version=2.5.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "83655c5381217895844bfe1a95f47b6651cb25d1", "html_url": "https://github.com/backtick-se/cowait/commit/83655c5381217895844bfe1a95f47b6651cb25d1", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-06-07T12:14:42Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-06-07T12:14:42Z"}, "message": "Bump tensorflow-cpu from 2.4.0 to 2.5.0 in /examples/06-tensorflow\n\nBumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.0.\n- [Release notes](https://github.com/tensorflow/tensorflow/releases)\n- [Changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md)\n- [Commits](https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.0)\n\n---\nupdated-dependencies:\n- dependency-name: tensorflow-cpu\n  dependency-type: direct:production\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "d16ca096111bf9bdfc46a84335124d063d9101f5", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/d16ca096111bf9bdfc46a84335124d063d9101f5"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/83655c5381217895844bfe1a95f47b6651cb25d1", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJgvg2yCRBK7hj4Ov3rIwAA2/EIAHarP0wYgl2DadF3NE8JBPV2\nurwEAOUa6gJYgxQ9q5KaHdJS20v/AdY2Ek0ihxdG+39jz3Dtufg1+ndpkQNvzAsn\n3QL03z9I0DEy+I5+QdCQAq12qWq6c00YfHErOmbC/L0mhc60mWMcjL7q84pVY844\nTn0EzaOr4hyF4mIepo573Pj6cgTkrwe6llBIlGKsaFtAaVttv6j9zKp32J2upe6u\nMhnNL0MXKe2+bDLMyyt0JPYIevJiQYHjgsT3KoDSgbOTStzkoCyicXC2DjQVfUJi\n3rkXKb19p1vmjvznSt1uQPxN+Kxach7yuq4kt1ZMAq3VPnRmhcwH9enCCIdLt/0=\n=G9qy\n-----END PGP SIGNATURE-----\n", "payload": "tree d16ca096111bf9bdfc46a84335124d063d9101f5\nparent 63f3651af7dae309bbe09f07f245eb3d17f2f1d5\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1623068082 +0000\ncommitter GitHub <noreply@github.com> 1623068082 +0000\n\nBump tensorflow-cpu from 2.4.0 to 2.5.0 in /examples/06-tensorflow\n\nBumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.0.\n- [Release notes](https://github.com/tensorflow/tensorflow/releases)\n- [Changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md)\n- [Commits](https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.0)\n\n---\nupdated-dependencies:\n- dependency-name: tensorflow-cpu\n  dependency-type: direct:production\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "1c2f7f62d2496f87f07e7e4c80ab1df4aa8181e7", "filename": "examples/06-tensorflow/requirements.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/83655c5381217895844bfe1a95f47b6651cb25d1/examples/06-tensorflow/requirements.txt", "raw_url": "https://github.com/backtick-se/cowait/raw/83655c5381217895844bfe1a95f47b6651cb25d1/examples/06-tensorflow/requirements.txt", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/examples/06-tensorflow/requirements.txt?ref=83655c5381217895844bfe1a95f47b6651cb25d1", "patch": "@@ -1,2 +1,2 @@\n-tensorflow-cpu==2.4.0\n+tensorflow-cpu==2.5.0\n keras==2.4.0"}], "stats": {"total": 2, "additions": 1, "deletions": 1}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 317, "title": "Bump dns-packet from 1.3.1 to 1.3.4 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/317", "html_url": "https://github.com/backtick-se/cowait/pull/317", "diff_url": "https://github.com/backtick-se/cowait/pull/317.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/317.patch", "merged_at": "2021-06-22T09:56:34Z"}, "body": "Bumps [dns-packet](https://github.com/mafintosh/dns-packet) from 1.3.1 to 1.3.4.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/mafintosh/dns-packet/commit/ebdf849da5dc0d96836e87628349776c623c5be7\"><code>ebdf849</code></a> 1.3.4</li>\n<li><a href=\"https://github.com/mafintosh/dns-packet/commit/ac578722f2707310b841b65aae61d6332f8882a1\"><code>ac57872</code></a> move all allocUnsafes to allocs for easier maintenance</li>\n<li><a href=\"https://github.com/mafintosh/dns-packet/commit/c64c9507e51532c9e9a3cbefa146a134ecc025fd\"><code>c64c950</code></a> 1.3.3</li>\n<li><a href=\"https://github.com/mafintosh/dns-packet/commit/0598ba19d18da4568b32415e60a9629061b3c45c\"><code>0598ba1</code></a> fix .. in encodingLength</li>\n<li><a href=\"https://github.com/mafintosh/dns-packet/commit/010aedb33c1ee8c3f558db5249c1d46e2bd7a101\"><code>010aedb</code></a> 1.3.2</li>\n<li><a href=\"https://github.com/mafintosh/dns-packet/commit/0d0d593f8df4e2712c43957a6c62e95047f12b2d\"><code>0d0d593</code></a> backport encodingLength fix to v1</li>\n<li>See full diff in <a href=\"https://github.com/mafintosh/dns-packet/compare/v1.3.1...v1.3.4\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=dns-packet&package-manager=npm_and_yarn&previous-version=1.3.1&new-version=1.3.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "e0e3c3299cc686d9bde7f6d61dd8c8e05d06de49", "html_url": "https://github.com/backtick-se/cowait/commit/e0e3c3299cc686d9bde7f6d61dd8c8e05d06de49", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-05-28T19:49:27Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-05-28T19:49:27Z"}, "message": "Bump dns-packet from 1.3.1 to 1.3.4 in /cloud\n\nBumps [dns-packet](https://github.com/mafintosh/dns-packet) from 1.3.1 to 1.3.4.\n- [Release notes](https://github.com/mafintosh/dns-packet/releases)\n- [Changelog](https://github.com/mafintosh/dns-packet/blob/master/CHANGELOG.md)\n- [Commits](https://github.com/mafintosh/dns-packet/compare/v1.3.1...v1.3.4)\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "beb42e6ea5a770b69a298d1c64955fa19ec94902", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/beb42e6ea5a770b69a298d1c64955fa19ec94902"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/e0e3c3299cc686d9bde7f6d61dd8c8e05d06de49", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJgsUlHCRBK7hj4Ov3rIwAA+C0IAK6E+CJxFPosYz3NA7WGunQu\nUFUlW8V0GIJ7nVz50J6fACHYMmn42aTFKuhPAtN1SDYxdyxupzMAJsqMIvuEyeQF\n7AY6KJKAFXN+sFSE+ZH22Xw3nhBPoCklF2Y8kIHPl6dJNbADGnl9+DXwxW4GHFzx\nb6UUs/d17zYBzOYfmnnucKfx5oDj/NfeWDScU/DXlvFO2kMUYrE/r+beSq46DlR9\nkXWq5EEycl3OqWT5PF7EWVxgAnT2DOw6zHfV27Sdjm/W346ApHWRqXJZYIlhUfP6\noHTla15w2/1Sb22QG0wnyVpd6qssC2WWkH14uAXPALKepVRCsnF9E1EoIyndtZ0=\n=ovAg\n-----END PGP SIGNATURE-----\n", "payload": "tree beb42e6ea5a770b69a298d1c64955fa19ec94902\nparent f3bb97a409f1b5ef52903468c7006c3620c7bd77\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1622231367 +0000\ncommitter GitHub <noreply@github.com> 1622231367 +0000\n\nBump dns-packet from 1.3.1 to 1.3.4 in /cloud\n\nBumps [dns-packet](https://github.com/mafintosh/dns-packet) from 1.3.1 to 1.3.4.\n- [Release notes](https://github.com/mafintosh/dns-packet/releases)\n- [Changelog](https://github.com/mafintosh/dns-packet/blob/master/CHANGELOG.md)\n- [Commits](https://github.com/mafintosh/dns-packet/compare/v1.3.1...v1.3.4)\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "5693467659f6496878098759bdd86f019d3a4990", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/e0e3c3299cc686d9bde7f6d61dd8c8e05d06de49/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/e0e3c3299cc686d9bde7f6d61dd8c8e05d06de49/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=e0e3c3299cc686d9bde7f6d61dd8c8e05d06de49", "patch": "@@ -3887,9 +3887,9 @@ dns-equal@^1.0.0:\n   integrity sha1-s55/HabrCnW6nBcySzR1PEfgZU0=\n \n dns-packet@^1.3.1:\n-  version \"1.3.1\"\n-  resolved \"https://registry.yarnpkg.com/dns-packet/-/dns-packet-1.3.1.tgz#12aa426981075be500b910eedcd0b47dd7deda5a\"\n-  integrity sha512-0UxfQkMhYAUaZI+xrNZOz/as5KgDU0M/fQ9b6SpkyLbk3GEswDi6PADJVaYJradtRVsRIlF1zLyOodbcTCDzUg==\n+  version \"1.3.4\"\n+  resolved \"https://registry.yarnpkg.com/dns-packet/-/dns-packet-1.3.4.tgz#e3455065824a2507ba886c55a89963bb107dec6f\"\n+  integrity sha512-BQ6F4vycLXBvdrJZ6S3gZewt6rcrks9KBgM9vrhW+knGRqc8uEdT7fuCwloc7nny5xNoMJ17HGH0R/6fpo8ECA==\n   dependencies:\n     ip \"^1.1.0\"\n     safe-buffer \"^5.0.1\""}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 316, "title": "Fix CVE\u20132020\u201328275", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/316", "html_url": "https://github.com/backtick-se/cowait/pull/316", "diff_url": "https://github.com/backtick-se/cowait/pull/316.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/316.patch", "merged_at": null}, "body": "<h3>Debricked has created this Pull Request to remediate CVE\u20132020\u201328275.</h3></br>\n                    This pull request could introduce breaking changes, please review the changes under the tab <strong>Files changed</strong> above before merging.\n                </br></br>This message is a <strong>work in progress</strong>; more information about these changes and how they fix the vulnerability coming in future iterations. Until then, check out the CVE details at <a href=\"https://app.debricked.com/en/service/vulnerability/204989?repositoryId=11964&commitId=324968\">Debricked</a>.", "commits": [{"sha": "5d1841136ed3c822dd8572ed64569e67f1b9f6c5", "html_url": "https://github.com/backtick-se/cowait/commit/5d1841136ed3c822dd8572ed64569e67f1b9f6c5", "commit": {"author": {"name": "debricked[bot]", "email": "47180885+debricked[bot]@users.noreply.github.com", "date": "2021-05-12T15:05:35Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-05-12T15:05:35Z"}, "message": "Fix CVE\u20132020\u201328275", "tree": {"sha": "27ee0d0b362e346cb1a586d921adb19410a456b8", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/27ee0d0b362e346cb1a586d921adb19410a456b8"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/5d1841136ed3c822dd8572ed64569e67f1b9f6c5", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJgm+6/CRBK7hj4Ov3rIwAAo/IIAH0htz0gUvg5EKJoI0au0NQH\nUzFJ7uIbwhKrH37jAI7pJyZ5ehDrDhIY2FZLl9Fhr8E971+qur6leqoR3Ecl0r1Q\nj9QammG7UWYO8rJufjQfQBjqm2ut+PozcAsPn6LLsxq9X3Vf315B6Sw+vB7G5N30\n9nb0QllfjdLJr5H/RBvehcv+K8R98pMjPO/kzWt/Cacwx9RkFcW3XogaHYwPZktX\n+qlgLBKrfwPQNZsh59tujp847PTgxrdA9TduPf4BChjifY/L6BBbAGsIz9IIg4fF\nAw5T12FZb5H2qRsypHL6OBtztPtQezuFL4c49UOkpmtHiHsgZHrBU0QE3QEh5EE=\n=BfDI\n-----END PGP SIGNATURE-----\n", "payload": "tree 27ee0d0b362e346cb1a586d921adb19410a456b8\nparent 684c740b91b8f23da42954363184b72f95529416\nauthor debricked[bot] <47180885+debricked[bot]@users.noreply.github.com> 1620831935 +0000\ncommitter GitHub <noreply@github.com> 1620831935 +0000\n\nFix CVE\u20132020\u201328275"}}, "files": [{"sha": "b7801f9235f958a95f2bae98bf61d0acf3574934", "filename": "cloud/package.json", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/5d1841136ed3c822dd8572ed64569e67f1b9f6c5/cloud/package.json", "raw_url": "https://github.com/backtick-se/cowait/raw/5d1841136ed3c822dd8572ed64569e67f1b9f6c5/cloud/package.json", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/package.json?ref=5d1841136ed3c822dd8572ed64569e67f1b9f6c5", "patch": "@@ -26,7 +26,7 @@\n     \"react-dom\": \"^16.13.0\",\n     \"react-redux\": \"^7.2.0\",\n     \"react-router-dom\": \"^5.1.2\",\n-    \"react-scripts\": \"3.1.2\",\n+    \"react-scripts\": \"0.0.0\",\n     \"react-syntax-highlighter\": \"^12.2.1\",\n     \"redux\": \"^4.0.5\",\n     \"redux-devtools-extension\": \"^2.13.8\","}, {"sha": "5f1eeeca2d714021aea23ac9bb42dafd295c20f2", "filename": "cloud/yarn.lock", "status": "modified", "additions": 673, "deletions": 10481, "changes": 11154, "blob_url": "https://github.com/backtick-se/cowait/blob/5d1841136ed3c822dd8572ed64569e67f1b9f6c5/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/5d1841136ed3c822dd8572ed64569e67f1b9f6c5/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=5d1841136ed3c822dd8572ed64569e67f1b9f6c5"}], "stats": {"total": 11156, "additions": 674, "deletions": 10482}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 314, "title": "Bump lodash from 4.17.19 to 4.17.21 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/314", "html_url": "https://github.com/backtick-se/cowait/pull/314", "diff_url": "https://github.com/backtick-se/cowait/pull/314.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/314.patch", "merged_at": "2021-05-10T14:41:16Z"}, "body": "Bumps [lodash](https://github.com/lodash/lodash) from 4.17.19 to 4.17.21.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/lodash/lodash/commit/f299b52f39486275a9e6483b60a410e06520c538\"><code>f299b52</code></a> Bump to v4.17.21</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/c4847ebe7d14540bb28a8b932a9ce1b9ecbfee1a\"><code>c4847eb</code></a> Improve performance of <code>toNumber</code>, <code>trim</code> and <code>trimEnd</code> on large input strings</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/3469357cff396a26c363f8c1b5a91dde28ba4b1c\"><code>3469357</code></a> Prevent command injection through <code>_.template</code>'s <code>variable</code> option</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/ded9bc66583ed0b4e3b7dc906206d40757b4a90a\"><code>ded9bc6</code></a> Bump to v4.17.20.</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/63150ef7645ac07961b63a86490f419f356429aa\"><code>63150ef</code></a> Documentation fixes.</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/00f0f62a979d2f5fa0287c06eae70cf9a62d8794\"><code>00f0f62</code></a> test.js: Remove trailing comma.</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/846e434c7a5b5692c55ebf5715ed677b70a32389\"><code>846e434</code></a> Temporarily use a custom fork of <code>lodash-cli</code>.</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/5d046f39cbd27f573914768e3b36eeefcc4f1229\"><code>5d046f3</code></a> Re-enable Travis tests on <code>4.17</code> branch.</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/aa816b36d402a1ad9385142ce7188f17dae514fd\"><code>aa816b3</code></a> Remove <code>/npm-package</code>.</li>\n<li>See full diff in <a href=\"https://github.com/lodash/lodash/compare/4.17.19...4.17.21\">compare view</a></li>\n</ul>\n</details>\n<details>\n<summary>Maintainer changes</summary>\n<p>This version was pushed to npm by <a href=\"https://www.npmjs.com/~bnjmnt4n\">bnjmnt4n</a>, a new releaser for lodash since your current version.</p>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=lodash&package-manager=npm_and_yarn&previous-version=4.17.19&new-version=4.17.21)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "39bac1ecdfbce2ef7c1b1e87a3f79d0bd5cc8390", "html_url": "https://github.com/backtick-se/cowait/commit/39bac1ecdfbce2ef7c1b1e87a3f79d0bd5cc8390", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-05-10T11:35:29Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-05-10T11:35:29Z"}, "message": "Bump lodash from 4.17.19 to 4.17.21 in /cloud\n\nBumps [lodash](https://github.com/lodash/lodash) from 4.17.19 to 4.17.21.\n- [Release notes](https://github.com/lodash/lodash/releases)\n- [Commits](https://github.com/lodash/lodash/compare/4.17.19...4.17.21)\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "ba3d64108c8d185c45eff3e73958353203b4bb9b", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/ba3d64108c8d185c45eff3e73958353203b4bb9b"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/39bac1ecdfbce2ef7c1b1e87a3f79d0bd5cc8390", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJgmRqBCRBK7hj4Ov3rIwAA7hYIAH+trtLpJZFLkkS4TcwW6soq\nYo+hpo+aYxx9olbfS6vV/WdtCIKP6jOlUeIo5hHEwQojXRe9/C5eCZcbtM34RBaW\nRADFq1JY8rkIav+XM5nIioYlCF4qgs2yHraOvSCX6LdC3e7a+mc+desRJXGi1WUk\nkctkbFqtDMvTnGM6tRUDOsuubrte+MqWIgAekt7VS69yOY93k6rrOC98+daXi3L/\nxOj2Z/VmZSLB2IxAsabLSODj+JyRVXpfe55sz9nBi8xiHR0NKwdKyoz0NGPWNMKR\nbQ/Ru5TkUTmljYzCBI1gHWV2TQNUW8yUgczCfXKy1z7gxw6BEhJwP3Hq0OaR6N0=\n=eFgJ\n-----END PGP SIGNATURE-----\n", "payload": "tree ba3d64108c8d185c45eff3e73958353203b4bb9b\nparent d8e98c79bfaeae538be816098e617c3542acab9e\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1620646529 +0000\ncommitter GitHub <noreply@github.com> 1620646529 +0000\n\nBump lodash from 4.17.19 to 4.17.21 in /cloud\n\nBumps [lodash](https://github.com/lodash/lodash) from 4.17.19 to 4.17.21.\n- [Release notes](https://github.com/lodash/lodash/releases)\n- [Commits](https://github.com/lodash/lodash/compare/4.17.19...4.17.21)\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "3255f381f5a7f058d1e5584113591b358d64cdda", "filename": "cloud/package.json", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/39bac1ecdfbce2ef7c1b1e87a3f79d0bd5cc8390/cloud/package.json", "raw_url": "https://github.com/backtick-se/cowait/raw/39bac1ecdfbce2ef7c1b1e87a3f79d0bd5cc8390/cloud/package.json", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/package.json?ref=39bac1ecdfbce2ef7c1b1e87a3f79d0bd5cc8390", "patch": "@@ -18,7 +18,7 @@\n     \"@types/react-router-dom\": \"^5.1.5\",\n     \"@types/react-syntax-highlighter\": \"^11.0.4\",\n     \"@types/redux-logger\": \"^3.0.7\",\n-    \"lodash\": \"^4.17.19\",\n+    \"lodash\": \"^4.17.21\",\n     \"pg-promise\": \"^9.3.6\",\n     \"polished\": \"^3.6.3\",\n     \"query-string\": \"^6.11.1\","}, {"sha": "e88240bc5f9a65d3c84a2b14d3b0f619de7b8eab", "filename": "cloud/yarn.lock", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/backtick-se/cowait/blob/39bac1ecdfbce2ef7c1b1e87a3f79d0bd5cc8390/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/39bac1ecdfbce2ef7c1b1e87a3f79d0bd5cc8390/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=39bac1ecdfbce2ef7c1b1e87a3f79d0bd5cc8390", "patch": "@@ -6760,10 +6760,10 @@ lodash.uniq@^4.5.0:\n   resolved \"https://registry.yarnpkg.com/lodash.uniq/-/lodash.uniq-4.5.0.tgz#d0225373aeb652adc1bc82e4945339a842754773\"\n   integrity sha1-0CJTc662Uq3BvILklFM5qEJ1R3M=\n \n-\"lodash@>=3.5 <5\", lodash@^4.17.11, lodash@^4.17.12, lodash@^4.17.13, lodash@^4.17.14, lodash@^4.17.15, lodash@^4.17.19, lodash@^4.17.5, lodash@^4.2.1:\n-  version \"4.17.19\"\n-  resolved \"https://registry.yarnpkg.com/lodash/-/lodash-4.17.19.tgz#e48ddedbe30b3321783c5b4301fbd353bc1e4a4b\"\n-  integrity sha512-JNvd8XER9GQX0v2qJgsaN/mzFCNA5BRe/j8JN9d+tWyGLSodKQHKFicdwNYzWwI3wjRnaKPsGj1XkBjx/F96DQ==\n+\"lodash@>=3.5 <5\", lodash@^4.17.11, lodash@^4.17.12, lodash@^4.17.13, lodash@^4.17.14, lodash@^4.17.15, lodash@^4.17.21, lodash@^4.17.5, lodash@^4.2.1:\n+  version \"4.17.21\"\n+  resolved \"https://registry.yarnpkg.com/lodash/-/lodash-4.17.21.tgz#679591c564c3bffaae8454cf0b3df370c3d6911c\"\n+  integrity sha512-v2kDEe57lecTulaDIuNTPy3Ry4gLGJ6Z1O3vE1krgXZNrsQ+LFTGHVxVjcXPs17LhbZVGedAJv8XZ1tvj5FvSg==\n \n loglevel@^1.4.1:\n   version \"1.6.8\""}], "stats": {"total": 10, "additions": 5, "deletions": 5}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 313, "title": "Bump hosted-git-info from 2.8.8 to 2.8.9 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/313", "html_url": "https://github.com/backtick-se/cowait/pull/313", "diff_url": "https://github.com/backtick-se/cowait/pull/313.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/313.patch", "merged_at": "2021-05-10T14:41:07Z"}, "body": "Bumps [hosted-git-info](https://github.com/npm/hosted-git-info) from 2.8.8 to 2.8.9.\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/npm/hosted-git-info/blob/v2.8.9/CHANGELOG.md\">hosted-git-info's changelog</a>.</em></p>\n<blockquote>\n<h2><a href=\"https://github.com/npm/hosted-git-info/compare/v2.8.8...v2.8.9\">2.8.9</a> (2021-04-07)</h2>\n<h3>Bug Fixes</h3>\n<ul>\n<li>backport regex fix from <a href=\"https://github-redirect.dependabot.com/npm/hosted-git-info/issues/76\">#76</a> (<a href=\"https://github.com/npm/hosted-git-info/commit/29adfe5\">29adfe5</a>), closes <a href=\"https://github-redirect.dependabot.com/npm/hosted-git-info/issues/84\">#84</a></li>\n</ul>\n<p><!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/npm/hosted-git-info/commit/8d4b3697d79bcd89cdb36d1db165e3696c783a01\"><code>8d4b369</code></a> chore(release): 2.8.9</li>\n<li><a href=\"https://github.com/npm/hosted-git-info/commit/29adfe5ef789784c861b2cdeb15051ec2ba651a7\"><code>29adfe5</code></a> fix: backport regex fix from <a href=\"https://github-redirect.dependabot.com/npm/hosted-git-info/issues/76\">#76</a></li>\n<li>See full diff in <a href=\"https://github.com/npm/hosted-git-info/compare/v2.8.8...v2.8.9\">compare view</a></li>\n</ul>\n</details>\n<details>\n<summary>Maintainer changes</summary>\n<p>This version was pushed to npm by <a href=\"https://www.npmjs.com/~nlf\">nlf</a>, a new releaser for hosted-git-info since your current version.</p>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=hosted-git-info&package-manager=npm_and_yarn&previous-version=2.8.8&new-version=2.8.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "7defad333614974a82344e931e05ca5b44c4760a", "html_url": "https://github.com/backtick-se/cowait/commit/7defad333614974a82344e931e05ca5b44c4760a", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-05-10T11:30:40Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-05-10T11:30:40Z"}, "message": "Bump hosted-git-info from 2.8.8 to 2.8.9 in /cloud\n\nBumps [hosted-git-info](https://github.com/npm/hosted-git-info) from 2.8.8 to 2.8.9.\n- [Release notes](https://github.com/npm/hosted-git-info/releases)\n- [Changelog](https://github.com/npm/hosted-git-info/blob/v2.8.9/CHANGELOG.md)\n- [Commits](https://github.com/npm/hosted-git-info/compare/v2.8.8...v2.8.9)\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "bf9500ca8f07e1c8566dd7fcbb5770060b8417ab", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/bf9500ca8f07e1c8566dd7fcbb5770060b8417ab"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/7defad333614974a82344e931e05ca5b44c4760a", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJgmRlgCRBK7hj4Ov3rIwAAYDYIAErMGrzy3wBSh3ZwqVDyb9S+\naYjI/ZBf57DVdUv9lXzp46Q3eFd1lQmM59VUoHzThzLZLxVeT3Qi7SO3e5kqiBMX\nETki1sIEBrWE6dKuH16CWYGtyB4+rX4N9sbtUcXJy2BIVSk0I5dItOBKm5IWYUE9\n1xh2LTVRsZY6BEqAjsMM37mH6LZ2HQwQ6vCkVcTOnCsW5DzH+L8PQjnFBLgpNpBo\nYTGF8BGmJI7LZfvkIegXCVbE8xPHEDtl+2PXjyHIaXGl4TyfJwOEgoX1OHy4ICZu\n/iamm0elW17H2auJl5GNlt/l9RPJsx4J4rZFNSUvMs48Kxj+qnsjIwQ07QLyxJY=\n=M37r\n-----END PGP SIGNATURE-----\n", "payload": "tree bf9500ca8f07e1c8566dd7fcbb5770060b8417ab\nparent d8e98c79bfaeae538be816098e617c3542acab9e\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1620646240 +0000\ncommitter GitHub <noreply@github.com> 1620646240 +0000\n\nBump hosted-git-info from 2.8.8 to 2.8.9 in /cloud\n\nBumps [hosted-git-info](https://github.com/npm/hosted-git-info) from 2.8.8 to 2.8.9.\n- [Release notes](https://github.com/npm/hosted-git-info/releases)\n- [Changelog](https://github.com/npm/hosted-git-info/blob/v2.8.9/CHANGELOG.md)\n- [Commits](https://github.com/npm/hosted-git-info/compare/v2.8.8...v2.8.9)\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "ee12a813a588eb864c08a567ecb3b7789fabc2f7", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/7defad333614974a82344e931e05ca5b44c4760a/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/7defad333614974a82344e931e05ca5b44c4760a/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=7defad333614974a82344e931e05ca5b44c4760a", "patch": "@@ -5247,9 +5247,9 @@ hoist-non-react-statics@^3.1.0, hoist-non-react-statics@^3.3.0:\n     react-is \"^16.7.0\"\n \n hosted-git-info@^2.1.4:\n-  version \"2.8.8\"\n-  resolved \"https://registry.yarnpkg.com/hosted-git-info/-/hosted-git-info-2.8.8.tgz#7539bd4bc1e0e0a895815a2e0262420b12858488\"\n-  integrity sha512-f/wzC2QaWBs7t9IYqB4T3sR1xviIViXJRJTWBlx2Gf3g0Xi5vI7Yy4koXQ1c9OYDGHN9sBy1DQ2AB8fqZBWhUg==\n+  version \"2.8.9\"\n+  resolved \"https://registry.yarnpkg.com/hosted-git-info/-/hosted-git-info-2.8.9.tgz#dffc0bf9a21c02209090f2aa69429e1414daf3f9\"\n+  integrity sha512-mxIDAb9Lsm6DoOJ7xH+5+X4y1LU/4Hi50L9C5sIswK3JzULS4bwk1FvjdBgvYR4bzT4tuUQiC15FE2f5HbLvYw==\n \n hpack.js@^2.1.6:\n   version \"2.1.6\""}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 312, "title": "Add resource limit arguments to cowait test", "labels": [{"id": 2465797289, "node_id": "MDU6TGFiZWwyNDY1Nzk3Mjg5", "url": "https://api.github.com/repos/backtick-se/cowait/labels/cli", "name": "cli", "color": "ffc8a8", "default": false, "description": "Command Line Tool"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/312", "html_url": "https://github.com/backtick-se/cowait/pull/312", "diff_url": "https://github.com/backtick-se/cowait/pull/312.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/312.patch", "merged_at": "2021-05-10T11:11:12Z"}, "body": "resolve #311 ", "commits": [{"sha": "71f14d0e44fb6148432203add06df8a23c7f5528", "html_url": "https://github.com/backtick-se/cowait/commit/71f14d0e44fb6148432203add06df8a23c7f5528", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-10T11:02:49Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-10T11:02:49Z"}, "message": "add resource limits to cowait test", "tree": {"sha": "ef75a7e9b319562b663e64f1214d5419eb5ae24f", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/ef75a7e9b319562b663e64f1214d5419eb5ae24f"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/71f14d0e44fb6148432203add06df8a23c7f5528", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "2a3058f21dda7068a7ee2cb7bd970b03fa087808", "filename": "cowait/cli/app/task.py", "status": "modified", "additions": 25, "deletions": 6, "changes": 31, "blob_url": "https://github.com/backtick-se/cowait/blob/71f14d0e44fb6148432203add06df8a23c7f5528/cowait/cli/app/task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/71f14d0e44fb6148432203add06df8a23c7f5528/cowait/cli/app/task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/app/task.py?ref=71f14d0e44fb6148432203add06df8a23c7f5528", "patch": "@@ -27,9 +27,9 @@\n               type=str,\n               multiple=True,\n               help='define enviornment variable')\n-@click.option('-p', '--port', \n-              type=int, \n-              multiple=True, \n+@click.option('-p', '--port',\n+              type=int,\n+              multiple=True,\n               help='open a port')\n @click.option('-r', '--route',\n               type=str,\n@@ -132,9 +132,28 @@ def run(\n               default=True,\n               type=bool,\n               help='mount working directory')\n+@click.option('--cpu',\n+              help='cpu request',\n+              type=str,\n+              default=None)\n+@click.option('--cpu-limit',\n+              help='cpu limit',\n+              type=str,\n+              default=None)\n+@click.option('--memory', '--mem',\n+              help='memory request',\n+              type=str,\n+              default=None)\n+@click.option('--memory-limit', '--mem-limit',\n+              help='memory limit',\n+              type=str,\n+              default=None)\n @click.pass_context\n-def test(ctx, cluster: str, mount: bool):\n-    cowait.cli.test(ctx.obj, cluster_name=cluster, mount=mount)\n+def test(ctx, cluster: str, mount: bool, cpu: str, cpu_limit: str, memory: str, memory_limit: str):\n+    cowait.cli.test(\n+        ctx.obj, cluster_name=cluster, mount=mount,\n+        cpu=cpu, cpu_limit=cpu_limit, memory=memory, memory_limit=memory_limit\n+    )\n \n \n @click.command(help='destroy tasks')\n@@ -184,6 +203,7 @@ def kill(ctx, cluster: str, task: str):\n def agent(ctx, cluster: str, detach: bool, upstream: str):\n     cowait.cli.agent(ctx.obj, detach, upstream, cluster_name=cluster)\n \n+\n @click.command(help='stream logs from a running task')\n @click.option('-c', '--cluster',\n               default=None,\n@@ -197,4 +217,3 @@ def agent(ctx, cluster: str, detach: bool, upstream: str):\n @click.pass_context\n def logs(ctx, cluster: str, raw: bool, task: str):\n     cowait.cli.logs(ctx.obj, task, cluster_name=cluster, raw=raw)\n-"}, {"sha": "edef091c7ab13039251b5890b9a8c310d423451b", "filename": "cowait/cli/commands/test.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "blob_url": "https://github.com/backtick-se/cowait/blob/71f14d0e44fb6148432203add06df8a23c7f5528/cowait/cli/commands/test.py", "raw_url": "https://github.com/backtick-se/cowait/raw/71f14d0e44fb6148432203add06df8a23c7f5528/cowait/cli/commands/test.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/test.py?ref=71f14d0e44fb6148432203add06df8a23c7f5528", "patch": "@@ -6,14 +6,16 @@\n from ..context import Context\n from ..utils import ExitTrap\n from ..logger import Logger\n-from .build import build as run_build\n-from .push import push as run_push\n \n \n def test(\n     config: Config,\n     cluster_name: str = None,\n     mount: bool = True,\n+    cpu: str = None,\n+    cpu_limit: str = None,\n+    memory: str = None,\n+    memory_limit: str = None,\n ):\n     logger = TestLogger()\n     try:\n@@ -46,6 +48,10 @@ def test(\n                 **context.get('volumes', {}),\n                 **volumes,\n             },\n+            cpu=context.override('cpu', cpu),\n+            cpu_limit=context.override('cpu_limit', cpu_limit),\n+            memory=context.override('memory', memory),\n+            memory_limit=context.override('memory_limit', memory_limit),\n         ))\n \n         def destroy(*args):"}], "stats": {"total": 41, "additions": 33, "deletions": 8}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 310, "title": "Bump url-parse from 1.4.7 to 1.5.1 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/310", "html_url": "https://github.com/backtick-se/cowait/pull/310", "diff_url": "https://github.com/backtick-se/cowait/pull/310.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/310.patch", "merged_at": "2021-05-10T09:14:35Z"}, "body": "Bumps [url-parse](https://github.com/unshiftio/url-parse) from 1.4.7 to 1.5.1.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/eb6d9f51e395b7e47bf2594e457d541db21c713b\"><code>eb6d9f5</code></a> [dist] 1.5.1</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/750d8e8a9d45dbce9ff09759f0fe4564cdd47d74\"><code>750d8e8</code></a> [fix] Fixes relative path resolving <a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/199\">#199</a> <a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/200\">#200</a> (<a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/201\">#201</a>)</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/3ac777474ba5dc48a7e33771cbb311fc6f69bef8\"><code>3ac7774</code></a> [test] Make test consistent for browser testing</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/267a0c6f7ef1a58271be61611c5103daace602c9\"><code>267a0c6</code></a> [dist] 1.5.0</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/d1e7e8822f26e8a49794b757123b51386325b2b0\"><code>d1e7e88</code></a> [security] More backslash fixes (<a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/197\">#197</a>)</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/d99bf4cf259b7378c855f786edc253e70405ffdc\"><code>d99bf4c</code></a> [ignore] Remove npm-debug.log from .gitignore</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/422c8b5e4cac6a79cd35b4e86731476dcbeec7e4\"><code>422c8b5</code></a> [pkg] Replace nyc with c8</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/933809d630c7b21399b4e5df59fccccd80033b21\"><code>933809d</code></a> [pkg] Move coveralls to dev dependencies</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/190b2168035899a2a88f2dc2625963bf7e2f338f\"><code>190b216</code></a> [pkg] Add .npmrc</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/ce3783f4ea25753cfa36376769c14e4e2fe6ea80\"><code>ce3783f</code></a> [test] Do not test on all available versions of Edge and Safari</li>\n<li>Additional commits viewable in <a href=\"https://github.com/unshiftio/url-parse/compare/1.4.7...1.5.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=url-parse&package-manager=npm_and_yarn&previous-version=1.4.7&new-version=1.5.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "27d89a509335f3599745521fd034cbf8dc817f38", "html_url": "https://github.com/backtick-se/cowait/commit/27d89a509335f3599745521fd034cbf8dc817f38", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-05-09T21:47:43Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-05-09T21:47:43Z"}, "message": "Bump url-parse from 1.4.7 to 1.5.1 in /cloud\n\nBumps [url-parse](https://github.com/unshiftio/url-parse) from 1.4.7 to 1.5.1.\n- [Release notes](https://github.com/unshiftio/url-parse/releases)\n- [Commits](https://github.com/unshiftio/url-parse/compare/1.4.7...1.5.1)\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "b2e8a3eed515536b89de9c18cdec72c15649275c", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/b2e8a3eed515536b89de9c18cdec72c15649275c"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/27d89a509335f3599745521fd034cbf8dc817f38", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJgmFh/CRBK7hj4Ov3rIwAAycwIAFAD59KpiI/Mzy3yj8nbVuVO\nM7e3Ej+LsXLZDOpBTWOtaz+XpJR3q0LfQQb6sKsaMboqWHnWIn9jI+ZXSwEEhuOY\nBTaGZdsiixDT90hgv/tULY4iZ/uBO0GPKXvPmFVxqPH7yCUseypS2iHPgDoUnlVd\nUGLJDfyrZzAKP2+zWnM/dc8fh2wn+oVtIIO0z0GVuWP8hN/6s5dEM4xENkRXCT/W\nC4E6Y1CZ4L1lDF7lPH2w1FICaCV+Fd0ce1ePBN1UCj2xXXK75MRx8kzuEC5y03M2\naGXCRMbkeDLc75EHxP34h4rETlGg0E5+SNoPDNeXzExPbGwMxAB7cHhT/hPE9Nw=\n=2q2m\n-----END PGP SIGNATURE-----\n", "payload": "tree b2e8a3eed515536b89de9c18cdec72c15649275c\nparent 9dec95633483e69501bf02a36233180990cee1dc\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1620596863 +0000\ncommitter GitHub <noreply@github.com> 1620596863 +0000\n\nBump url-parse from 1.4.7 to 1.5.1 in /cloud\n\nBumps [url-parse](https://github.com/unshiftio/url-parse) from 1.4.7 to 1.5.1.\n- [Release notes](https://github.com/unshiftio/url-parse/releases)\n- [Commits](https://github.com/unshiftio/url-parse/compare/1.4.7...1.5.1)\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "3e9d0a58bdd77800ac0e71ebfa7c85785995213f", "filename": "cloud/yarn.lock", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/backtick-se/cowait/blob/27d89a509335f3599745521fd034cbf8dc817f38/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/27d89a509335f3599745521fd034cbf8dc817f38/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=27d89a509335f3599745521fd034cbf8dc817f38", "patch": "@@ -8871,9 +8871,9 @@ querystring@0.2.0:\n   integrity sha1-sgmEkgO7Jd+CDadW50cAWHhSFiA=\n \n querystringify@^2.1.1:\n-  version \"2.1.1\"\n-  resolved \"https://registry.yarnpkg.com/querystringify/-/querystringify-2.1.1.tgz#60e5a5fd64a7f8bfa4d2ab2ed6fdf4c85bad154e\"\n-  integrity sha512-w7fLxIRCRT7U8Qu53jQnJyPkYZIaR4n5151KMfcJlO/A9397Wxb1amJvROTK6TOnp7PfoAmg/qXiNHI+08jRfA==\n+  version \"2.2.0\"\n+  resolved \"https://registry.yarnpkg.com/querystringify/-/querystringify-2.2.0.tgz#3345941b4153cb9d082d8eee4cda2016a9aef7f6\"\n+  integrity sha512-FIqgj2EUvTa7R50u0rGsyTftzjYmv/a3hO345bZNrqabNqjtgiDMgmo4mkUjd+nzU5oF3dClKqFIPUKybUyqoQ==\n \n raf@^3.4.1:\n   version \"3.4.1\"\n@@ -10907,9 +10907,9 @@ url-loader@2.1.0:\n     schema-utils \"^2.0.0\"\n \n url-parse@^1.4.3:\n-  version \"1.4.7\"\n-  resolved \"https://registry.yarnpkg.com/url-parse/-/url-parse-1.4.7.tgz#a8a83535e8c00a316e403a5db4ac1b9b853ae278\"\n-  integrity sha512-d3uaVyzDB9tQoSXFvuSUNFibTd9zxd2bkVrDRvF5TmvWWQwqE4lgYJ5m+x1DbecWkw+LK4RNl2CU1hHuOKPVlg==\n+  version \"1.5.1\"\n+  resolved \"https://registry.yarnpkg.com/url-parse/-/url-parse-1.5.1.tgz#d5fa9890af8a5e1f274a2c98376510f6425f6e3b\"\n+  integrity sha512-HOfCOUJt7iSYzEx/UqgtwKRMC6EU91NFhsCHMv9oM03VJcVo2Qrp8T8kI9D7amFf1cu+/3CEhgb3rF9zL7k85Q==\n   dependencies:\n     querystringify \"^2.1.1\"\n     requires-port \"^1.0.0\""}], "stats": {"total": 12, "additions": 6, "deletions": 6}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 309, "title": "Fix RPC call race condition", "labels": [{"id": 1910804761, "node_id": "MDU6TGFiZWwxOTEwODA0NzYx", "url": "https://api.github.com/repos/backtick-se/cowait/labels/bug", "name": "bug", "color": "f2eb30", "default": true, "description": "Bug report"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/309", "html_url": "https://github.com/backtick-se/cowait/pull/309", "diff_url": "https://github.com/backtick-se/cowait/pull/309.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/309.patch", "merged_at": "2021-05-10T09:14:53Z"}, "body": "Refactor to avoid a potential race condition where the `RpcCall` object may be deleted from the pending call list before reaching the `wrap_future`\r\n\r\nresolve #308 ", "commits": [{"sha": "23b2c39d5cafffdf90672a6a073aa7106d833bd7", "html_url": "https://github.com/backtick-se/cowait/commit/23b2c39d5cafffdf90672a6a073aa7106d833bd7", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-07T14:21:18Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-07T15:35:56Z"}, "message": "fix a race condition in RPC calls", "tree": {"sha": "5173d890488553f235a7d686d0d91466899b3b6e", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/5173d890488553f235a7d686d0d91466899b3b6e"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/23b2c39d5cafffdf90672a6a073aa7106d833bd7", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "dab8fbd54f2721b619a2d6480dbe8ea402528467", "filename": "cowait/network/rpc_client.py", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "blob_url": "https://github.com/backtick-se/cowait/blob/23b2c39d5cafffdf90672a6a073aa7106d833bd7/cowait/network/rpc_client.py", "raw_url": "https://github.com/backtick-se/cowait/raw/23b2c39d5cafffdf90672a6a073aa7106d833bd7/cowait/network/rpc_client.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/network/rpc_client.py?ref=23b2c39d5cafffdf90672a6a073aa7106d833bd7", "patch": "@@ -23,18 +23,22 @@ async def call(self, method, args):\n         nonce = self.nonce\n         self.nonce += 1\n \n-        self.calls[nonce] = RpcCall(method, args, nonce)\n+        call = RpcCall(method, args, nonce)\n+\n+        # track pending rpc calls\n+        self.calls[nonce] = call\n+\n         await self.ws.send_json({\n             'type':   RPC_CALL,\n             'method': method,\n             'args':   args,\n             'nonce':  nonce,\n         })\n \n-        return await asyncio.wrap_future(self.calls[nonce])\n+        return await asyncio.wrap_future(call)\n \n     def cancel_all(self):\n-        for nonce, future in self.calls.items():\n+        for future in self.calls.values():\n             if not future.done():\n                 future.set_exception(RpcError('Cancelled'))\n "}], "stats": {"total": 10, "additions": 7, "deletions": 3}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 307, "title": "Bump ssri from 6.0.1 to 6.0.2 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/307", "html_url": "https://github.com/backtick-se/cowait/pull/307", "diff_url": "https://github.com/backtick-se/cowait/pull/307.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/307.patch", "merged_at": "2021-05-10T09:14:23Z"}, "body": "Bumps [ssri](https://github.com/npm/ssri) from 6.0.1 to 6.0.2.\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/npm/ssri/blob/v6.0.2/CHANGELOG.md\">ssri's changelog</a>.</em></p>\n<blockquote>\n<h2><a href=\"https://github.com/zkat/ssri/compare/v6.0.1...v6.0.2\">6.0.2</a> (2021-04-07)</h2>\n<h3>Bug Fixes</h3>\n<ul>\n<li>backport regex change from 8.0.1 (<a href=\"https://github.com/zkat/ssri/commit/b30dfdb\">b30dfdb</a>), closes <a href=\"https://github-redirect.dependabot.com/zkat/ssri/issues/19\">#19</a></li>\n</ul>\n<p><!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/npm/ssri/commit/b7c8c7c61db89aeb9fbf7596c0ef17071bc216ef\"><code>b7c8c7c</code></a> chore(release): 6.0.2</li>\n<li><a href=\"https://github.com/npm/ssri/commit/b30dfdb00bb94ddc49a25a85a18fb27afafdfbb1\"><code>b30dfdb</code></a> fix: backport regex change from 8.0.1</li>\n<li>See full diff in <a href=\"https://github.com/npm/ssri/compare/v6.0.1...v6.0.2\">compare view</a></li>\n</ul>\n</details>\n<details>\n<summary>Maintainer changes</summary>\n<p>This version was pushed to npm by <a href=\"https://www.npmjs.com/~nlf\">nlf</a>, a new releaser for ssri since your current version.</p>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=ssri&package-manager=npm_and_yarn&previous-version=6.0.1&new-version=6.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>", "commits": [{"sha": "44cc71cbc64315c0677096418f3c864533fb8d8f", "html_url": "https://github.com/backtick-se/cowait/commit/44cc71cbc64315c0677096418f3c864533fb8d8f", "commit": {"author": {"name": "dependabot[bot]", "email": "49699333+dependabot[bot]@users.noreply.github.com", "date": "2021-04-30T18:36:53Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-04-30T18:36:53Z"}, "message": "Bump ssri from 6.0.1 to 6.0.2 in /cloud\n\nBumps [ssri](https://github.com/npm/ssri) from 6.0.1 to 6.0.2.\n- [Release notes](https://github.com/npm/ssri/releases)\n- [Changelog](https://github.com/npm/ssri/blob/v6.0.2/CHANGELOG.md)\n- [Commits](https://github.com/npm/ssri/compare/v6.0.1...v6.0.2)\n\nSigned-off-by: dependabot[bot] <support@github.com>", "tree": {"sha": "bd7495615c1f30732660b4d0707dd0e0027d6334", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/bd7495615c1f30732660b4d0707dd0e0027d6334"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/44cc71cbc64315c0677096418f3c864533fb8d8f", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJgjE5FCRBK7hj4Ov3rIwAAGCUIACCY6t1AU+7z/9DiMovvCjZ8\nGRyIq3rT+2GW/PI3RQKcK9XXBvBBOLzFrpRCTTbpuSi0svcyvfea69unmydgGWMr\npT/ZqLaDlCb7HLrZwaHZPdLr892HX5YBNvqjzph0dBqgN7lVGeVzjoDahkeQN3nR\n/vHLEFV8N7YfKfhwqBc8v6l8gXRjRfmG5mjM6NfnRs+otQTNfBOOtD+eLgWTgcl2\n7RaNmTTI2HYWY6U4FnONKMVk4wcCGCjV165isjNBo/9wiIb2ks2fOJVkEcmBBNp3\nV1J6tIK9NtH4IZQHvm4nYnNnOoLOFeB6ZYOTZotsl6ucGTnq4CUn3l5eLiwItUU=\n=j/IX\n-----END PGP SIGNATURE-----\n", "payload": "tree bd7495615c1f30732660b4d0707dd0e0027d6334\nparent 1918798187736bfbd13602a4b82e4fd2badd5679\nauthor dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com> 1619807813 +0000\ncommitter GitHub <noreply@github.com> 1619807813 +0000\n\nBump ssri from 6.0.1 to 6.0.2 in /cloud\n\nBumps [ssri](https://github.com/npm/ssri) from 6.0.1 to 6.0.2.\n- [Release notes](https://github.com/npm/ssri/releases)\n- [Changelog](https://github.com/npm/ssri/blob/v6.0.2/CHANGELOG.md)\n- [Commits](https://github.com/npm/ssri/compare/v6.0.1...v6.0.2)\n\nSigned-off-by: dependabot[bot] <support@github.com>"}}, "files": [{"sha": "a8ea3d82c46fc1e62827faf12cc1b54ee62f7a16", "filename": "cloud/yarn.lock", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/44cc71cbc64315c0677096418f3c864533fb8d8f/cloud/yarn.lock", "raw_url": "https://github.com/backtick-se/cowait/raw/44cc71cbc64315c0677096418f3c864533fb8d8f/cloud/yarn.lock", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cloud/yarn.lock?ref=44cc71cbc64315c0677096418f3c864533fb8d8f", "patch": "@@ -10119,9 +10119,9 @@ sshpk@^1.7.0:\n     tweetnacl \"~0.14.0\"\n \n ssri@^6.0.1:\n-  version \"6.0.1\"\n-  resolved \"https://registry.yarnpkg.com/ssri/-/ssri-6.0.1.tgz#2a3c41b28dd45b62b63676ecb74001265ae9edd8\"\n-  integrity sha512-3Wge10hNcT1Kur4PDFwEieXSCMCJs/7WvSACcrMYrNp+b8kDL1/0wJch5Ni2WrtwEa2IO8OsVfeKIciKCDx/QA==\n+  version \"6.0.2\"\n+  resolved \"https://registry.yarnpkg.com/ssri/-/ssri-6.0.2.tgz#157939134f20464e7301ddba3e90ffa8f7728ac5\"\n+  integrity sha512-cepbSq/neFK7xB6A50KHN0xHDotYzq58wWCa5LeWqnPrHG8GzfEjO/4O8kpmcGW+oaxkvhEJCWgbgNk4/ZV93Q==\n   dependencies:\n     figgy-pudding \"^3.5.1\"\n "}], "stats": {"total": 6, "additions": 3, "deletions": 3}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 304, "title": "Cowait Deploy command", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/304", "html_url": "https://github.com/backtick-se/cowait/pull/304", "diff_url": "https://github.com/backtick-se/cowait/pull/304.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/304.patch", "merged_at": "2021-06-07T12:14:09Z"}, "body": "Leverage Restart Policies to implement `cowait deploy`", "commits": [{"sha": "cbe159cf9bad5f96edfad07454a8c46063dda68b", "html_url": "https://github.com/backtick-se/cowait/commit/cbe159cf9bad5f96edfad07454a8c46063dda68b", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-25T11:30:27Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-25T11:30:27Z"}, "message": "add deploy option to cowait run", "tree": {"sha": "61d52ff4a6fcc4f9c72af976876755fb5fd2def8", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/61d52ff4a6fcc4f9c72af976876755fb5fd2def8"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/cbe159cf9bad5f96edfad07454a8c46063dda68b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "e38d56e8ed99530d76c0b9fc93d9d3454f2242ec", "filename": "cowait/cli/app/task.py", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cbe159cf9bad5f96edfad07454a8c46063dda68b/cowait/cli/app/task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/cbe159cf9bad5f96edfad07454a8c46063dda68b/cowait/cli/app/task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/app/task.py?ref=cbe159cf9bad5f96edfad07454a8c46063dda68b", "patch": "@@ -78,13 +78,17 @@\n               help='task affinity (stack/spread)',\n               type=str,\n               default=None)\n+@click.option('--deploy',\n+              type=bool, is_flag=True,\n+              help='run forever',\n+              default=False)\n @click.pass_context\n def run(\n     ctx, task: str, image: str, cluster: str, name: str,\n     input, env, port, route,\n     upstream: str, build: bool, detach: bool,\n     cpu: str, cpu_limit: str, memory: str, memory_limit: str,\n-    file: str, raw: bool, quiet: bool, affinity: str\n+    file: str, raw: bool, quiet: bool, affinity: str, deploy: bool\n ):\n     file_inputs = {}\n     if file is not None:\n@@ -120,6 +124,7 @@ def run(\n         memory_limit=memory_limit,\n         affinity=affinity,\n         cluster_name=cluster,\n+        deploy=deploy,\n     )\n \n "}, {"sha": "65dcec41671e6ae397681cd2872311498b9f6cc3", "filename": "cowait/cli/commands/run.py", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "blob_url": "https://github.com/backtick-se/cowait/blob/cbe159cf9bad5f96edfad07454a8c46063dda68b/cowait/cli/commands/run.py", "raw_url": "https://github.com/backtick-se/cowait/raw/cbe159cf9bad5f96edfad07454a8c46063dda68b/cowait/cli/commands/run.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/run.py?ref=cbe159cf9bad5f96edfad07454a8c46063dda68b", "patch": "@@ -39,6 +39,7 @@ def run(\n     quiet: bool = False,\n     affinity: str = None,\n     cluster_name: str = None,\n+    deploy: bool = False,\n ):\n     logger = RunLogger(raw, quiet)\n     try:\n@@ -101,16 +102,20 @@ def run(\n             TaskImage.pull(image, tag='latest')\n \n         # submit task to cluster\n-        task = cluster.spawn(taskdef)\n+        task = cluster.spawn(taskdef, deploy=deploy)\n \n         if detach:\n             logger.header('detached')\n             return\n \n         def destroy(*args):\n-            logger.header('interrupt')\n-            cluster.destroy(task.id)\n-            sys.exit(1)\n+            if deploy:\n+                logger.header('detached')\n+                sys.exit(0)\n+            else:\n+                logger.header('interrupt')\n+                cluster.destroy(task.id)\n+                sys.exit(1)\n \n         with ExitTrap(destroy):\n             # capture & print logs"}, {"sha": "7a52e6fd1a83df13a823ef94fde3b0167409a808", "filename": "cowait/engine/docker/docker.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/backtick-se/cowait/blob/cbe159cf9bad5f96edfad07454a8c46063dda68b/cowait/engine/docker/docker.py", "raw_url": "https://github.com/backtick-se/cowait/raw/cbe159cf9bad5f96edfad07454a8c46063dda68b/cowait/engine/docker/docker.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/docker/docker.py?ref=cbe159cf9bad5f96edfad07454a8c46063dda68b", "patch": "@@ -24,7 +24,7 @@ def __init__(self, args={}):\n     def network(self):\n         return self.args.get('network', DEFAULT_NETWORK)\n \n-    def spawn(self, taskdef: TaskDefinition) -> DockerTask:\n+    def spawn(self, taskdef: TaskDefinition, deploy: bool = False) -> DockerTask:\n         try:\n             self.ensure_network()\n \n@@ -46,6 +46,7 @@ def spawn(self, taskdef: TaskDefinition) -> DockerTask:\n                 cpu_period=int(cpu_period),\n                 mem_reservation=str(taskdef.memory or 0),\n                 mem_limit=str(taskdef.memory_limit or 0),\n+                restart_policy=None if not deploy else {'Name': 'always'},\n                 labels={\n                     LABEL_TASK_ID: taskdef.id,\n                     LABEL_PARENT_ID: taskdef.parent,"}, {"sha": "e99ca8d210407815359ba2a0eb18cec32f43cb0b", "filename": "cowait/engine/kubernetes/kubernetes.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/backtick-se/cowait/blob/cbe159cf9bad5f96edfad07454a8c46063dda68b/cowait/engine/kubernetes/kubernetes.py", "raw_url": "https://github.com/backtick-se/cowait/raw/cbe159cf9bad5f96edfad07454a8c46063dda68b/cowait/engine/kubernetes/kubernetes.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/kubernetes/kubernetes.py?ref=cbe159cf9bad5f96edfad07454a8c46063dda68b", "patch": "@@ -53,7 +53,7 @@ def domain(self):\n     def timeout(self):\n         return self.args.get('timeout', 180)\n \n-    def spawn(self, taskdef: TaskDefinition) -> KubernetesTask:\n+    def spawn(self, taskdef: TaskDefinition, deploy: bool = False) -> KubernetesTask:\n         try:\n             self.emit_sync('prepare', taskdef=taskdef)\n \n@@ -93,7 +93,7 @@ def spawn(self, taskdef: TaskDefinition) -> KubernetesTask:\n                     ),\n                     spec=client.V1PodSpec(\n                         hostname=taskdef.id,\n-                        restart_policy='Never',\n+                        restart_policy='Always' if deploy else 'Never',\n                         image_pull_secrets=self.get_pull_secrets(),\n                         volumes=volumes,\n "}], "stats": {"total": 27, "additions": 19, "deletions": 8}}, {"sha": "619fc179771659d8c8ee2fc0fa35b6843dc36236", "html_url": "https://github.com/backtick-se/cowait/commit/619fc179771659d8c8ee2fc0fa35b6843dc36236", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-25T11:30:49Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-25T11:30:49Z"}, "message": "deployed tasks have static ids", "tree": {"sha": "ba6b667b341b869f22536f931a382b3d153beedc", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/ba6b667b341b869f22536f931a382b3d153beedc"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/619fc179771659d8c8ee2fc0fa35b6843dc36236", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "c694519326b05d67494b25b376b46be081231874", "filename": "cowait/cli/commands/run.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/backtick-se/cowait/blob/619fc179771659d8c8ee2fc0fa35b6843dc36236/cowait/cli/commands/run.py", "raw_url": "https://github.com/backtick-se/cowait/raw/619fc179771659d8c8ee2fc0fa35b6843dc36236/cowait/cli/commands/run.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/run.py?ref=619fc179771659d8c8ee2fc0fa35b6843dc36236", "patch": "@@ -2,7 +2,7 @@\n import json\n import getpass\n import docker.errors\n-from cowait.tasks import TaskDefinition\n+from cowait.tasks.definition import TaskDefinition, generate_task_id\n from cowait.engine.errors import TaskCreationError, ProviderError\n from cowait.tasks.messages import TASK_INIT, TASK_STATUS, TASK_FAIL, TASK_RETURN, TASK_LOG\n from ..config import Config\n@@ -73,7 +73,7 @@ def run(\n \n         # create task definition\n         taskdef = TaskDefinition(\n-            id=name,\n+            id=generate_task_id(task, unique=not deploy),\n             name=task,\n             image=image,\n             inputs=inputs,"}, {"sha": "e6cd0136de24d6259539caa02c68fec319f6948f", "filename": "cowait/tasks/definition.py", "status": "modified", "additions": 16, "deletions": 2, "changes": 18, "blob_url": "https://github.com/backtick-se/cowait/blob/619fc179771659d8c8ee2fc0fa35b6843dc36236/cowait/tasks/definition.py", "raw_url": "https://github.com/backtick-se/cowait/raw/619fc179771659d8c8ee2fc0fa35b6843dc36236/cowait/tasks/definition.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/tasks/definition.py?ref=619fc179771659d8c8ee2fc0fa35b6843dc36236", "patch": "@@ -3,15 +3,29 @@\n from ..utils import uuid\n \n \n-def generate_task_id(name: str) -> str:\n+def generate_task_id(name: str, unique: bool = True) -> str:\n+    \"\"\"\n+    Generates a new Task ID from a task import path.\n+\n+    Args:\n+        name (str): Task import path name\n+        unique (bool): True if the ID should be unique. Adds a random string\n+\n+    Returns:\n+        id (str): Task ID\n+    \"\"\"\n+\n     if '.' in name:\n         dot = name.rfind('.')\n         name = name[dot+1:]\n \n     name = name.replace('.', '-')\n     name = name.replace('_', '-')\n \n-    return '%s-%s' % (name.lower(), uuid())\n+    if unique:\n+        return '%s-%s' % (name.lower(), uuid())\n+    else:\n+        return name.lower()\n \n \n class TaskDefinition(object):"}], "stats": {"total": 22, "additions": 18, "deletions": 4}}, {"sha": "86a34ac4201fd92ff67c62d21df668cc0e2bc6d0", "html_url": "https://github.com/backtick-se/cowait/commit/86a34ac4201fd92ff67c62d21df668cc0e2bc6d0", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-25T11:31:02Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-06-07T12:07:31Z"}, "message": "kill existing tasks when deploying", "tree": {"sha": "432bddc9912284aa06a3bff470355eb1d1495ecf", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/432bddc9912284aa06a3bff470355eb1d1495ecf"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/86a34ac4201fd92ff67c62d21df668cc0e2bc6d0", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "bd78478dcb9d021f8f3c3468b8f77dceb851a347", "filename": "cowait/engine/kubernetes/kubernetes.py", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/backtick-se/cowait/blob/86a34ac4201fd92ff67c62d21df668cc0e2bc6d0/cowait/engine/kubernetes/kubernetes.py", "raw_url": "https://github.com/backtick-se/cowait/raw/86a34ac4201fd92ff67c62d21df668cc0e2bc6d0/cowait/engine/kubernetes/kubernetes.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/kubernetes/kubernetes.py?ref=86a34ac4201fd92ff67c62d21df668cc0e2bc6d0", "patch": "@@ -57,6 +57,11 @@ def spawn(self, taskdef: TaskDefinition, deploy: bool = False) -> KubernetesTask\n         try:\n             self.emit_sync('prepare', taskdef=taskdef)\n \n+            if deploy:\n+                # if deploying, destroy any existing pod\n+                self.kill(taskdef.id)\n+                self.wait_until_deleted(taskdef.id)\n+\n             volumes, mounts = create_volumes(taskdef.volumes)\n \n             # container definition\n@@ -176,6 +181,10 @@ def wait_until_ready(self, task_id: str, poll_interval: float = 1):\n                 # for now, leave the pod running so the user may inspect it\n                 raise TaskCreationError('Pod configuration error') from None\n \n+    def wait_until_deleted(self, task_id: str, poll_interval: float = 1):\n+        while self.get_task_pod(task_id) is not None:\n+            time.sleep(poll_interval)\n+\n     def logs(self, task_id: str):\n         # wait for pod to become ready\n         self.wait_until_ready(task_id)"}], "stats": {"total": 9, "additions": 9, "deletions": 0}}, {"sha": "22e05b6e4dbc6686e071e30b8edb87af7ac54191", "html_url": "https://github.com/backtick-se/cowait/commit/22e05b6e4dbc6686e071e30b8edb87af7ac54191", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-25T11:31:13Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-06-07T12:07:31Z"}, "message": "bump version", "tree": {"sha": "f98aa5c78f8b5cbc8a57839eb97a69ec6db4614e", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/f98aa5c78f8b5cbc8a57839eb97a69ec6db4614e"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/22e05b6e4dbc6686e071e30b8edb87af7ac54191", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "840c83923b2621223003d2788a8703150946959b", "filename": "cowait/version.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/22e05b6e4dbc6686e071e30b8edb87af7ac54191/cowait/version.py", "raw_url": "https://github.com/backtick-se/cowait/raw/22e05b6e4dbc6686e071e30b8edb87af7ac54191/cowait/version.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/version.py?ref=22e05b6e4dbc6686e071e30b8edb87af7ac54191", "patch": "@@ -1,2 +1,2 @@\n-version=\"0.4.25\"\n+version=\"0.4.26\"\n "}, {"sha": "068e1338a88765048ac43e4160c3bac09af91541", "filename": "pyproject.toml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/22e05b6e4dbc6686e071e30b8edb87af7ac54191/pyproject.toml", "raw_url": "https://github.com/backtick-se/cowait/raw/22e05b6e4dbc6686e071e30b8edb87af7ac54191/pyproject.toml", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/pyproject.toml?ref=22e05b6e4dbc6686e071e30b8edb87af7ac54191", "patch": "@@ -1,6 +1,6 @@\n [tool.poetry]\n name = \"cowait\"\n-version = \"0.4.25\"\n+version = \"0.4.26\"\n description = \"\"\n authors = [\"Backtick Technologies <johan@backtick.se>\"]\n license = \"Apache License v2.0\""}], "stats": {"total": 4, "additions": 2, "deletions": 2}}, {"sha": "1772acd29a099fe8ab032cee0fae7c9a7f9d7c22", "html_url": "https://github.com/backtick-se/cowait/commit/1772acd29a099fe8ab032cee0fae7c9a7f9d7c22", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-06-07T12:07:17Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-06-07T12:07:31Z"}, "message": "fix broken example tests", "tree": {"sha": "372fc2ec8314ed063bc4a9726ea264982aabfbff", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/372fc2ec8314ed063bc4a9726ea264982aabfbff"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/1772acd29a099fe8ab032cee0fae7c9a7f9d7c22", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "907ed3e00c203239931bc673dce64789f71376d4", "filename": "examples/06-tensorflow/requirements.txt", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/backtick-se/cowait/blob/1772acd29a099fe8ab032cee0fae7c9a7f9d7c22/examples/06-tensorflow/requirements.txt", "raw_url": "https://github.com/backtick-se/cowait/raw/1772acd29a099fe8ab032cee0fae7c9a7f9d7c22/examples/06-tensorflow/requirements.txt", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/examples/06-tensorflow/requirements.txt?ref=1772acd29a099fe8ab032cee0fae7c9a7f9d7c22", "patch": "@@ -1,2 +1,2 @@\n-keras\n-tensorflow\n+tensorflow-cpu==2.4.0\n+keras==2.4.0"}, {"sha": "907ed3e00c203239931bc673dce64789f71376d4", "filename": "examples/10-imdb/requirements.txt", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/backtick-se/cowait/blob/1772acd29a099fe8ab032cee0fae7c9a7f9d7c22/examples/10-imdb/requirements.txt", "raw_url": "https://github.com/backtick-se/cowait/raw/1772acd29a099fe8ab032cee0fae7c9a7f9d7c22/examples/10-imdb/requirements.txt", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/examples/10-imdb/requirements.txt?ref=1772acd29a099fe8ab032cee0fae7c9a7f9d7c22", "patch": "@@ -1,2 +1,2 @@\n-keras\n-tensorflow\n+tensorflow-cpu==2.4.0\n+keras==2.4.0"}, {"sha": "0b64f7436385652f679b959759a2fd6a8ff657e6", "filename": "pytest.ini", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/1772acd29a099fe8ab032cee0fae7c9a7f9d7c22/pytest.ini", "raw_url": "https://github.com/backtick-se/cowait/raw/1772acd29a099fe8ab032cee0fae7c9a7f9d7c22/pytest.ini", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/pytest.ini?ref=1772acd29a099fe8ab032cee0fae7c9a7f9d7c22", "patch": "@@ -1,6 +1,6 @@\n [pytest]\n norecursedirs = dist *.egg-info .git build cloud examples\n-default_async_timeout = 10\n+default_async_timeout = 20\n markers =\n     slow\n     network"}], "stats": {"total": 10, "additions": 5, "deletions": 5}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 303, "title": "Fix ContainerTask", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/303", "html_url": "https://github.com/backtick-se/cowait/pull/303", "diff_url": "https://github.com/backtick-se/cowait/pull/303.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/303.patch", "merged_at": "2021-04-20T19:52:04Z"}, "body": "resolve #301 ", "commits": [{"sha": "368eab0794ce741146d72ab15eae4aa104c7fa22", "html_url": "https://github.com/backtick-se/cowait/commit/368eab0794ce741146d72ab15eae4aa104c7fa22", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-20T18:35:28Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-20T19:49:35Z"}, "message": "update ContainerTask", "tree": {"sha": "1e90b3276479f31561bb2518013606fa8f04278a", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/1e90b3276479f31561bb2518013606fa8f04278a"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/368eab0794ce741146d72ab15eae4aa104c7fa22", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "10c8b029995964548c403bad2fa1a60aa2ec445d", "filename": "cowait/tasks/container/container_task.py", "status": "modified", "additions": 11, "deletions": 5, "changes": 16, "blob_url": "https://github.com/backtick-se/cowait/blob/368eab0794ce741146d72ab15eae4aa104c7fa22/cowait/tasks/container/container_task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/368eab0794ce741146d72ab15eae4aa104c7fa22/cowait/tasks/container/container_task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/tasks/container/container_task.py?ref=368eab0794ce741146d72ab15eae4aa104c7fa22", "patch": "@@ -4,23 +4,29 @@\n class ContainerTask(Task):\n     async def run(\n         self, name: str, image: str, env: dict = {},\n-        routes: dict = {}, ports: dict = {},\n-        cpu: any = 0, memory: any = 0,\n+        routes: dict = {}, ports: dict = {}, meta: dict = {},\n+        cpu: any = 0, memory: any = 0, affinity: str = None,\n         **inputs\n     ):\n         taskdef = TaskDefinition(\n             name=name,\n             image=image,\n             parent=self.id,\n+            owner=self.taskdef.owner,\n             inputs=inputs,\n             ports=ports,\n             routes=routes,\n             cpu=cpu,\n             memory=memory,\n+            affinity=affinity,\n             env={\n-                **self.env,\n+                **self.taskdef.env,\n                 **env,\n             },\n+            meta={\n+                **self.taskdef.meta,\n+                **meta,\n+            },\n         )\n \n         # run it\n@@ -30,9 +36,9 @@ async def run(\n         await self.watch(task)\n \n         # clean up\n-        self.cluster.destroy(task)\n+        self.cluster.destroy(task.id)\n \n     async def watch(self, task):\n-        logs = self.cluster.logs(task)\n+        logs = self.cluster.logs(task.id)\n         for log in logs:\n             print(log)"}], "stats": {"total": 16, "additions": 11, "deletions": 5}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 302, "title": "cowait test correctly mounts the context root when running locally", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/302", "html_url": "https://github.com/backtick-se/cowait/pull/302", "diff_url": "https://github.com/backtick-se/cowait/pull/302.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/302.patch", "merged_at": "2021-04-20T19:49:54Z"}, "body": "- Add `--mount`/`--no-mount` flags to `cowait test`.\r\n\r\nIf `--mount` is set, the working directory is mounted into the container when running in Docker. Mount is enabled by default.", "commits": [{"sha": "154b21b90e84a445757424bd5fed0f43f77f4da6", "html_url": "https://github.com/backtick-se/cowait/commit/154b21b90e84a445757424bd5fed0f43f77f4da6", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-20T18:29:01Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-20T18:29:01Z"}, "message": "cowait test correctly mounts the context root when running locally", "tree": {"sha": "4ad33b5fce89a36aa47d2c0142838a6057deae2a", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/4ad33b5fce89a36aa47d2c0142838a6057deae2a"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/154b21b90e84a445757424bd5fed0f43f77f4da6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "5ada5b3d3bf3a3b93e2923c4883781fb87518d43", "filename": "cowait/cli/commands/test.py", "status": "modified", "additions": 16, "deletions": 1, "changes": 17, "blob_url": "https://github.com/backtick-se/cowait/blob/154b21b90e84a445757424bd5fed0f43f77f4da6/cowait/cli/commands/test.py", "raw_url": "https://github.com/backtick-se/cowait/raw/154b21b90e84a445757424bd5fed0f43f77f4da6/cowait/cli/commands/test.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/test.py?ref=154b21b90e84a445757424bd5fed0f43f77f4da6", "patch": "@@ -19,6 +19,18 @@ def test(\n         context = Context.open(config)\n         cluster = context.get_cluster(cluster_name)\n \n+        volumes = {}\n+        if cluster.type == 'docker':\n+            # when testing in docker, mount the local directory\n+            # this avoids the problem of having to constantly rebuild in order to test\n+            volumes['/var/task'] = {\n+                'bind': {\n+                    'src': context.root_path,\n+                    'mode': 'rw',\n+                    'inherit': 'same-image',\n+                },\n+            }\n+\n         # execute the test task within the current image\n         task = cluster.spawn(TaskDefinition(\n             name='cowait.test',\n@@ -28,7 +40,10 @@ def test(\n                 **context.environment,\n                 **context.dotenv,\n             },\n-            volumes=context.get('volumes', {}),\n+            volumes={\n+                **context.get('volumes', {}),\n+                **volumes,\n+            },\n         ))\n \n         def destroy(*args):"}], "stats": {"total": 17, "additions": 16, "deletions": 1}}, {"sha": "98e68bda941971faa1c5f2013aa5ef7e1a5c8087", "html_url": "https://github.com/backtick-se/cowait/commit/98e68bda941971faa1c5f2013aa5ef7e1a5c8087", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-20T19:41:55Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-20T19:41:55Z"}, "message": "add mount flag to cowait test", "tree": {"sha": "6effbe7094dd54662b0600dfa42d65b66f8154ef", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/6effbe7094dd54662b0600dfa42d65b66f8154ef"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/98e68bda941971faa1c5f2013aa5ef7e1a5c8087", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "eb75fc4ddb583d989d3134d34d29c8b4659772bb", "filename": "cowait/cli/app/task.py", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "blob_url": "https://github.com/backtick-se/cowait/blob/98e68bda941971faa1c5f2013aa5ef7e1a5c8087/cowait/cli/app/task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/98e68bda941971faa1c5f2013aa5ef7e1a5c8087/cowait/cli/app/task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/app/task.py?ref=98e68bda941971faa1c5f2013aa5ef7e1a5c8087", "patch": "@@ -128,9 +128,13 @@ def run(\n               default=None,\n               type=str,\n               help='cluster name')\n+@click.option('-m', '--mount/--no-mount',\n+              default=True,\n+              type=bool,\n+              help='mount working directory')\n @click.pass_context\n-def test(ctx, cluster: str):\n-    cowait.cli.test(ctx.obj, cluster_name=cluster)\n+def test(ctx, cluster: str, mount: bool):\n+    cowait.cli.test(ctx.obj, cluster_name=cluster, mount=mount)\n \n \n @click.command(help='destroy tasks')"}, {"sha": "811d79a84d4bf9318cafcd0e173635409336ceea", "filename": "cowait/cli/commands/test.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/backtick-se/cowait/blob/98e68bda941971faa1c5f2013aa5ef7e1a5c8087/cowait/cli/commands/test.py", "raw_url": "https://github.com/backtick-se/cowait/raw/98e68bda941971faa1c5f2013aa5ef7e1a5c8087/cowait/cli/commands/test.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/test.py?ref=98e68bda941971faa1c5f2013aa5ef7e1a5c8087", "patch": "@@ -13,16 +13,18 @@\n def test(\n     config: Config,\n     cluster_name: str = None,\n+    mount: bool = True,\n ):\n     logger = TestLogger()\n     try:\n         context = Context.open(config)\n         cluster = context.get_cluster(cluster_name)\n \n         volumes = {}\n-        if cluster.type == 'docker':\n+        if mount and cluster.type == 'docker':\n             # when testing in docker, mount the local directory\n             # this avoids the problem of having to constantly rebuild in order to test\n+            print('** Mounting', context.root_path)\n             volumes['/var/task'] = {\n                 'bind': {\n                     'src': context.root_path,"}, {"sha": "dd96af58605f1d9ea3003d0b1c41adf250f93ed8", "filename": "examples/test_examples.sh", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/98e68bda941971faa1c5f2013aa5ef7e1a5c8087/examples/test_examples.sh", "raw_url": "https://github.com/backtick-se/cowait/raw/98e68bda941971faa1c5f2013aa5ef7e1a5c8087/examples/test_examples.sh", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/examples/test_examples.sh?ref=98e68bda941971faa1c5f2013aa5ef7e1a5c8087", "patch": "@@ -18,7 +18,7 @@ do\n     cowait build\n \n     # run tests\n-    cowait test\n+    cowait test --no-mount\n     if [ \"$?\" != \"0\" ]; then\n         echo \"~~ Test failed in $dir\"\n         exit 1"}], "stats": {"total": 14, "additions": 10, "deletions": 4}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 300, "title": "Test command fixes", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/300", "html_url": "https://github.com/backtick-se/cowait/pull/300", "diff_url": "https://github.com/backtick-se/cowait/pull/300.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/300.patch", "merged_at": "2021-04-12T13:02:05Z"}, "body": "- Remove built-in `build` and `push` functionality from `cowait test`\r\n- Pass `environment` and `volumes` configuration when running `cowait test`\r\n- Implement `KubernetesProvider.wait()` so that tests may run on kubernetes clusters. `wait()` should probably be removed or changes, since its API currently breaks the convention of passing task ids to `ClusterProvider` methods.", "commits": [{"sha": "7fc1ac814a2fbbf2f4752bd41074b7b69263cb20", "html_url": "https://github.com/backtick-se/cowait/commit/7fc1ac814a2fbbf2f4752bd41074b7b69263cb20", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-12T11:21:37Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-12T12:44:52Z"}, "message": "remove push argument & automatic builds from cowait test", "tree": {"sha": "42d36e14dcbbc46e03aef683769e59428ec1e008", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/42d36e14dcbbc46e03aef683769e59428ec1e008"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/7fc1ac814a2fbbf2f4752bd41074b7b69263cb20", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "a2ef4424005935968b67d488775c9ac9c33dec88", "filename": "cowait/cli/app/task.py", "status": "modified", "additions": 2, "deletions": 6, "changes": 8, "blob_url": "https://github.com/backtick-se/cowait/blob/7fc1ac814a2fbbf2f4752bd41074b7b69263cb20/cowait/cli/app/task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/7fc1ac814a2fbbf2f4752bd41074b7b69263cb20/cowait/cli/app/task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/app/task.py?ref=7fc1ac814a2fbbf2f4752bd41074b7b69263cb20", "patch": "@@ -128,13 +128,9 @@ def run(\n               default=None,\n               type=str,\n               help='cluster name')\n-@click.option('--push',\n-              type=bool, is_flag=True,\n-              help='build and push first',\n-              default=False)\n @click.pass_context\n-def test(ctx, cluster: str, push: bool):\n-    cowait.cli.test(ctx.obj, push, cluster_name=cluster)\n+def test(ctx, cluster: str):\n+    cowait.cli.test(ctx.obj, cluster_name=cluster)\n \n \n @click.command(help='destroy tasks')"}, {"sha": "0f80b3d3fdf122e7fc4db6f5b1c99f4995239289", "filename": "cowait/cli/commands/test.py", "status": "modified", "additions": 0, "deletions": 6, "changes": 6, "blob_url": "https://github.com/backtick-se/cowait/blob/7fc1ac814a2fbbf2f4752bd41074b7b69263cb20/cowait/cli/commands/test.py", "raw_url": "https://github.com/backtick-se/cowait/raw/7fc1ac814a2fbbf2f4752bd41074b7b69263cb20/cowait/cli/commands/test.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/test.py?ref=7fc1ac814a2fbbf2f4752bd41074b7b69263cb20", "patch": "@@ -11,19 +11,13 @@\n \n def test(\n     config: Config,\n-    push: bool,\n     cluster_name: str = None,\n ):\n     logger = TestLogger()\n     try:\n         context = Context.open(config)\n         cluster = context.get_cluster(cluster_name)\n \n-        if push:\n-            run_push(config)\n-        else:\n-            run_build(config)\n-\n         # execute the test task within the current image\n         task = cluster.spawn(TaskDefinition(\n             name='cowait.test',"}, {"sha": "4dbb07d252235f78356e06caf561ba975687edb4", "filename": "examples/test_examples.sh", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/backtick-se/cowait/blob/7fc1ac814a2fbbf2f4752bd41074b7b69263cb20/examples/test_examples.sh", "raw_url": "https://github.com/backtick-se/cowait/raw/7fc1ac814a2fbbf2f4752bd41074b7b69263cb20/examples/test_examples.sh", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/examples/test_examples.sh?ref=7fc1ac814a2fbbf2f4752bd41074b7b69263cb20", "patch": "@@ -14,6 +14,9 @@ do\n     echo \"~~ Running tests in $dir\"\n     cd $dir\n \n+    # build\n+    cowait build\n+\n     # run tests\n     cowait test\n     if [ \"$?\" != \"0\" ]; then"}], "stats": {"total": 17, "additions": 5, "deletions": 12}}, {"sha": "8398e54fc620678394ee12fff2658c1e50ab0fc8", "html_url": "https://github.com/backtick-se/cowait/commit/8398e54fc620678394ee12fff2658c1e50ab0fc8", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-12T11:21:57Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-12T12:44:52Z"}, "message": "pass configured env and volumes to cowait test", "tree": {"sha": "0e8f97723ba158905ff4002a24cf38dcff04b056", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/0e8f97723ba158905ff4002a24cf38dcff04b056"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/8398e54fc620678394ee12fff2658c1e50ab0fc8", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "26eb280a7247011c4895fd241e84e835842c99e6", "filename": "cowait/cli/commands/test.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/8398e54fc620678394ee12fff2658c1e50ab0fc8/cowait/cli/commands/test.py", "raw_url": "https://github.com/backtick-se/cowait/raw/8398e54fc620678394ee12fff2658c1e50ab0fc8/cowait/cli/commands/test.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/test.py?ref=8398e54fc620678394ee12fff2658c1e50ab0fc8", "patch": "@@ -1,4 +1,5 @@\n import sys\n+import getpass\n from cowait.tasks import TaskDefinition, TASK_LOG\n from cowait.engine import ProviderError, TaskCreationError\n from ..config import Config\n@@ -22,6 +23,12 @@ def test(\n         task = cluster.spawn(TaskDefinition(\n             name='cowait.test',\n             image=context.image,\n+            owner=getpass.getuser(),\n+            env={\n+                **context.environment,\n+                **context.dotenv,\n+            },\n+            volumes=context.get('volumes', {}),\n         ))\n \n         def destroy(*args):"}], "stats": {"total": 7, "additions": 7, "deletions": 0}}, {"sha": "9cb9f994836b9c06ecd92f6cc8860655a1682b3e", "html_url": "https://github.com/backtick-se/cowait/commit/9cb9f994836b9c06ecd92f6cc8860655a1682b3e", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-12T12:25:06Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-12T12:44:52Z"}, "message": "implement ClusterProvider.wait() for KubernetesProvider", "tree": {"sha": "273731087179346b0f84bf379969228dbf5e0c22", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/273731087179346b0f84bf379969228dbf5e0c22"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/9cb9f994836b9c06ecd92f6cc8860655a1682b3e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "3fbb64ec75d0722daf3e55889742295271c42609", "filename": "cowait/engine/kubernetes/kubernetes.py", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/backtick-se/cowait/blob/9cb9f994836b9c06ecd92f6cc8860655a1682b3e/cowait/engine/kubernetes/kubernetes.py", "raw_url": "https://github.com/backtick-se/cowait/raw/9cb9f994836b9c06ecd92f6cc8860655a1682b3e/cowait/engine/kubernetes/kubernetes.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/kubernetes/kubernetes.py?ref=9cb9f994836b9c06ecd92f6cc8860655a1682b3e", "patch": "@@ -275,3 +275,17 @@ def find_agent(self):\n         token = pod.metadata.labels['http_token']\n         return get_remote_url(pod.status.pod_ip, token)\n \n+    def wait(self, task: KubernetesTask) -> bool:\n+        while True:\n+            pod = self.get_task_pod(task.id)\n+            if pod is None:\n+                return False\n+\n+            if pod.status.phase == 'Running':\n+                time.sleep(2)\n+            elif pod.status.phase == 'Pending':\n+                time.sleep(2)\n+            elif pod.status.phase == 'Succeeded':\n+                return True\n+            else:\n+                return False"}], "stats": {"total": 14, "additions": 14, "deletions": 0}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 299, "title": "Traefik2 fixes", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/299", "html_url": "https://github.com/backtick-se/cowait/pull/299", "diff_url": "https://github.com/backtick-se/cowait/pull/299.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/299.patch", "merged_at": "2021-04-06T21:07:40Z"}, "body": "- Cast port numbers to integers\r\n- Properly clean up IngressRoutes on exit\r\n- Add support for configuring certificate resolvers", "commits": [{"sha": "2ed5579509e1ce47be7a65c1c40469eb30c0a983", "html_url": "https://github.com/backtick-se/cowait/commit/2ed5579509e1ce47be7a65c1c40469eb30c0a983", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-02T11:43:24Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-06T20:55:19Z"}, "message": "traefik2: properly cast port numbers to integers", "tree": {"sha": "b173a88fbc420a4cb2ec193679e6446514f82eba", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/b173a88fbc420a4cb2ec193679e6446514f82eba"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/2ed5579509e1ce47be7a65c1c40469eb30c0a983", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "c37b7abe075b838ed8584002279bcc7dd61c7246", "filename": "cowait/engine/routers/traefik2_router.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/backtick-se/cowait/blob/2ed5579509e1ce47be7a65c1c40469eb30c0a983/cowait/engine/routers/traefik2_router.py", "raw_url": "https://github.com/backtick-se/cowait/raw/2ed5579509e1ce47be7a65c1c40469eb30c0a983/cowait/engine/routers/traefik2_router.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/routers/traefik2_router.py?ref=2ed5579509e1ce47be7a65c1c40469eb30c0a983", "patch": "@@ -34,7 +34,7 @@ def on_prepare(self, taskdef):\n                 raise RuntimeError(f'Paths must start with /, got {path}')\n \n             taskdef.routes[path] = {\n-                'port': port,\n+                'port': int(port),\n                 'path': path,\n                 'url': f'{protocol}://{taskdef.id}.{domain}{path}',\n             }\n@@ -51,8 +51,8 @@ def on_spawn(self, task):\n             idx += 1\n \n             ports.append(client.V1ServicePort(\n-                port=port,\n-                target_port=port,\n+                port=int(port),\n+                target_port=int(port),\n             ))\n \n             host = f'{task.id}.{self.cluster.domain}'\n@@ -61,7 +61,7 @@ def on_spawn(self, task):\n                 'middlewares': self.middlewares,\n                 'kind': 'Rule',\n                 'services': [\n-                    {'name': task.id, 'port': port}\n+                    {'name': task.id, 'port': int(port)}\n                 ]\n             })\n "}], "stats": {"total": 8, "additions": 4, "deletions": 4}}, {"sha": "bbfcbdc6b46f876a70dc9dfe0567d9b3893cf7cb", "html_url": "https://github.com/backtick-se/cowait/commit/bbfcbdc6b46f876a70dc9dfe0567d9b3893cf7cb", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-06T20:56:30Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-06T20:56:30Z"}, "message": "traefik2: properly delete IngressRoutes & display teardown errors", "tree": {"sha": "23f4890e7b7304e6f1693cd4d6bb42d2b6eabfab", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/23f4890e7b7304e6f1693cd4d6bb42d2b6eabfab"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/bbfcbdc6b46f876a70dc9dfe0567d9b3893cf7cb", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "2302c475bac54efec56537559a9f198f8d7753f1", "filename": "cowait/engine/routers/traefik2_router.py", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "blob_url": "https://github.com/backtick-se/cowait/blob/bbfcbdc6b46f876a70dc9dfe0567d9b3893cf7cb/cowait/engine/routers/traefik2_router.py", "raw_url": "https://github.com/backtick-se/cowait/raw/bbfcbdc6b46f876a70dc9dfe0567d9b3893cf7cb/cowait/engine/routers/traefik2_router.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/routers/traefik2_router.py?ref=bbfcbdc6b46f876a70dc9dfe0567d9b3893cf7cb", "patch": "@@ -118,15 +118,16 @@ def on_kill(self, task_id):\n                 namespace=self.cluster.namespace,\n                 name=task_id,\n             )\n-        except client.rest.ApiException:\n-            pass\n+        except client.rest.ApiException as e:\n+            print('! error deleting Kubernetes Service:', e)\n \n         try:\n-            self.cluster.custom.delete_cluster_custom_object(\n+            self.cluster.custom.delete_namespaced_custom_object(\n                 group=TRAEFIK2_API_GROUP,\n                 version=TRAEFIK2_API_VERSION,\n                 plural=TRAEFIK2_INGRESSROUTE_PLURAL,\n+                namespace=self.cluster.namespace,\n                 name=task_id,\n             )\n-        except client.rest.ApiException:\n-            pass\n+        except client.rest.ApiException as e:\n+            print('! error deleting IngressRoute:', e)"}], "stats": {"total": 11, "additions": 6, "deletions": 5}}, {"sha": "5c95ab299373f777e5c443b561e2941f99859007", "html_url": "https://github.com/backtick-se/cowait/commit/5c95ab299373f777e5c443b561e2941f99859007", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-06T20:58:02Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-06T20:58:02Z"}, "message": "traefik2: add support for configuring CertificateResolver", "tree": {"sha": "3f2271f8968c83aba5ecbb22463e6446f18d287b", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/3f2271f8968c83aba5ecbb22463e6446f18d287b"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/5c95ab299373f777e5c443b561e2941f99859007", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "cb6fe044fdfafc93c8889f5425e9cd4d8de89ac5", "filename": "cowait/engine/routers/traefik2_router.py", "status": "modified", "additions": 36, "deletions": 14, "changes": 50, "blob_url": "https://github.com/backtick-se/cowait/blob/5c95ab299373f777e5c443b561e2941f99859007/cowait/engine/routers/traefik2_router.py", "raw_url": "https://github.com/backtick-se/cowait/raw/5c95ab299373f777e5c443b561e2941f99859007/cowait/engine/routers/traefik2_router.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/routers/traefik2_router.py?ref=5c95ab299373f777e5c443b561e2941f99859007", "patch": "@@ -1,3 +1,4 @@\n+from typing import List\n from kubernetes import client\n from .router import Router\n from ..const import LABEL_TASK_ID\n@@ -16,17 +17,32 @@ def __init__(self, cluster):\n         cluster.on('prepare', self.on_prepare)\n         cluster.on('spawn', self.on_spawn)\n         cluster.on('kill', self.on_kill)\n-\n         self.config = cluster.args.get('traefik2', {})\n-        self.secure = self.config.get('secure', False)\n-        self.middlewares = self.config.get('middlewares', [])\n-        self.entrypoints = self.config.get('entrypoints', ['web'])\n \n-    def on_prepare(self, taskdef):\n+    @property\n+    def secure(self) -> bool:\n+        return self.cert_resolver is not None\n+\n+    @property\n+    def middlewares(self) -> List[str]:\n+        return self.config.get('middlewares', [])\n+\n+    @property\n+    def entrypoints(self) -> List[str]:\n+        return self.config.get('entrypoints', ['web', 'websecure'])\n+\n+    @property\n+    def cert_resolver(self) -> str:\n+        return self.config.get('certresolver', None)\n+\n+    @property\n+    def domain(self) -> str:\n         domain = self.cluster.domain\n         if domain is None:\n             raise RuntimeError('No cluster domain configured')\n+        return domain\n \n+    def on_prepare(self, taskdef):\n         protocol = 'https' if self.secure else 'http'\n \n         for path, port in taskdef.routes.items():\n@@ -36,7 +52,7 @@ def on_prepare(self, taskdef):\n             taskdef.routes[path] = {\n                 'port': int(port),\n                 'path': path,\n-                'url': f'{protocol}://{taskdef.id}.{domain}{path}',\n+                'url': f'{protocol}://{taskdef.id}.{self.domain}{path}',\n             }\n \n         return taskdef\n@@ -57,12 +73,12 @@ def on_spawn(self, task):\n \n             host = f'{task.id}.{self.cluster.domain}'\n             routes.append({\n+                'kind': 'Rule',\n                 'match': f'Host(`{host}`) && PathPrefix(`{path}`)',\n                 'middlewares': self.middlewares,\n-                'kind': 'Rule',\n                 'services': [\n                     {'name': task.id, 'port': int(port)}\n-                ]\n+                ],\n             })\n \n         if len(routes) == 0:\n@@ -89,6 +105,15 @@ def on_spawn(self, task):\n             ),\n         )\n \n+        ingress = {\n+            'entryPoints': self.entrypoints,\n+            'routes': routes,\n+        }\n+        if self.secure:\n+            ingress['tls'] = {\n+                'certResolver': self.cert_resolver,\n+            }\n+\n         self.cluster.custom.create_namespaced_custom_object(\n             group=TRAEFIK2_API_GROUP,\n             version=TRAEFIK2_API_VERSION,\n@@ -105,10 +130,7 @@ def on_spawn(self, task):\n                         'ingress-for': task.id\n                     },\n                 },\n-                'spec': {\n-                    'entryPoints': self.entrypoints,\n-                    'routes': routes,\n-                }\n+                'spec': ingress,\n             },\n         )\n \n@@ -119,7 +141,7 @@ def on_kill(self, task_id):\n                 name=task_id,\n             )\n         except client.rest.ApiException as e:\n-            print('! error deleting Kubernetes Service:', e)\n+            print('!! error deleting Kubernetes Service:', e)\n \n         try:\n             self.cluster.custom.delete_namespaced_custom_object(\n@@ -130,4 +152,4 @@ def on_kill(self, task_id):\n                 name=task_id,\n             )\n         except client.rest.ApiException as e:\n-            print('! error deleting IngressRoute:', e)\n+            print('!! error deleting IngressRoute:', e)"}], "stats": {"total": 50, "additions": 36, "deletions": 14}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 298, "title": "Notebook/Clientfs Integration", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/298", "html_url": "https://github.com/backtick-se/cowait/pull/298", "diff_url": "https://github.com/backtick-se/cowait/pull/298.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/298.patch", "merged_at": "2021-05-06T14:39:56Z"}, "body": "Kubernetes notebooks now require `clientfs`. For now, `clientfs-darwin` or `clientfs-linux` must exist in the working directory.", "commits": [{"sha": "77b2fc48138c0843d7c45556a18f2979b702df01", "html_url": "https://github.com/backtick-se/cowait/commit/77b2fc48138c0843d7c45556a18f2979b702df01", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-02T19:19:18Z"}, "committer": {"name": "Martin Jakobsson", "email": "martin.jakobsson.5210@student.lu.se", "date": "2021-04-22T11:58:07Z"}, "message": "notebook clientfs integration", "tree": {"sha": "26fd9c57520e441867141103f7a40aeece266697", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/26fd9c57520e441867141103f7a40aeece266697"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/77b2fc48138c0843d7c45556a18f2979b702df01", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "02191088ebf987aedaef5b6b0053037f5bfae167", "filename": "cowait/cli/commands/notebook.py", "status": "modified", "additions": 140, "deletions": 26, "changes": 166, "blob_url": "https://github.com/backtick-se/cowait/blob/77b2fc48138c0843d7c45556a18f2979b702df01/cowait/cli/commands/notebook.py", "raw_url": "https://github.com/backtick-se/cowait/raw/77b2fc48138c0843d7c45556a18f2979b702df01/cowait/cli/commands/notebook.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/notebook.py?ref=77b2fc48138c0843d7c45556a18f2979b702df01", "patch": "@@ -1,15 +1,25 @@\n import os\n-from .run import run as run_cmd\n-from .build import build as build_cmd\n+import sys\n+import time\n+import getpass\n+import traceback\n+import subprocess\n+from kubernetes import client\n+from .run import RunLogger, run as run_cmd\n+# from .build import build as build_cmd\n from ..context import Context\n-from ..docker_file import Dockerfile\n-from ..task_image import TaskImage\n+# from ..docker_file import Dockerfile\n+# from ..task_image import TaskImage\n+from ..utils import ExitTrap\n+from cowait.utils import uuid\n+from cowait.tasks import TaskDefinition\n \n \n def notebook(config, build: bool, image: str = None, cluster_name: str = None) -> None:\n-    if image is None:\n-        context = Context.open(config)\n+    context = Context.open(config)\n \n+    \"\"\"\n+    if image is None:\n         # rebuild task image first\n         if build:\n             build_cmd(config, quiet=False)\n@@ -24,32 +34,136 @@ def notebook(config, build: bool, image: str = None, cluster_name: str = None) -\n             image = nbimage.short_id[7:]\n         finally:\n             os.removedirs(buildctx)\n+    \"\"\"\n \n-    return run_cmd(\n-        config=config,\n-        task='cowait.notebook',\n-        build=False,\n-        image=image,\n-        routes={\n-            '/': '8888',\n-        },\n-        cluster_name=cluster_name,\n-        volumes={\n-            '/var/task': {\n-                'bind': {\n-                    'src': os.getcwd(),\n-                    'mode': 'rw',\n-                    'inherit': 'same-image',\n-                },\n+    volumes = {\n+        '/var/task': {\n+            'bind': {\n+                'src': os.getcwd(),\n+                'mode': 'rw',\n+                'inherit': 'same-image',\n             },\n-        },\n+        }\n+    }\n+\n+    cluster = context.get_cluster(cluster_name)\n+    core = client.CoreV1Api()\n+    notebook_id = 'notebook-' + uuid(4)\n+\n+    core.create_namespaced_persistent_volume_claim(\n+        namespace=cluster.namespace,\n+        body=client.V1PersistentVolumeClaim(\n+            metadata=client.V1ObjectMeta(\n+                name=notebook_id,\n+                namespace=cluster.namespace,\n+            ),\n+            spec=client.V1PersistentVolumeClaimSpec(\n+                storage_class_name='clientfs',\n+                access_modes=['ReadWriteMany'],\n+                resources=client.V1ResourceRequirements(\n+                    requests={\n+                        'storage': '1G',\n+                    },\n+                ),\n+            ),\n+        ),\n     )\n \n+    def delete_pvc(task_id):\n+        print('destroy', task_id)\n+        if task_id != notebook_id:\n+            return\n+\n+        print('* stopping clientfs')\n+        clientfs.terminate()\n+\n+        print('* deleting volume')\n+        core.delete_namespaced_persistent_volume_claim(notebook_id, cluster.namespace)\n+\n+    cluster.on('kill', delete_pvc)\n+\n+    pvc_id = None\n+\n+    while True:\n+        time.sleep(1)\n+        volume = core.read_namespaced_persistent_volume_claim(notebook_id, cluster.namespace)\n+        if volume.status.phase == 'Bound':\n+            pvc_id = 'pvc-' + volume.metadata.uid\n+            print('* created volume', notebook_id, '/', pvc_id)\n+            break\n+\n+    volumes['/var/task'] = {\n+        'persistent_volume_claim': {\n+            'claim_name': notebook_id,\n+        },\n+    }\n+\n+    # start clientfs\n+    print('* starting clientfs')\n+    clientfs = subprocess.Popen([\n+        \"clientfs\",\n+        \"--proxy=hq.backtick.se:9091\",\n+        \"--uid=100\",\n+        \"--gid=1000\",\n+        f\"--volume={pvc_id}\"\n+    ])\n+\n+    logger = RunLogger()\n+    try:\n+        # default to agent as upstream\n+        agent = cluster.find_agent()\n+\n+        # create task definition\n+        taskdef = TaskDefinition(\n+            id=notebook_id,\n+            name='cowait.notebook',\n+            image=context.image,\n+            env={\n+                **context.extend('environment', {}),\n+                **context.dotenv,\n+            },\n+            routes={\n+                '/': '8888',\n+            },\n+            parent=None,  # root task\n+            upstream=agent,\n+            owner=getpass.getuser(),\n+            volumes=context.extend('volumes', volumes),\n+        )\n+\n+        # print execution info\n+        logger.print_info(taskdef, cluster)\n+\n+        # submit task to cluster\n+        task = cluster.spawn(taskdef)\n+\n+        detach = False\n+        if detach:\n+            logger.header('detached')\n+            return\n+\n+        def destroy(*args):\n+            logger.header('interrupt')\n+            cluster.destroy(task.id)\n+            sys.exit(1)\n+\n+        with ExitTrap(destroy):\n+            # capture & print logs\n+            logs = cluster.logs(task.id)\n+            logger.header('task output')\n+            for msg in logs:\n+                logger.handle(msg)\n+\n+    except Exception:\n+        traceback.print_exc()\n+        sys.exit(1)\n+\n+\n def run_notebook(\n-    config, path: str, \n+    config, path: str,\n     image: str, cluster: str, name: str, inputs: dict, env: dict,\n-    build: bool, detach: bool, quiet: bool):\n-\n+    build: bool, detach: bool, quiet: bool\n+):\n     run_cmd(\n         config,\n         task='cowait.tasks.notebook',"}], "stats": {"total": 166, "additions": 140, "deletions": 26}}, {"sha": "68c8692d01fb913ae83f18e65f7bc707fbe3950f", "html_url": "https://github.com/backtick-se/cowait/commit/68c8692d01fb913ae83f18e65f7bc707fbe3950f", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-04-02T19:20:06Z"}, "committer": {"name": "Martin Jakobsson", "email": "martin.jakobsson.5210@student.lu.se", "date": "2021-04-22T12:33:01Z"}, "message": "tasks inherit metadata", "tree": {"sha": "307840b49aa895ac3d6274f4be51f844aaf8efdf", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/307840b49aa895ac3d6274f4be51f844aaf8efdf"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/68c8692d01fb913ae83f18e65f7bc707fbe3950f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "59b96bc4fdb80d880dbec5d1437accdd0ac4200e", "filename": "cowait/cli/commands/notebook.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/backtick-se/cowait/blob/68c8692d01fb913ae83f18e65f7bc707fbe3950f/cowait/cli/commands/notebook.py", "raw_url": "https://github.com/backtick-se/cowait/raw/68c8692d01fb913ae83f18e65f7bc707fbe3950f/cowait/cli/commands/notebook.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/notebook.py?ref=68c8692d01fb913ae83f18e65f7bc707fbe3950f", "patch": "@@ -129,6 +129,9 @@ def delete_pvc(task_id):\n             upstream=agent,\n             owner=getpass.getuser(),\n             volumes=context.extend('volumes', volumes),\n+            meta={\n+                'agent': agent,\n+            },\n         )\n \n         # print execution info"}, {"sha": "3d4371e99cffec1805da510e60871028b4f7a337", "filename": "cowait/cli/commands/run.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/backtick-se/cowait/blob/68c8692d01fb913ae83f18e65f7bc707fbe3950f/cowait/cli/commands/run.py", "raw_url": "https://github.com/backtick-se/cowait/raw/68c8692d01fb913ae83f18e65f7bc707fbe3950f/cowait/cli/commands/run.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/run.py?ref=68c8692d01fb913ae83f18e65f7bc707fbe3950f", "patch": "@@ -91,6 +91,9 @@ def run(\n             memory=context.override('memory', memory),\n             memory_limit=context.override('memory_limit', memory_limit),\n             affinity=context.override('affinity', affinity),\n+            meta={\n+                'agent': agent,\n+            },\n         )\n \n         # print execution info"}, {"sha": "d751bf070901f8a5034fe557e8d6c16578ae226b", "filename": "cowait/notebook/kernel.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/backtick-se/cowait/blob/68c8692d01fb913ae83f18e65f7bc707fbe3950f/cowait/notebook/kernel.py", "raw_url": "https://github.com/backtick-se/cowait/raw/68c8692d01fb913ae83f18e65f7bc707fbe3950f/cowait/notebook/kernel.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/notebook/kernel.py?ref=68c8692d01fb913ae83f18e65f7bc707fbe3950f", "patch": "@@ -36,6 +36,7 @@ async def init_node(self):\n             env=parent.env,\n             upstream=get_local_url(),\n             meta={\n+                **parent.meta,\n                 'virtual': True,\n             },\n         )"}, {"sha": "e9ab5e54e21753e4cc09fe46272fcb34b94d94ab", "filename": "cowait/tasks/task.py", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/backtick-se/cowait/blob/68c8692d01fb913ae83f18e65f7bc707fbe3950f/cowait/tasks/task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/68c8692d01fb913ae83f18e65f7bc707fbe3950f/cowait/tasks/task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/tasks/task.py?ref=68c8692d01fb913ae83f18e65f7bc707fbe3950f", "patch": "@@ -153,7 +153,10 @@ def spawn(\n             parent=self.id,\n             image=image if image else self.image,\n             upstream=self.node.get_url(),\n-            meta=meta,\n+            meta={\n+                **self.taskdef.meta,\n+                **meta,\n+            },\n             ports=ports,\n             routes=routes,\n             cpu=cpu if cpu else self.taskdef.cpu,"}], "stats": {"total": 12, "additions": 11, "deletions": 1}}, {"sha": "af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "html_url": "https://github.com/backtick-se/cowait/commit/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "commit": {"author": {"name": "Martin Jakobsson", "email": "martin.jakobsson.5210@student.lu.se", "date": "2021-05-03T17:11:59Z"}, "committer": {"name": "Martin Jakobsson", "email": "martin.jakobsson.5210@student.lu.se", "date": "2021-05-03T17:11:59Z"}, "message": "fix some bugs related to cloud notebooks", "tree": {"sha": "d25bc0067d9693c3009b78065c0f25f2d1abf075", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/d25bc0067d9693c3009b78065c0f25f2d1abf075"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "ae9915ac25e17de0330b70acd9b97060156e3255", "filename": "cowait/cli/commands/agent.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/cli/commands/agent.py", "raw_url": "https://github.com/backtick-se/cowait/raw/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/cli/commands/agent.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/agent.py?ref=af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "patch": "@@ -44,6 +44,8 @@ def agent(\n             },\n         )\n \n+        logger.id = taskdef.id\n+\n         # submit task to cluster\n         task = cluster.spawn(taskdef)\n "}, {"sha": "eaaad6638e199c47fb8948d52cac2e483178665b", "filename": "cowait/cli/commands/notebook.py", "status": "modified", "additions": 18, "deletions": 5, "changes": 23, "blob_url": "https://github.com/backtick-se/cowait/blob/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/cli/commands/notebook.py", "raw_url": "https://github.com/backtick-se/cowait/raw/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/cli/commands/notebook.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/notebook.py?ref=af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "patch": "@@ -47,6 +47,22 @@ def notebook(config, build: bool, image: str = None, cluster_name: str = None) -\n     }\n \n     cluster = context.get_cluster(cluster_name)\n+\n+    # Docker\n+    if cluster.type == 'docker':\n+        return run_cmd(\n+            config=config,\n+            task='cowait.notebook',\n+            build=False,\n+            image=image,\n+            routes={\n+                '/': '8888',\n+            },\n+            cluster_name=cluster_name,\n+            volumes=volumes,\n+        )\n+\n+    # Kubernetes\n     core = client.CoreV1Api()\n     notebook_id = 'notebook-' + uuid(4)\n \n@@ -103,8 +119,8 @@ def delete_pvc(task_id):\n     clientfs = subprocess.Popen([\n         \"clientfs\",\n         \"--proxy=hq.backtick.se:9091\",\n-        \"--uid=100\",\n-        \"--gid=1000\",\n+        \"--uid=0\",\n+        \"--gid=0\",\n         f\"--volume={pvc_id}\"\n     ])\n \n@@ -129,9 +145,6 @@ def delete_pvc(task_id):\n             upstream=agent,\n             owner=getpass.getuser(),\n             volumes=context.extend('volumes', volumes),\n-            meta={\n-                'agent': agent,\n-            },\n         )\n \n         # print execution info"}, {"sha": "576e17e254ae824e1c65c14c51ba94cef190bee2", "filename": "cowait/cli/commands/run.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "blob_url": "https://github.com/backtick-se/cowait/blob/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/cli/commands/run.py", "raw_url": "https://github.com/backtick-se/cowait/raw/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/cli/commands/run.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/run.py?ref=af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "patch": "@@ -91,9 +91,6 @@ def run(\n             memory=context.override('memory', memory),\n             memory_limit=context.override('memory_limit', memory_limit),\n             affinity=context.override('affinity', affinity),\n-            meta={\n-                'agent': agent,\n-            },\n         )\n \n         # print execution info"}, {"sha": "7fa7301bff26d231d7da8bb70d9647743469517a", "filename": "cowait/cli/commands/task.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/backtick-se/cowait/blob/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/cli/commands/task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/cli/commands/task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/task.py?ref=af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "patch": "@@ -48,6 +48,7 @@ def logs(config: Config, task_id: str, cluster_name: str, raw: bool = False):\n     \n         logs = cluster.logs(task_id)\n         logger = RunLogger(raw)\n+        logger.id = task_id\n         logger.header('task output')\n         for msg in logs:\n             logger.handle(msg)"}, {"sha": "86edd4e9ff8068b250b3df8d3471d632e76f1ba3", "filename": "cowait/engine/const.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/engine/const.py", "raw_url": "https://github.com/backtick-se/cowait/raw/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/engine/const.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/const.py?ref=af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "patch": "@@ -21,6 +21,6 @@\n # Settings\n # ---------------------------------------------------\n \n-# Maximum length of environment variables, 100kB\n+# Maximum length of environment variables, 500kB\n MAX_ENV_LENGTH = 500 * 1024\n "}, {"sha": "bc15aa749e24a938dd53de55597a79d714e46197", "filename": "cowait/tasks/notebook/notebook_runner.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/tasks/notebook/notebook_runner.py", "raw_url": "https://github.com/backtick-se/cowait/raw/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/tasks/notebook/notebook_runner.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/tasks/notebook/notebook_runner.py?ref=af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "patch": "@@ -28,7 +28,7 @@ def cells_to_code(cells):\n         if cell_type == 'code':\n             source_rows = [row[:-1] if row[-1] == '\\n' else row for row in cell['source']]\n \n-            if len(source_rows) >= 0:\n+            if len(source_rows) > 0:\n                 code.appendBlock(0, code_from_source(source_rows))\n                 code.append(0, '')\n "}, {"sha": "e471b3797966d68536cd4fe139840a93aac760a9", "filename": "cowait/tasks/task.py", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "blob_url": "https://github.com/backtick-se/cowait/blob/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/tasks/task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/tasks/task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/tasks/task.py?ref=af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "patch": "@@ -153,10 +153,7 @@ def spawn(\n             parent=self.id,\n             image=image if image else self.image,\n             upstream=self.node.get_url(),\n-            meta={\n-                **self.taskdef.meta,\n-                **meta,\n-            },\n+            meta=meta,\n             ports=ports,\n             routes=routes,\n             cpu=cpu if cpu else self.taskdef.cpu,"}], "stats": {"total": 38, "additions": 24, "deletions": 14}}, {"sha": "a2b7bbb9116c9a28d14073757471b796e7913f79", "html_url": "https://github.com/backtick-se/cowait/commit/a2b7bbb9116c9a28d14073757471b796e7913f79", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-06T12:22:41Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-06T12:22:41Z"}, "message": "removed PickledTask", "tree": {"sha": "6877622e30d3caabd35f96790c35829c37bb8e32", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/6877622e30d3caabd35f96790c35829c37bb8e32"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/a2b7bbb9116c9a28d14073757471b796e7913f79", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "6e8d8bea7f1559052575d884c6b9ae25751377e9", "filename": "cowait/notebook/__init__.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/backtick-se/cowait/blob/a2b7bbb9116c9a28d14073757471b796e7913f79/cowait/notebook/__init__.py", "raw_url": "https://github.com/backtick-se/cowait/raw/a2b7bbb9116c9a28d14073757471b796e7913f79/cowait/notebook/__init__.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/notebook/__init__.py?ref=a2b7bbb9116c9a28d14073757471b796e7913f79", "patch": "@@ -1,4 +1,3 @@\n # flake8: noqa: F401\n \n from .task import NotebookTask\n-from .spawn import task"}, {"sha": "080387d8f448e520e4de4f9038b88b5453c46b48", "filename": "cowait/notebook/spawn.py", "status": "removed", "additions": 0, "deletions": 9, "changes": 9, "blob_url": "https://github.com/backtick-se/cowait/blob/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/notebook/spawn.py", "raw_url": "https://github.com/backtick-se/cowait/raw/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/notebook/spawn.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/notebook/spawn.py?ref=af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "patch": "@@ -1,9 +0,0 @@\n-from cowait.tasks.pickled import PickledTask, pickle_task\n-\n-\n-def task(task_func):\n-    def spawn_task(*args, **inputs):\n-        if len(args) > 0:\n-            raise TypeError('Use keyword arguments for task inputs')\n-        return PickledTask(func=pickle_task(task_func), **inputs)\n-    return spawn_task"}, {"sha": "f3962f8d0099e829dd5cbad62f1e1b8041e3d50c", "filename": "cowait/tasks/pickled/__init__.py", "status": "removed", "additions": 0, "deletions": 4, "changes": 4, "blob_url": "https://github.com/backtick-se/cowait/blob/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/tasks/pickled/__init__.py", "raw_url": "https://github.com/backtick-se/cowait/raw/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/tasks/pickled/__init__.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/tasks/pickled/__init__.py?ref=af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "patch": "@@ -1,4 +0,0 @@\n-# flake8: noqa: F401\n-\n-from .pickled_task import PickledTask\n-from .pickle_task import pickle_task, unpickle_task"}, {"sha": "e4d5c2b84087dcbf5c9725d7fba9fc6fc2e283d1", "filename": "cowait/tasks/pickled/pickle_task.py", "status": "removed", "additions": 0, "deletions": 12, "changes": 12, "blob_url": "https://github.com/backtick-se/cowait/blob/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/tasks/pickled/pickle_task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/tasks/pickled/pickle_task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/tasks/pickled/pickle_task.py?ref=af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "patch": "@@ -1,12 +0,0 @@\n-import dill\n-from base64 import b64encode, b64decode\n-\n-\n-def pickle_task(task):\n-    serialized = dill.dumps(task)\n-    return str(b64encode(serialized), encoding='utf-8')\n-\n-\n-def unpickle_task(task):\n-    funcbytes = b64decode(task)\n-    return dill.loads(funcbytes)"}, {"sha": "e0c2b127c8188d0dae9716b29f126de93718591e", "filename": "cowait/tasks/pickled/pickled_task.py", "status": "removed", "additions": 0, "deletions": 12, "changes": 12, "blob_url": "https://github.com/backtick-se/cowait/blob/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/tasks/pickled/pickled_task.py", "raw_url": "https://github.com/backtick-se/cowait/raw/af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb/cowait/tasks/pickled/pickled_task.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/tasks/pickled/pickled_task.py?ref=af6d44904f7e3dcf13390b2cbc7ab0657a3cd2bb", "patch": "@@ -1,12 +0,0 @@\n-import inspect\n-from ..task import Task\n-from .pickle_task import unpickle_task\n-\n-\n-class PickledTask(Task):\n-    async def run(self, func: str, **inputs):\n-        func = unpickle_task(func)\n-        if inspect.iscoroutinefunction(func):\n-            return await func(**inputs)\n-        else:\n-            return func(**inputs)"}], "stats": {"total": 38, "additions": 0, "deletions": 38}}, {"sha": "92bbd0f1f7cbff82fc30a778d42f3bc6105bee1c", "html_url": "https://github.com/backtick-se/cowait/commit/92bbd0f1f7cbff82fc30a778d42f3bc6105bee1c", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-06T12:39:17Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-06T12:39:17Z"}, "message": "increase linter max line length to 120", "tree": {"sha": "9b46df3fad70b6298c578d62b6bb111dde0098fc", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/9b46df3fad70b6298c578d62b6bb111dde0098fc"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/92bbd0f1f7cbff82fc30a778d42f3bc6105bee1c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "2f1e04657037685ecdf76669460c5170b8d8da6d", "filename": ".flake8", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/92bbd0f1f7cbff82fc30a778d42f3bc6105bee1c/.flake8", "raw_url": "https://github.com/backtick-se/cowait/raw/92bbd0f1f7cbff82fc30a778d42f3bc6105bee1c/.flake8", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/.flake8?ref=92bbd0f1f7cbff82fc30a778d42f3bc6105bee1c", "patch": "@@ -1,5 +1,5 @@\n [flake8]\n-max-line-length = 100\n+max-line-length = 120\n exclude =\n     __pycache__,\n     .git,"}], "stats": {"total": 2, "additions": 1, "deletions": 1}}, {"sha": "404b8cce89a58bc48ed4d1f7c7d825d48e02f927", "html_url": "https://github.com/backtick-se/cowait/commit/404b8cce89a58bc48ed4d1f7c7d825d48e02f927", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-06T12:39:57Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-06T14:18:59Z"}, "message": "improved notebook building", "tree": {"sha": "3bcf2a800d3ae96e3bd6e3c674031164fc323719", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/3bcf2a800d3ae96e3bd6e3c674031164fc323719"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/404b8cce89a58bc48ed4d1f7c7d825d48e02f927", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "aa68b019b1129c135b0facd3ceddb81eb1dd901c", "filename": "cowait/cli/app/notebook.py", "status": "modified", "additions": 5, "deletions": 9, "changes": 14, "blob_url": "https://github.com/backtick-se/cowait/blob/404b8cce89a58bc48ed4d1f7c7d825d48e02f927/cowait/cli/app/notebook.py", "raw_url": "https://github.com/backtick-se/cowait/raw/404b8cce89a58bc48ed4d1f7c7d825d48e02f927/cowait/cli/app/notebook.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/app/notebook.py?ref=404b8cce89a58bc48ed4d1f7c7d825d48e02f927", "patch": "@@ -10,18 +10,14 @@\n               default=None,\n               type=str,\n               help='cluster name')\n-@click.option('-b', '--build',\n-              type=bool, is_flag=True,\n-              help='build and push first',\n-              default=False)\n @click.option('-i', '--image',\n               type=str,\n               default=None,\n-              help='default image')\n+              help='remote image')\n @click.pass_context\n-def notebook(ctx, cluster, build, image):\n+def notebook(ctx, cluster, image):\n     if ctx.invoked_subcommand is None:\n-        cowait.cli.notebook(ctx.obj, build, image, cluster_name=cluster)\n+        cowait.cli.notebook(ctx.obj, image, cluster_name=cluster)\n \n \n @notebook.command(help='run notebook as a task')\n@@ -65,8 +61,8 @@ def notebook(ctx, cluster, build, image):\n @click.pass_context\n def run(\n     ctx, path: str, image: str, cluster: str, name: str,\n-    input, env, build: bool, detach: bool, file: str, quiet: bool):\n-\n+    input, env, build: bool, detach: bool, file: str, quiet: bool\n+):\n     file_inputs = {}\n     if file is not None:\n         try:"}, {"sha": "e553733e1f52514188a2a5316caf8d2bd5d2e177", "filename": "cowait/cli/commands/notebook.py", "status": "modified", "additions": 23, "deletions": 29, "changes": 52, "blob_url": "https://github.com/backtick-se/cowait/blob/404b8cce89a58bc48ed4d1f7c7d825d48e02f927/cowait/cli/commands/notebook.py", "raw_url": "https://github.com/backtick-se/cowait/raw/404b8cce89a58bc48ed4d1f7c7d825d48e02f927/cowait/cli/commands/notebook.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/commands/notebook.py?ref=404b8cce89a58bc48ed4d1f7c7d825d48e02f927", "patch": "@@ -2,39 +2,28 @@\n import sys\n import time\n import getpass\n+import platform\n import traceback\n import subprocess\n from kubernetes import client\n-from .run import RunLogger, run as run_cmd\n-# from .build import build as build_cmd\n-from ..context import Context\n-# from ..docker_file import Dockerfile\n-# from ..task_image import TaskImage\n-from ..utils import ExitTrap\n-from cowait.utils import uuid\n+from cowait.cli.utils import ExitTrap\n+from cowait.cli.context import Context\n+from cowait.cli.commands.run import RunLogger, run as run_cmd\n from cowait.tasks import TaskDefinition\n+from cowait.utils import uuid\n \n \n-def notebook(config, build: bool, image: str = None, cluster_name: str = None) -> None:\n+def notebook(config, image: str = None, cluster_name: str = None) -> None:\n     context = Context.open(config)\n \n-    \"\"\"\n-    if image is None:\n-        # rebuild task image first\n-        if build:\n-            build_cmd(config, quiet=False)\n-\n-        df = Dockerfile(context.image)\n-        df.run('pip install jupyterlab dill --no-cache-dir')\n+    if not context.notebook:\n+        print('Notebook funcitonaility is not enabled.')\n+        print('To enable, set features.notebook to True in cowait.yml and rebuild.')\n+        sys.exit(1)\n \n-        buildctx = '/tmp/cowait-notebook-ctx'\n-        os.makedirs(buildctx, exist_ok=True)\n-        try:\n-            nbimage = TaskImage.build_image(path=buildctx, dockerfile=str(df), quiet=False)\n-            image = nbimage.short_id[7:]\n-        finally:\n-            os.removedirs(buildctx)\n-    \"\"\"\n+    if image is not None:\n+        print('Remote images are currently not supported')\n+        sys.exit(1)\n \n     volumes = {\n         '/var/task': {\n@@ -62,6 +51,12 @@ def notebook(config, build: bool, image: str = None, cluster_name: str = None) -\n             volumes=volumes,\n         )\n \n+    # check for clientfs\n+    clientfs_executable = './clientfs-' + platform.system().lower()\n+    if not os.path.exists(clientfs_executable):\n+        print('Kubernetes notebooks are not supported in this build of Cowait')\n+        sys.exit(1)\n+\n     # Kubernetes\n     core = client.CoreV1Api()\n     notebook_id = 'notebook-' + uuid(4)\n@@ -115,12 +110,11 @@ def delete_pvc(task_id):\n     }\n \n     # start clientfs\n-    print('* starting clientfs')\n+    clientfs_host = f'{cluster.domain}:9091'\n+    print(f'* connecting clientfs volume to {clientfs_host}...')\n     clientfs = subprocess.Popen([\n-        \"clientfs\",\n-        \"--proxy=hq.backtick.se:9091\",\n-        \"--uid=0\",\n-        \"--gid=0\",\n+        clientfs_executable,\n+        f\"--proxy={clientfs_host}\",\n         f\"--volume={pvc_id}\"\n     ])\n "}, {"sha": "0740f405a9cf3ec22118aedc97410402d2dfac5a", "filename": "cowait/cli/context.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/backtick-se/cowait/blob/404b8cce89a58bc48ed4d1f7c7d825d48e02f927/cowait/cli/context.py", "raw_url": "https://github.com/backtick-se/cowait/raw/404b8cce89a58bc48ed4d1f7c7d825d48e02f927/cowait/cli/context.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/context.py?ref=404b8cce89a58bc48ed4d1f7c7d825d48e02f927", "patch": "@@ -32,6 +32,10 @@ def base(self):\n     def environment(self):\n         return self.get('environment', {}, False)\n \n+    @property\n+    def notebook(self) -> bool:\n+        return self.get('features.notebook', False, False)\n+\n     @property\n     def dotenv(self) -> dict:\n         return dotenv_values(self.file('.env'))"}, {"sha": "b02211ac87ebc459f6e354f3bb7b965f331778ba", "filename": "cowait/cli/docker_file.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/backtick-se/cowait/blob/404b8cce89a58bc48ed4d1f7c7d825d48e02f927/cowait/cli/docker_file.py", "raw_url": "https://github.com/backtick-se/cowait/raw/404b8cce89a58bc48ed4d1f7c7d825d48e02f927/cowait/cli/docker_file.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/docker_file.py?ref=404b8cce89a58bc48ed4d1f7c7d825d48e02f927", "patch": "@@ -24,6 +24,9 @@ def env(self, key: str, value: str):\n     def workdir(self, path):\n         self.lines.append(f'WORKDIR {path}')\n \n+    def label(self, key: str, value: str):\n+        self.lines.append(f'LABEL {key}={value}')\n+\n     def __str__(self):\n         return '\\n'.join(self.lines)\n "}, {"sha": "e41d8cbb8391e6feb03b7537829c20e2f759d8d8", "filename": "cowait/cli/task_image.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "blob_url": "https://github.com/backtick-se/cowait/blob/404b8cce89a58bc48ed4d1f7c7d825d48e02f927/cowait/cli/task_image.py", "raw_url": "https://github.com/backtick-se/cowait/raw/404b8cce89a58bc48ed4d1f7c7d825d48e02f927/cowait/cli/task_image.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/cli/task_image.py?ref=404b8cce89a58bc48ed4d1f7c7d825d48e02f927", "patch": "@@ -1,8 +1,8 @@\n import os\n import os.path\n import sys\n-import json\n import docker\n+import cowait\n from .context import Context\n from .docker_file import Dockerfile\n \n@@ -26,10 +26,15 @@ def build(self, base: str, requirements: str = None, buildargs: dict = {}, quiet\n         # create temporary dockerfile\n         df = Dockerfile(base)\n \n+        # extra features\n+        if self.context.notebook:\n+            df.run('pip install jupyterlab --no-cache-dir')\n+            df.label('cowait.feature.notebook', 'true')\n+\n         # install task-specific requirements\n         if requirements:\n             df.copy(f'./{requirements}', './requirements.txt')\n-            df.run('pip install -r ./requirements.txt')\n+            df.run('pip install -r ./requirements.txt --no-cache-dir')\n \n         # copy source code\n         df.copy('.', '.')\n@@ -38,6 +43,7 @@ def build(self, base: str, requirements: str = None, buildargs: dict = {}, quiet\n         if workdir != '.':\n             df.workdir(os.path.join('/var/task', workdir))\n \n+        df.label('cowait.version', cowait.version)\n         self.image = TaskImage.build_image(\n             dockerfile=str(df),\n             path=self.context.root_path,"}, {"sha": "63e4d660fa462006bd6aa048c566b8dcb5b8227f", "filename": "cowait/tasks/notebook/notebook_runner.py", "status": "modified", "additions": 16, "deletions": 9, "changes": 25, "blob_url": "https://github.com/backtick-se/cowait/blob/404b8cce89a58bc48ed4d1f7c7d825d48e02f927/cowait/tasks/notebook/notebook_runner.py", "raw_url": "https://github.com/backtick-se/cowait/raw/404b8cce89a58bc48ed4d1f7c7d825d48e02f927/cowait/tasks/notebook/notebook_runner.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/tasks/notebook/notebook_runner.py?ref=404b8cce89a58bc48ed4d1f7c7d825d48e02f927", "patch": "@@ -1,25 +1,29 @@\n+import ast\n+import json\n import cowait\n from cowait import Task\n from .code_builder import CodeBuilder\n-import ast\n-import json\n+\n \n class NotebookRunner(Task):\n     async def run(self, path: str, **inputs):\n+        if not path.endswith('.ipynb'):\n+            path += '.ipynb'\n         cells = file_to_json(path)['cells']\n \n         code = CodeBuilder()\n \n         code.append(0, 'async def _notebook_runner():')\n \n         code.appendBlock(4, cells_to_code(cells))\n-        \n+\n         global_scope = {'cowait': cowait, 'NotebookRunner': self.__class__}\n         local_scope = {}\n         exec(str(code), global_scope, local_scope)\n         handle = local_scope['_notebook_runner']\n         return await handle()\n \n+\n def cells_to_code(cells):\n     code = CodeBuilder()\n     for cell in cells:\n@@ -34,6 +38,7 @@ def cells_to_code(cells):\n \n     return code\n \n+\n def code_from_source(source_rows):\n     code = CodeBuilder()\n \n@@ -50,7 +55,7 @@ def code_from_source(source_rows):\n                 new_line = 'nonlocal ' + line[7:]\n                 code.append(indentation, new_line)\n                 print(f\"Warning: Replaced '{line}' with '{new_line}'\")\n-            elif line.startswith('%'): # Not supported: a = %ls\n+            elif line.startswith('%'):  # Not supported: a = %ls\n                 command, *args = line[1:].split(' ', 1)\n                 new_code = transform_line_magic(command, args)\n                 if new_code:\n@@ -64,21 +69,24 @@ def code_from_source(source_rows):\n     except SyntaxError as e:\n         syntax_error = SyntaxError(f\"{e.msg}\\nThe error is located somewhere in this cell:\\n\\n{str(code)}\", e.args[1])\n         raise syntax_error from None\n-    \n+\n     return code\n \n+\n def strip_indentation(row):\n     line = row.lstrip(' ')\n     return len(row) - len(line), line\n \n+\n def transform_line_magic(command: str, args: str):\n     ignorables = ['lsmagic', 'matplotlib']\n \n     if command in ignorables:\n         return None\n-    \n+\n     raise ValueError(f\"Magic command %{command} is not supported\")\n \n+\n def transform_cell_magic(rows: list, command: str, args: str):\n     ignorables = ['html', 'HTML', 'markdown', 'latex']\n \n@@ -87,8 +95,7 @@ def transform_cell_magic(rows: list, command: str, args: str):\n \n     raise ValueError(f\"Magic command %%{command} is not supported\")\n \n+\n def file_to_json(path):\n-    s = None\n     with open(path, 'r') as f:\n-        s = f.read()\n-    return json.loads(s)\n+        return json.load(f)"}], "stats": {"total": 108, "additions": 59, "deletions": 49}}, {"sha": "04e7373ec1c54b465a18fcc1cdf8a98672c1a4e6", "html_url": "https://github.com/backtick-se/cowait/commit/04e7373ec1c54b465a18fcc1cdf8a98672c1a4e6", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-06T14:02:50Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-06T14:18:59Z"}, "message": "custom StoppedError display for notebooks", "tree": {"sha": "224d908df48386f33cafa537b082ba8000049d03", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/224d908df48386f33cafa537b082ba8000049d03"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/04e7373ec1c54b465a18fcc1cdf8a98672c1a4e6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "0062d74cb2a48296fb440e83b02c983d540b1a86", "filename": "cowait/tasks/errors.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/04e7373ec1c54b465a18fcc1cdf8a98672c1a4e6/cowait/tasks/errors.py", "raw_url": "https://github.com/backtick-se/cowait/raw/04e7373ec1c54b465a18fcc1cdf8a98672c1a4e6/cowait/tasks/errors.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/tasks/errors.py?ref=04e7373ec1c54b465a18fcc1cdf8a98672c1a4e6", "patch": "@@ -1,10 +1,17 @@\n+import json\n \n \n class StoppedError(RuntimeError):\n     def __init__(self, result):\n         super().__init__('Exited')\n         self.result = result\n \n+    def _render_traceback_(self):\n+        try:\n+            return ['Returned with result:'] + json.dumps(self.result, indent=2).split('\\n')\n+        except TypeError:\n+            return ['Returned with result:', str(self.result)]\n+\n \n class TaskError(RuntimeError):\n     \"\"\""}], "stats": {"total": 7, "additions": 7, "deletions": 0}}, {"sha": "2a514cd422c784498f31a4aebaa48a3e1c9edcec", "html_url": "https://github.com/backtick-se/cowait/commit/2a514cd422c784498f31a4aebaa48a3e1c9edcec", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-06T14:03:52Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-06T14:18:59Z"}, "message": "supress NotFound exceptions from traefik2 router cleanup", "tree": {"sha": "742d961415fa5a3e54866b7e97c22df61e850317", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/742d961415fa5a3e54866b7e97c22df61e850317"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/2a514cd422c784498f31a4aebaa48a3e1c9edcec", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "8267038dcc44fe46748472221516735880653c16", "filename": "cowait/engine/routers/traefik2_router.py", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "blob_url": "https://github.com/backtick-se/cowait/blob/2a514cd422c784498f31a4aebaa48a3e1c9edcec/cowait/engine/routers/traefik2_router.py", "raw_url": "https://github.com/backtick-se/cowait/raw/2a514cd422c784498f31a4aebaa48a3e1c9edcec/cowait/engine/routers/traefik2_router.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/engine/routers/traefik2_router.py?ref=2a514cd422c784498f31a4aebaa48a3e1c9edcec", "patch": "@@ -1,5 +1,6 @@\n from typing import List\n from kubernetes import client\n+import kubernetes.client.exceptions\n from .router import Router\n from ..const import LABEL_TASK_ID\n \n@@ -140,8 +141,9 @@ def on_kill(self, task_id):\n                 namespace=self.cluster.namespace,\n                 name=task_id,\n             )\n-        except client.rest.ApiException as e:\n-            print('!! error deleting Kubernetes Service:', e)\n+        except kubernetes.client.exceptions.ApiException as e:\n+            if e.status != 404:\n+                print('!! error deleting Kubernetes Service:', e)\n \n         try:\n             self.cluster.custom.delete_namespaced_custom_object(\n@@ -151,5 +153,6 @@ def on_kill(self, task_id):\n                 namespace=self.cluster.namespace,\n                 name=task_id,\n             )\n-        except client.rest.ApiException as e:\n-            print('!! error deleting IngressRoute:', e)\n+        except kubernetes.client.exceptions.ApiException as e:\n+            if e.status != 404:\n+                print('!! error deleting IngressRoute:', e)"}], "stats": {"total": 11, "additions": 7, "deletions": 4}}, {"sha": "157fe225f22b55685391210d5ea6cbcc8739bcb9", "html_url": "https://github.com/backtick-se/cowait/commit/157fe225f22b55685391210d5ea6cbcc8739bcb9", "commit": {"author": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-06T14:20:10Z"}, "committer": {"name": "Johan Henriksson", "email": "johanhenriksson@live.com", "date": "2021-05-06T14:20:10Z"}, "message": "bump version to 0.4.21", "tree": {"sha": "0a62a23f39c0bfee5b68d3e8bae0c4ead3058d9e", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/0a62a23f39c0bfee5b68d3e8bae0c4ead3058d9e"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/157fe225f22b55685391210d5ea6cbcc8739bcb9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "c5454e9cd8f75ad1ff61f0cf1e09862f6f226943", "filename": "cowait/version.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/157fe225f22b55685391210d5ea6cbcc8739bcb9/cowait/version.py", "raw_url": "https://github.com/backtick-se/cowait/raw/157fe225f22b55685391210d5ea6cbcc8739bcb9/cowait/version.py", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/cowait/version.py?ref=157fe225f22b55685391210d5ea6cbcc8739bcb9", "patch": "@@ -1,2 +1,2 @@\n-version=\"0.4.20\"\n+version=\"0.4.21\"\n "}, {"sha": "25733961d8ca7a716e9e289d8c6bd4c0e48a6a2d", "filename": "pyproject.toml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/backtick-se/cowait/blob/157fe225f22b55685391210d5ea6cbcc8739bcb9/pyproject.toml", "raw_url": "https://github.com/backtick-se/cowait/raw/157fe225f22b55685391210d5ea6cbcc8739bcb9/pyproject.toml", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/pyproject.toml?ref=157fe225f22b55685391210d5ea6cbcc8739bcb9", "patch": "@@ -1,6 +1,6 @@\n [tool.poetry]\n name = \"cowait\"\n-version = \"0.4.20\"\n+version = \"0.4.21\"\n description = \"\"\n authors = [\"Backtick Technologies <johan@backtick.se>\"]\n license = \"Apache License v2.0\""}], "stats": {"total": 4, "additions": 2, "deletions": 2}}]}, {"docs": [["cowait/docs/why-cowait.md", "---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/tasks/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/tasks/type-system.md", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n"], ["cowait/docs/tasks/task-lifecycle-methods.md", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n"], ["cowait/docs/tasks/built-in-tasks.md", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n"], ["cowait/docs/tasks/remote-procedure-calls.md", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n"], ["cowait/docs/setup/custom-dockerfile.md", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n"], ["cowait/docs/setup/configuration.md", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/contributing.md", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/overview.md", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/no-scheduler.md", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/everything-is-a-task.md", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/task-hierarchy.md", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/core-concepts/engines.md", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/routing.md", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n"], ["cowait/docs/kubernetes/cluster-management.md", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/kubernetes/setup.md", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n"], ["cowait/docs/kubernetes/testing.md", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n"], ["cowait/docs/kubernetes/pushing-and-running.md", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n"], ["cowait/docs/get-started/first-steps.md", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n"], ["cowait/docs/get-started/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n"], ["cowait/docs/get-started/dependencies.md", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n"], ["cowait/docs/get-started/building-and-pushing.md", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n"], ["cowait/docs/get-started/tests.md", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n"], ["cowait/docs/get-started/next-steps.md", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n"], ["cowait/docs/get-started/installation.md", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n"], ["cowait/docs/get-started/asyncio.md", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n"], ["cowait/docs/extras/spark.md", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/dashboard.md", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/dask.md", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n"], ["cowait/docs/extras/databricks.md", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], ["cowait/docs/extras/notebook-integration.md", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"]], "number": 296, "title": "Docs content", "labels": [{"id": 1562464618, "node_id": "MDU6TGFiZWwxNTYyNDY0NjE4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/documentation", "name": "documentation", "color": "0075ca", "default": true, "description": "Improvements or additions to documentation"}, {"id": 1562464620, "node_id": "MDU6TGFiZWwxNTYyNDY0NjIw", "url": "https://api.github.com/repos/backtick-se/cowait/labels/enhancement", "name": "enhancement", "color": "9cdd56", "default": true, "description": "Improve existing features"}, {"id": 2035840512, "node_id": "MDU6TGFiZWwyMDM1ODQwNTEy", "url": "https://api.github.com/repos/backtick-se/cowait/labels/onboarding", "name": "onboarding", "color": "f4d9a6", "default": false, "description": "Onboarding / Installation"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/296", "html_url": "https://github.com/backtick-se/cowait/pull/296", "diff_url": "https://github.com/backtick-se/cowait/pull/296.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/296.patch", "merged_at": "2021-05-17T12:33:47Z"}, "body": "Leaving this PR open for some time to test some markdown + images rendering.\r\n\r\nEdit:\r\nAdded markdown folders and content. The `/docs` folder specifies to docs content on https://cowait.io/docs/", "commits": [{"sha": "cb3bb128bc29286848b861b907037896f6c0e3ad", "html_url": "https://github.com/backtick-se/cowait/commit/cb3bb128bc29286848b861b907037896f6c0e3ad", "commit": {"author": {"name": "Oskar Handmark", "email": "oskarhandmark@gmail.com", "date": "2021-04-30T11:51:53Z"}, "committer": {"name": "Oskar Handmark", "email": "oskarhandmark@gmail.com", "date": "2021-04-30T11:51:53Z"}, "message": "add documentation markdown folders and content", "tree": {"sha": "c5b345beef587820b27c541beeffd0d6280a64b1", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/c5b345beef587820b27c541beeffd0d6280a64b1"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/cb3bb128bc29286848b861b907037896f6c0e3ad", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "55ce95a0e2354dcb511e951a08bb6e1c88061874", "filename": "docs/contributing.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/contributing.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/contributing.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/contributing.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,7 @@\n+---\n+title: Contributing\n+---\n+\n+## Contributions Welcome\n+\n+This part of the documentation does not yet exist. You can help create it on github, see the link below."}, {"sha": "8b3d418290e6ee6ccb55f600a4b038277473d89b", "filename": "docs/core-concepts/engines.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/core-concepts/engines.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/core-concepts/engines.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/core-concepts/engines.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,7 @@\n+---\n+title: Engines\n+---\n+\n+## Contributions Welcome\n+\n+This part of the documentation does not yet exist. You can help create it on github, see the link below."}, {"sha": "293b3c2d201eb1f58e61af61fcb6b064a746e904", "filename": "docs/core-concepts/everything-is-a-task.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/core-concepts/everything-is-a-task.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/core-concepts/everything-is-a-task.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/core-concepts/everything-is-a-task.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,7 @@\n+---\n+title: Everything is a task\n+---\n+\n+## Contributions Welcome\n+\n+This part of the documentation does not yet exist. You can help create it on github, see the link below."}, {"sha": "7beabe76b7c257b133ecf7ca339e104a9021fa90", "filename": "docs/core-concepts/no-scheduler.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/core-concepts/no-scheduler.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/core-concepts/no-scheduler.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/core-concepts/no-scheduler.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,7 @@\n+---\n+title: No scheduler\n+---\n+\n+## Contributions Welcome\n+\n+This part of the documentation does not yet exist. You can help create it on github, see the link below."}, {"sha": "af565b54d34b70941cd9a1ff548e2c01e338d9f1", "filename": "docs/core-concepts/overview.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/core-concepts/overview.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/core-concepts/overview.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/core-concepts/overview.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,7 @@\n+---\n+title: Overview\n+---\n+\n+## Contributions Welcome\n+\n+This part of the documentation does not yet exist. You can help create it on github, see the link below."}, {"sha": "de0615cf3ad3131d42e475f5780d0144028afda4", "filename": "docs/core-concepts/task-hierarchy.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/core-concepts/task-hierarchy.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/core-concepts/task-hierarchy.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/core-concepts/task-hierarchy.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,7 @@\n+---\n+title: Task hierarchy\n+---\n+\n+## Contributions Welcome\n+\n+This part of the documentation does not yet exist. You can help create it on github, see the link below."}, {"sha": "178cc49c287d6970c8b005c83e416b2bd2fed919", "filename": "docs/extras/dashboard.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/extras/dashboard.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/extras/dashboard.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/extras/dashboard.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,7 @@\n+---\n+title: Dashboard\n+---\n+\n+## Contributions Welcome\n+\n+This part of the documentation does not yet exist. You can help create it on github, see the link below."}, {"sha": "c4e478500d6c2c5770d3290378645e158b64ffd9", "filename": "docs/extras/dask.md", "status": "added", "additions": 62, "deletions": 0, "changes": 62, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/extras/dask.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/extras/dask.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/extras/dask.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,62 @@\n+---\n+title: Dask\n+---\n+\n+Automatically deploy Dask clusters for your projects with a single line of code.\n+\n+**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n+\n+## Defining and running a Dask Cluster\n+\n+Dask clusters can be created using the `DaskCluster` task.\n+\n+```python:title=dask_cluster.py\n+from cowait.tasks import Task\n+from cowait.tasks.dask import DaskCluster\n+\n+class YourDaskJob(Task):\n+    async def run(self, dask, inputs**):\n+        cluster = DaskCluster(workers=5)\n+        client = await cluster.get_client()\n+        # dask client ready to use!\n+\n+        def square(x):\n+            return x ** 2\n+\n+        def neg(x):\n+            return -x\n+\n+        A = client.map(square, range(10))\n+        B = client.map(neg, A)\n+\n+        total = client.submit(sum, B)\n+        result = total.result()\n+\n+        print(result)\n+\n+        return result\n+```\n+\n+Run it:\n+\n+```shell\n+cowait run dask_cluster\n+```\n+\n+## DaskCluster RPC Methods\n+\n+The DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n+\n+### `cowait.tasks.dask.DaskCluster`\n+\n+| RPC Method            |                                  Description |\n+| --------------------- | -------------------------------------------: |\n+| `get_workers()`       |      Get informations about all Dask workers |\n+| `scale(workers: int)` | Can be used to scale up or down your cluster |\n+| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n+| `get_client()`        |                      Returns the dask client |\n+| `teardown()`          |     Stop your Dask cluster task from running |\n+\n+See [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n+\n+**WARNING**: This is very experimental and currently just a proof of concept."}, {"sha": "9d6c5479cca1f3fd596c67ab690f17b0926270cd", "filename": "docs/extras/databricks.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/extras/databricks.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/extras/databricks.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/extras/databricks.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,7 @@\n+---\n+title: Databricks\n+---\n+\n+## Contributions Welcome\n+\n+This part of the documentation does not yet exist. You can help create it on github, see the link below."}, {"sha": "829fc1cdb8f963532a0b1fd0bbcc8268c212d8fb", "filename": "docs/extras/notebook-integration.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/extras/notebook-integration.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/extras/notebook-integration.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/extras/notebook-integration.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,7 @@\n+---\n+title: Notebook Integration\n+---\n+\n+## Contributions Welcome\n+\n+This part of the documentation does not yet exist. You can help create it on github, see the link below."}, {"sha": "3163001fdbf44b507ad6a4a3f16ccab8b88809c3", "filename": "docs/extras/spark.md", "status": "added", "additions": 59, "deletions": 0, "changes": 59, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/extras/spark.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/extras/spark.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/extras/spark.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,59 @@\n+---\n+title: Spark\n+---\n+\n+Automatically deploy Spark clusters for your projects with a single line of code.\n+\n+**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n+\n+## Defining and running a Spark Cluster\n+\n+This requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n+\n+Spark clusters can be created using the `SparkCluster` task.\n+\n+```python:title=spark_cluster.py\n+from cowait.tasks import Task\n+from cowait.tasks.spark import SparkCluster\n+from pyspark.sql import SparkSession\n+\n+class YourSparkJob(Task):\n+    async def run(self, inputs**):\n+        cluster = SparkCluster(workers=5)\n+        conf = await cluster.get_config()\n+\n+        # create spark session\n+        session = SparkSession.builder \\\n+            .config(conf=conf) \\\n+            .getOrCreate()\n+\n+        # use your Spark SQL session!\n+\n+        # you can also scale the cluster at will:\n+        await cluster.scale(workers=2)\n+\n+        return \"Spark job exited\"\n+```\n+\n+Run it:\n+\n+```shell\n+cowait run Spark_cluster\n+```\n+\n+## SparkCluster RPC Methods\n+\n+The SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n+\n+### `cowait.tasks.Spark.SparkCluster`\n+\n+| RPC Method            |                                  Description |\n+| --------------------- | -------------------------------------------: |\n+| `get_workers()`       |     Get informations about all Spark workers |\n+| `scale(workers: int)` | Can be used to scale up or down your cluster |\n+| `get_config()`        |              Returns the Spark configuration |\n+| `teardown()`          |    Stop your Spark cluster task from running |\n+\n+See [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n+\n+**WARNING**: This is very experimental and currently just a proof of concept."}, {"sha": "2744b7de698aaf705cf1f2942ff2b45aa31a9a82", "filename": "docs/get-started/asyncio.md", "status": "added", "additions": 65, "deletions": 0, "changes": 65, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/asyncio.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/asyncio.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/get-started/asyncio.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,65 @@\n+---\n+title: Asyncio\n+---\n+\n+`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n+\n+## Asyncio & Cowait\n+\n+Cowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n+\n+Remember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n+\n+Before we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n+\n+```python:title=sleep.py\n+import asyncio\n+from cowait import task\n+\n+@task\n+async def Sleep(duration: int = 5):\n+    for i in range(duration):\n+      await asyncio.sleep(1) # blocking\n+      print(\"slept\", i + 1)\n+\n+    return {\n+        \"duration\": duration,\n+    }\n+```\n+\n+Cowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n+\n+```shell\n+cowait run sleep\n+```\n+\n+## Parallel Tasks\n+\n+One of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n+\n+```python:title=parallel.py\n+import asyncio\n+from cowait import task, join\n+from sleep import Sleep\n+\n+@task\n+async def Parallel():\n+    tasks = [Sleep(duration=5), Sleep(duration=5)]\n+\n+    result = await join(tasks)\n+\n+    return result\n+\n+```\n+\n+Here, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n+\n+```shell\n+cowait run parallel\n+```\n+\n+The `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n+\n+![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n+\n+You will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks."}, {"sha": "bce4a0276f5b51cfd6ab3d0f7ba9deebd5310af2", "filename": "docs/get-started/building-and-pushing.md", "status": "added", "additions": 50, "deletions": 0, "changes": 50, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/building-and-pushing.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/building-and-pushing.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/get-started/building-and-pushing.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,50 @@\n+---\n+title: Building and pushing\n+---\n+\n+## Overview\n+\n+Cowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n+\n+Cowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n+\n+## Task Context\n+\n+A task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n+\n+Example:\n+\n+```\n+/my_project\n+  \u2514\u2500\u2500 cowait.yml\n+  \u2514\u2500\u2500 hello.py\n+  \u2514\u2500\u2500 parallel.py\n+  \u2514\u2500\u2500 sleep.py\n+```\n+\n+In this case, `my_project` will be the context directory.\n+\n+## Cowait.yml\n+\n+In a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n+\n+To do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n+\n+```yml\n+version: 1\n+cowait:\n+  image: docker.io/username/cowait-task\n+```\n+\n+Now, if you run\n+\n+```shell\n+cowait build\n+cowait push\n+```\n+\n+Cowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n+\n+```shell\n+cowait build --push\n+```"}, {"sha": "9541bbce9f6cdc9f74ff9d80efe1b0e17901aaa5", "filename": "docs/get-started/dashboard.md", "status": "added", "additions": 48, "deletions": 0, "changes": 48, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/dashboard.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/dashboard.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/get-started/dashboard.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,48 @@\n+---\n+title: Dashboard\n+---\n+\n+## Cowait Agent\n+\n+The Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n+\n+The Agent is actually a Task in itself(!), and runs in a docker container.\n+\n+You can start it with a simple CLI command:\n+\n+```shell\n+cowait agent\n+```\n+\n+You should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n+\n+## Looking at tasks and logs in the Dashboard\n+\n+If you followed along the previous steps, you should have a project structure like this:\n+\n+```\n+my-project/\n+  \u251c\u2500\u2500 sleep.py\n+  \u251c\u2500\u2500 hello.py\n+  \u2514\u2500\u2500 parallel.py\n+```\n+\n+If you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n+\n+```shell\n+cowait run hello\n+```\n+\n+![Dashboard 1](../images/dashboard_overview.png)\n+\n+The Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n+\n+![Dashboard 2](../images/dashboard_hello_task.png)\n+\n+## Parallel\n+\n+Try running your `parallel` task and explore the task hierarchy in the UI!\n+\n+```shell\n+cowait run parallel\n+```"}, {"sha": "aee89eaf095426580dd199498ee07da36a7e7997", "filename": "docs/get-started/dependencies.md", "status": "added", "additions": 32, "deletions": 0, "changes": 32, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/dependencies.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/dependencies.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/get-started/dependencies.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,32 @@\n+---\n+title: Dependency management\n+---\n+\n+## Adding dependencies\n+\n+Cowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n+\n+1. Add a `requirements.txt` to the root of your project\n+\n+```\n+my-project/\n+  \u251c\u2500\u2500 cowait.yml\n+  \u251c\u2500\u2500 hello.py\n+  \u251c\u2500\u2500 parallel.py\n+  \u251c\u2500\u2500 requirements.txt\n+  \u2514\u2500\u2500 sleep.py\n+```\n+\n+2. Populate it\n+\n+```\n+pandas==1.2.4\n+```\n+\n+3. Build\n+\n+```shell\n+cowait build\n+```\n+\n+Cowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`."}, {"sha": "0144843ea15e62a0f58c015b68ca8ba8df8791a4", "filename": "docs/get-started/first-steps.md", "status": "added", "additions": 104, "deletions": 0, "changes": 104, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/first-steps.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/first-steps.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/get-started/first-steps.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,104 @@\n+---\n+title: First steps\n+---\n+\n+Your first steps into the world of Cowait.\n+\n+## Overview\n+\n+Cowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n+\n+## Tasks\n+\n+The basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n+\n+### Creating a task\n+\n+Create a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n+\n+```\n+my-project/\n+  \u2514\u2500\u2500 hello.py\n+```\n+\n+```python:title=hello.py\n+from cowait import task\n+\n+# function style\n+@task\n+async def Hello():\n+    print('Hello World')\n+```\n+\n+```python\n+from cowait import Task\n+\n+# class style\n+class Hello(Task):\n+    async def run(self):\n+        print('Hello World')\n+```\n+\n+### Running the task\n+\n+You can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n+\n+```shell\n+cd my-project\n+cowait run hello\n+```\n+\n+You should see something like this:\n+\n+```\n+-- TASK ---------------------------------------------\n+   task:       \"hello-plapdnoy\"\n+   cluster:    \"docker\" {  }\n+   image:      \"cowait/task\"\n+   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n+-- TASK OUTPUT --------------------------------------\n+15:53:28 hello * started with {  }\n+15:53:28 hello = returned null\n+15:53:28 hello   Hello World\n+-----------------------------------------------------\n+```\n+\n+### Volume Mounts\n+\n+Behind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n+\n+If you would like to build your Docker image with your added code, simply run:\n+\n+```shell\n+cowait build\n+```\n+\n+## Inputs & Outputs\n+\n+Cowait tasks can accept inputs and return outputs.\n+\n+```python:title=hello.py\n+import asyncio\n+from cowait import task\n+\n+@task\n+async def Hello(name: str, **inputs):\n+    print(\"Hello\", name)\n+\n+    return {\n+        \"hello\": name,\n+    }\n+```\n+\n+- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n+- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n+- The Cowait CLI allows you to pass inputs when running your task:\n+\n+```shell\n+cowait run hello --input name=world\n+```\n+\n+## Notes\n+\n+- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n+- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python."}, {"sha": "57ff3c059233a0d2e95a8b29584e9f5ee1817c50", "filename": "docs/get-started/installation.md", "status": "added", "additions": 57, "deletions": 0, "changes": 57, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/installation.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/installation.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/get-started/installation.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,57 @@\n+---\n+title: Installation\n+---\n+\n+Installing Cowait on your local machine.\n+\n+## Requirements\n+\n+Cowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n+\n+- Python 3.6+\n+- [Docker](https://docs.docker.com/get-docker/)\n+\n+## Installation\n+\n+Cowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n+\n+```shell\n+python -m pip install cowait\n+```\n+\n+We recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n+\n+To quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n+\n+```shell\n+docker pull cowait/task\n+```\n+\n+You are now ready for your [first steps](/docs/get-started/first-steps/).\n+\n+## Development\n+\n+If you would like to contribute to Cowait, you may install Cowait from source:\n+\n+1. Clone the repository\n+\n+```shell\n+git clone git@github.com:backtick-se/cowait.git\n+cd cowait\n+```\n+\n+2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n+\n+3. Install the library using pip's editable mode.\n+\n+```shell\n+python -m pip install -e .\n+```\n+\n+4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n+\n+```shell\n+./build.sh\n+```\n+\n+5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect."}, {"sha": "efcd44aee27135217e5aa358d869c65754b90f1d", "filename": "docs/get-started/next-steps.md", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/next-steps.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/next-steps.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/get-started/next-steps.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,16 @@\n+---\n+title: Next steps\n+---\n+\n+## Where to next?\n+\n+You should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n+\n+Some logical next steps after that:\n+\n+1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n+2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n+3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n+4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n+5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n+6. Look at some [built in tasks](/docs/tasks/built-in-tasks/)."}, {"sha": "3b017f0f6f8ae87d6bbae07f01031675185f44a0", "filename": "docs/get-started/tests.md", "status": "added", "additions": 147, "deletions": 0, "changes": 147, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/tests.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/get-started/tests.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/get-started/tests.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,147 @@\n+---\n+title: Tests\n+---\n+\n+## Overview\n+\n+Cowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n+\n+Writing tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n+\n+Good to know:\n+\n+- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n+- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n+- Cowait will create real instances of your tasks if you create them from your test code.\n+- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n+\n+## Black box task testing\n+\n+In this example, we have added a `test_sleep.py` file to our project:\n+\n+```\n+my-project/\n+  \u251c\u2500\u2500 cowait.yml\n+  \u251c\u2500\u2500 hello.py\n+  \u251c\u2500\u2500 parallel.py\n+  \u251c\u2500\u2500 requirements.txt\n+  \u251c\u2500\u2500 sleep.py\n+  \u2514\u2500\u2500 test_sleep.py\n+```\n+\n+```python:title=test_sleep.py\n+from sleep import Sleep\n+\n+async def test_sleep():\n+    result = await Sleep(duration=1)\n+\n+    assert result == {\n+        'duration': 1,\n+    }\n+```\n+\n+To run the test, use the Cowait CLI:\n+\n+```shell\n+cowait test\n+```\n+\n+## Testing functions and library code\n+\n+Of course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n+\n+```python:title=sleep.py\n+import asyncio\n+from cowait import task\n+\n+def add(a: int, b: int):\n+  return a + b\n+\n+@task\n+async def Sleep(duration: int = 5):\n+    for i in range(duration):\n+      await asyncio.sleep(1) # blocking\n+      print(\"slept\", i + 1)\n+\n+    return {\n+        \"duration\": duration,\n+    }\n+```\n+\n+You could simply import it in your test file and perform assertions like you normally would.\n+\n+```python:title=test_sleep.py\n+from sleep import Sleep, add\n+\n+def test_add():\n+    assert add(1, 2) == 3\n+\n+async def test_sleep():\n+    result = await Sleep(duration=1)\n+    assert result == {\n+        'duration': 1,\n+    }\n+```\n+\n+```bash\n+============================= test session starts ==============================\n+platform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\n+rootdir: /var/task, configfile: ../cowait/pytest.ini\n+plugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\n+collected 2 items\n+\n+test_sleep.py ..                                                         [100%]\n+\n+============================== 2 passed in 4.19s ===============================\n+```\n+\n+Moreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n+\n+## Testing reads and write of datasets\n+\n+In this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n+\n+Let's say your preprocessing task looks something like this:\n+\n+```python:title=preprocess.py\n+from cowait import task\n+\n+@task\n+async def Preprocess(dataset_url='s3://big-data-set'):\n+    #\n+    # data reading and data transformation code\n+    # ...\n+\n+    return {\n+      # New output location. We use self.task.id to\n+      # generate a unique identifier for this dataset.\n+      new_location: f's3://preprocessed/{self.task.id}'\n+    }\n+```\n+\n+Before investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n+\n+```python:title=test_preprocess.py\n+from preprocess import Preprocess\n+\n+async def test_preprocess():\n+    # define the task so we can grab the task id.\n+    # The task will start executing in the background\n+    task = Preprocess(dataset_url='s3://small-data-set')\n+\n+    # Wait for the task to finish\n+    result = await task\n+\n+    assert result == {\n+      new_location: f's3://preprocesssed/{task.id}'\n+    }\n+    # ...\n+    # further assertions like data written, rows, size, columns or whatever\n+    # ...\n+```\n+\n+```shell\n+cowait test\n+```\n+\n+Of course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/)."}, {"sha": "a593b82fad61825040e9f545c70f97b115434689", "filename": "docs/images/dashboard_hello_task.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/images/dashboard_hello_task.png", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/images/dashboard_hello_task.png", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/images/dashboard_hello_task.png?ref=cb3bb128bc29286848b861b907037896f6c0e3ad"}, {"sha": "0fa1448641e4dcc4731ce06cfc5408464e3f0d93", "filename": "docs/images/dashboard_overview.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/images/dashboard_overview.png", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/images/dashboard_overview.png", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/images/dashboard_overview.png?ref=cb3bb128bc29286848b861b907037896f6c0e3ad"}, {"sha": "bb34c2cdb96518cfe03fa6d40395d001d23d423b", "filename": "docs/images/parallel_tasks_docker.svg", "status": "added", "additions": 78, "deletions": 0, "changes": 78, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/images/parallel_tasks_docker.svg", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/images/parallel_tasks_docker.svg", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/images/parallel_tasks_docker.svg?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,78 @@\n+<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"423\" height=\"223\" viewBox=\"0 0 423 223\">\n+  <defs>\n+    <clipPath id=\"clip-Web_1920_1\">\n+      <rect width=\"423\" height=\"223\"/>\n+    </clipPath>\n+  </defs>\n+  <g id=\"Web_1920_1\" data-name=\"Web 1920 \u2013 1\" clip-path=\"url(#clip-Web_1920_1)\">\n+    <rect width=\"423\" height=\"223\" fill=\"#fff\"/>\n+    <line id=\"Line_1\" data-name=\"Line 1\" x2=\"360.766\" transform=\"translate(46.038 184.461)\" fill=\"none\" stroke=\"#364548\" stroke-width=\"1\"/>\n+    <g id=\"docker\" transform=\"translate(3.807 155.851)\">\n+      <path id=\"Path_1\" data-name=\"Path 1\" d=\"M68.367,14.662h7.3v7.494H79.36a15.951,15.951,0,0,0,5.073-.854A12.965,12.965,0,0,0,86.9,20.184a9.212,9.212,0,0,1-1.711-4.739c-.211-2.288.249-5.265,1.791-7.055l.768-.891.915.738a11.68,11.68,0,0,1,4.58,7.412,11.119,11.119,0,0,1,8.472.791l1,.581-.528,1.034c-2.067,4.05-6.388,5.3-10.613,5.083C85.254,38.947,71.491,46.431,54.8,46.431c-8.621,0-16.531-3.236-21.035-10.916L33.7,35.39,33.04,34.05a21.341,21.341,0,0,1-1.685-10.785l.1-1.108H37.7V14.662H45V7.331h14.6V0h8.762V14.662\" transform=\"translate(-20.94)\" fill=\"#364548\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_2\" data-name=\"Path 2\" d=\"M94.535,37.147c.489-3.932-2.355-7.02-4.118-8.486-2.032,2.431-2.348,8.8.84,11.485-1.779,1.635-5.528,3.117-9.367,3.117H35.161A20.991,20.991,0,0,0,37.1,54.5L37.63,55.5a18.716,18.716,0,0,0,1.1,1.71q2.882.192,5.313.135h0A25.242,25.242,0,0,0,51.8,56.18a.558.558,0,0,1,.7.364.587.587,0,0,1-.33.733c-.262.094-.535.182-.817.266h0a23.631,23.631,0,0,1-5.359.9c.128,0-.133.02-.133.02-.073,0-.165.016-.238.02-.844.049-1.756.059-2.687.059-1.019,0-2.023-.02-3.145-.079l-.029.02c3.893,4.528,9.981,7.244,17.613,7.244,16.15,0,29.849-7.408,35.915-24.039,4.3.457,8.44-.679,10.321-4.479a9.848,9.848,0,0,0-9.07-.065\" transform=\"translate(-23.494 -20.557)\" fill=\"#22a0c8\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_3\" data-name=\"Path 3\" d=\"M102.6,37.147c.506-3.932-2.436-7.02-4.261-8.486-2.1,2.431-2.429,8.8.869,11.485-1.841,1.635-5.72,3.117-9.693,3.117H44.025c-.2,6.35,2.159,11.171,6.329,14.085h0a26.954,26.954,0,0,0,8.022-1.168.581.581,0,0,1,.38,1.1c-.271.094-.554.182-.845.266h0a26.522,26.522,0,0,1-5.693.943l-.054-.051c5.687,2.917,13.934,2.907,23.389-.725A65.732,65.732,0,0,0,102.9,37q-.155.07-.3.144\" transform=\"translate(-31.873 -20.557)\" fill=\"#37b1d9\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_4\" data-name=\"Path 4\" d=\"M35.645,88.186a19.327,19.327,0,0,0,1.914,6.2l.55,1.006a18.638,18.638,0,0,0,1.14,1.71q2.982.192,5.5.135a26.954,26.954,0,0,0,8.022-1.168.581.581,0,0,1,.38,1.1c-.271.094-.554.182-.845.266h0a26.532,26.532,0,0,1-5.683.924c-.076,0-.209.006-.286.01-.873.049-1.807.079-2.771.079-1.054,0-2.133-.02-3.293-.079,4.029,4.528,10.367,7.254,18.264,7.254,14.307,0,26.754-5.43,33.978-17.429H35.645\" transform=\"translate(-23.884 -60.441)\" fill=\"#1b81a5\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_5\" data-name=\"Path 5\" d=\"M45.367,88.186a14.52,14.52,0,0,0,5.894,9.047,26.954,26.954,0,0,0,8.022-1.168.581.581,0,0,1,.38,1.1c-.271.094-.554.182-.845.266h0a26.929,26.929,0,0,1-5.723.924c5.687,2.917,13.91,2.875,23.364-.757a61.75,61.75,0,0,0,16.162-9.409H45.367\" transform=\"translate(-31.364 -60.441)\" fill=\"#1d91b4\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_6\" data-name=\"Path 6\" d=\"M55.26,34.222H61.8v6.539H55.26Zm.545.545h.516v5.449H55.8Zm.97,0h.537v5.449h-.537V34.767Zm.991,0H58.3v5.449h-.537Zm.991,0h.536v5.449h-.536Zm.991,0h.536v5.449h-.536Zm.991,0h.516v5.449h-.516Zm2.067-8.09h6.539v6.539H62.8V26.677Zm.545.545h.516v5.449H63.35Zm.97,0h.537v5.449H64.32V27.222Zm.991,0h.537v5.449H65.31Zm.991,0h.537v5.449H66.3Zm.991,0h.537v5.449h-.537Zm.991,0H68.8v5.449h-.517Z\" transform=\"translate(-37.647 -18.228)\" fill=\"#23a3c2\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_7\" data-name=\"Path 7\" d=\"M78.125,49.543h6.539v6.539h-6.54V49.543Zm.545.545h.516v5.449H78.67Zm.97,0h.537v5.449h-.536V50.088Zm.991,0h.536v5.449h-.536Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.517v5.449H83.6Z\" transform=\"translate(-53.138 -33.732)\" fill=\"#34bbde\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_8\" data-name=\"Path 8\" d=\"M100.993,49.543h6.539v6.539h-6.539Zm.545.545h.516v5.449h-.516Zm.97,0h.537v5.449h-.537Zm.991,0h.536v5.449H103.5Zm.991,0h.536v5.449h-.536Zm.991,0h.537v5.449h-.537Zm.991,0h.516v5.449h-.516Z\" transform=\"translate(-68.693 -33.732)\" fill=\"#23a3c2\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_9\" data-name=\"Path 9\" d=\"M100.993,26.677h6.539v6.539h-6.539Zm.545.545h.516v5.449h-.516Zm.97,0h.537v5.449h-.537Zm.991,0h.536v5.449H103.5Zm.991,0h.536v5.449h-.536Zm.991,0h.537v5.449h-.537Zm.991,0h.516v5.449h-.516Zm2.067,7h6.539v6.539h-6.539Zm.545.545h.516v5.449h-.516Zm.97,0h.536v5.449h-.536Zm.99,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.516v5.449h-.516Z\" transform=\"translate(-68.803 -18.228)\" fill=\"#34bbde\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_10\" data-name=\"Path 10\" d=\"M123.859,26.677H130.4v6.539h-6.539Zm.545.545h.516v5.449H124.4Zm.97,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.516v5.449h-.516Z\" transform=\"translate(-84.245 -18.163)\" fill=\"#23a3c2\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_11\" data-name=\"Path 11\" d=\"M123.859,3.81H130.4v6.54h-6.539Zm.545.545h.516V9.8H124.4V4.354Zm.97,0h.537V9.8h-.537V4.354Zm.991,0h.537V9.8h-.537V4.354Zm.991,0h.537V9.8h-.537V4.354Zm.991,0h.537V9.8h-.537V4.354Zm.991,0h.516V9.8h-.516V4.354Z\" transform=\"translate(-84.245 -2.553)\" fill=\"#34bbde\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_12\" data-name=\"Path 12\" d=\"M146.725,49.543h6.539v6.539h-6.539Zm.544.545h.516v5.449h-.516V50.088Zm.97,0h.537v5.449h-.536V50.088Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.536v5.449h-.536Zm.99,0h.516v5.449H152.2Z\" transform=\"translate(-99.798 -33.732)\" fill=\"#23a3c2\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_13\" data-name=\"Path 13\" d=\"M93.04,101.492a1.8,1.8,0,1,1-1.8,1.8,1.8,1.8,0,0,1,1.8-1.8\" transform=\"translate(-62.022 -69.029)\" fill=\"#d3ecec\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_14\" data-name=\"Path 14\" d=\"M31,87.336a1.219,1.219,0,0,1,.454.087.534.534,0,0,0-.238.592.515.515,0,0,0,.493.391.51.51,0,0,0,.449-.274,1.317,1.317,0,0,1-.362,1.493,1.227,1.227,0,0,1-1.494.076,1.311,1.311,0,0,1-.5-1.449,1.26,1.26,0,0,1,1.2-.917M0,83.086H81.534c-1.775-.463-5.617-1.09-4.983-3.484-3.228,3.845-11.013,2.7-12.978.8-2.188,3.266-14.925,2.024-15.813-.52-2.743,3.313-11.242,3.313-13.985,0-.889,2.544-13.626,3.786-15.814.52C16,82.3,8.212,83.447,4.984,79.6,5.617,82,1.776,82.623,0,83.087\" transform=\"translate(0 -54.285)\" fill=\"#364548\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_15\" data-name=\"Path 15\" d=\"M69.561,124.344a15.89,15.89,0,0,1-8.284-8.148,27.673,27.673,0,0,1-5.972.906q-1.31.073-2.755.074-1.667,0-3.512-.1c4.1,4.095,9.137,7.247,18.47,7.3q1.033,0,2.053-.039\" transform=\"translate(-33.463 -79.209)\" fill=\"#bdd9d7\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_16\" data-name=\"Path 16\" d=\"M75.663,119.1a15.658,15.658,0,0,1-1.658-2.9,27.656,27.656,0,0,1-5.973.907,19.017,19.017,0,0,0,7.632,2\" transform=\"translate(-46.283 -79.208)\" fill=\"#d3ecec\" fill-rule=\"evenodd\"/>\n+    </g>\n+    <text id=\"sleep_5_\" data-name=\"sleep(5)\" transform=\"translate(246 145)\" fill=\"#414141\" font-size=\"9\" font-family=\"HelveticaNeue, Helvetica Neue\" letter-spacing=\"0.08em\"><tspan x=\"-21.186\" y=\"0\">SLEEP(5)</tspan></text>\n+    <line id=\"Line_2\" data-name=\"Line 2\" y2=\"150\" transform=\"translate(152 33)\" fill=\"none\" stroke=\"#364548\" stroke-width=\"1\"/>\n+    <line id=\"Line_4\" data-name=\"Line 4\" x1=\"189.447\" transform=\"translate(152.092 115.571)\" fill=\"none\" stroke=\"#364548\" stroke-width=\"1\"/>\n+    <text id=\"Sleep_5_2\" data-name=\"Sleep(5)\" transform=\"translate(246 64)\" fill=\"#414141\" font-size=\"9\" font-family=\"HelveticaNeue, Helvetica Neue\" letter-spacing=\"0.08em\"><tspan x=\"-21.186\" y=\"0\">SLEEP(5)</tspan></text>\n+    <path id=\"Polygon_2\" data-name=\"Polygon 2\" d=\"M2.719,0,5.439,5.439H0Z\" transform=\"translate(343.352 154.548) rotate(180)\" fill=\"#364548\"/>\n+    <path id=\"Polygon_3\" data-name=\"Polygon 3\" d=\"M2.719,0,5.439,5.439H0Z\" transform=\"translate(343.352 80.22) rotate(180)\" fill=\"#364548\"/>\n+    <line id=\"Line_8\" data-name=\"Line 8\" y2=\"150\" transform=\"translate(341 33)\" fill=\"none\" stroke=\"#364548\" stroke-width=\"1\"/>\n+    <g id=\"docker-2\" data-name=\"docker\" transform=\"translate(205.039 5.441)\">\n+      <path id=\"Path_1-2\" data-name=\"Path 1\" d=\"M68.367,14.662h7.3v7.494H79.36a15.951,15.951,0,0,0,5.073-.854A12.965,12.965,0,0,0,86.9,20.184a9.212,9.212,0,0,1-1.711-4.739c-.211-2.288.249-5.265,1.791-7.055l.768-.891.915.738a11.68,11.68,0,0,1,4.58,7.412,11.119,11.119,0,0,1,8.472.791l1,.581-.528,1.034c-2.067,4.05-6.388,5.3-10.613,5.083C85.254,38.947,71.491,46.431,54.8,46.431c-8.621,0-16.531-3.236-21.035-10.916L33.7,35.39,33.04,34.05a21.341,21.341,0,0,1-1.685-10.785l.1-1.108H37.7V14.662H45V7.331h14.6V0h8.762V14.662\" transform=\"translate(-20.94)\" fill=\"#364548\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_2-2\" data-name=\"Path 2\" d=\"M94.535,37.147c.489-3.932-2.355-7.02-4.118-8.486-2.032,2.431-2.348,8.8.84,11.485-1.779,1.635-5.528,3.117-9.367,3.117H35.161A20.991,20.991,0,0,0,37.1,54.5L37.63,55.5a18.716,18.716,0,0,0,1.1,1.71q2.882.192,5.313.135h0A25.242,25.242,0,0,0,51.8,56.18a.558.558,0,0,1,.7.364.587.587,0,0,1-.33.733c-.262.094-.535.182-.817.266h0a23.631,23.631,0,0,1-5.359.9c.128,0-.133.02-.133.02-.073,0-.165.016-.238.02-.844.049-1.756.059-2.687.059-1.019,0-2.023-.02-3.145-.079l-.029.02c3.893,4.528,9.981,7.244,17.613,7.244,16.15,0,29.849-7.408,35.915-24.039,4.3.457,8.44-.679,10.321-4.479a9.848,9.848,0,0,0-9.07-.065\" transform=\"translate(-23.494 -20.557)\" fill=\"#22a0c8\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_3-2\" data-name=\"Path 3\" d=\"M102.6,37.147c.506-3.932-2.436-7.02-4.261-8.486-2.1,2.431-2.429,8.8.869,11.485-1.841,1.635-5.72,3.117-9.693,3.117H44.025c-.2,6.35,2.159,11.171,6.329,14.085h0a26.954,26.954,0,0,0,8.022-1.168.581.581,0,0,1,.38,1.1c-.271.094-.554.182-.845.266h0a26.522,26.522,0,0,1-5.693.943l-.054-.051c5.687,2.917,13.934,2.907,23.389-.725A65.732,65.732,0,0,0,102.9,37q-.155.07-.3.144\" transform=\"translate(-31.873 -20.557)\" fill=\"#37b1d9\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_4-2\" data-name=\"Path 4\" d=\"M35.645,88.186a19.327,19.327,0,0,0,1.914,6.2l.55,1.006a18.638,18.638,0,0,0,1.14,1.71q2.982.192,5.5.135a26.954,26.954,0,0,0,8.022-1.168.581.581,0,0,1,.38,1.1c-.271.094-.554.182-.845.266h0a26.532,26.532,0,0,1-5.683.924c-.076,0-.209.006-.286.01-.873.049-1.807.079-2.771.079-1.054,0-2.133-.02-3.293-.079,4.029,4.528,10.367,7.254,18.264,7.254,14.307,0,26.754-5.43,33.978-17.429H35.645\" transform=\"translate(-23.884 -60.441)\" fill=\"#1b81a5\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_5-2\" data-name=\"Path 5\" d=\"M45.367,88.186a14.52,14.52,0,0,0,5.894,9.047,26.954,26.954,0,0,0,8.022-1.168.581.581,0,0,1,.38,1.1c-.271.094-.554.182-.845.266h0a26.929,26.929,0,0,1-5.723.924c5.687,2.917,13.91,2.875,23.364-.757a61.75,61.75,0,0,0,16.162-9.409H45.367\" transform=\"translate(-31.364 -60.441)\" fill=\"#1d91b4\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_6-2\" data-name=\"Path 6\" d=\"M55.26,34.222H61.8v6.539H55.26Zm.545.545h.516v5.449H55.8Zm.97,0h.537v5.449h-.537V34.767Zm.991,0H58.3v5.449h-.537Zm.991,0h.536v5.449h-.536Zm.991,0h.536v5.449h-.536Zm.991,0h.516v5.449h-.516Zm2.067-8.09h6.539v6.539H62.8V26.677Zm.545.545h.516v5.449H63.35Zm.97,0h.537v5.449H64.32V27.222Zm.991,0h.537v5.449H65.31Zm.991,0h.537v5.449H66.3Zm.991,0h.537v5.449h-.537Zm.991,0H68.8v5.449h-.517Z\" transform=\"translate(-37.647 -18.228)\" fill=\"#23a3c2\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_7-2\" data-name=\"Path 7\" d=\"M78.125,49.543h6.539v6.539h-6.54V49.543Zm.545.545h.516v5.449H78.67Zm.97,0h.537v5.449h-.536V50.088Zm.991,0h.536v5.449h-.536Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.517v5.449H83.6Z\" transform=\"translate(-53.138 -33.732)\" fill=\"#34bbde\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_8-2\" data-name=\"Path 8\" d=\"M100.993,49.543h6.539v6.539h-6.539Zm.545.545h.516v5.449h-.516Zm.97,0h.537v5.449h-.537Zm.991,0h.536v5.449H103.5Zm.991,0h.536v5.449h-.536Zm.991,0h.537v5.449h-.537Zm.991,0h.516v5.449h-.516Z\" transform=\"translate(-68.693 -33.732)\" fill=\"#23a3c2\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_9-2\" data-name=\"Path 9\" d=\"M100.993,26.677h6.539v6.539h-6.539Zm.545.545h.516v5.449h-.516Zm.97,0h.537v5.449h-.537Zm.991,0h.536v5.449H103.5Zm.991,0h.536v5.449h-.536Zm.991,0h.537v5.449h-.537Zm.991,0h.516v5.449h-.516Zm2.067,7h6.539v6.539h-6.539Zm.545.545h.516v5.449h-.516Zm.97,0h.536v5.449h-.536Zm.99,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.516v5.449h-.516Z\" transform=\"translate(-68.803 -18.228)\" fill=\"#34bbde\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_10-2\" data-name=\"Path 10\" d=\"M123.859,26.677H130.4v6.539h-6.539Zm.545.545h.516v5.449H124.4Zm.97,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.516v5.449h-.516Z\" transform=\"translate(-84.245 -18.163)\" fill=\"#23a3c2\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_11-2\" data-name=\"Path 11\" d=\"M123.859,3.81H130.4v6.54h-6.539Zm.545.545h.516V9.8H124.4V4.354Zm.97,0h.537V9.8h-.537V4.354Zm.991,0h.537V9.8h-.537V4.354Zm.991,0h.537V9.8h-.537V4.354Zm.991,0h.537V9.8h-.537V4.354Zm.991,0h.516V9.8h-.516V4.354Z\" transform=\"translate(-84.245 -2.553)\" fill=\"#34bbde\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_12-2\" data-name=\"Path 12\" d=\"M146.725,49.543h6.539v6.539h-6.539Zm.544.545h.516v5.449h-.516V50.088Zm.97,0h.537v5.449h-.536V50.088Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.536v5.449h-.536Zm.99,0h.516v5.449H152.2Z\" transform=\"translate(-99.798 -33.732)\" fill=\"#23a3c2\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_13-2\" data-name=\"Path 13\" d=\"M93.04,101.492a1.8,1.8,0,1,1-1.8,1.8,1.8,1.8,0,0,1,1.8-1.8\" transform=\"translate(-62.022 -69.029)\" fill=\"#d3ecec\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_14-2\" data-name=\"Path 14\" d=\"M31,87.336a1.219,1.219,0,0,1,.454.087.534.534,0,0,0-.238.592.515.515,0,0,0,.493.391.51.51,0,0,0,.449-.274,1.317,1.317,0,0,1-.362,1.493,1.227,1.227,0,0,1-1.494.076,1.311,1.311,0,0,1-.5-1.449,1.26,1.26,0,0,1,1.2-.917M0,83.086H81.534c-1.775-.463-5.617-1.09-4.983-3.484-3.228,3.845-11.013,2.7-12.978.8-2.188,3.266-14.925,2.024-15.813-.52-2.743,3.313-11.242,3.313-13.985,0-.889,2.544-13.626,3.786-15.814.52C16,82.3,8.212,83.447,4.984,79.6,5.617,82,1.776,82.623,0,83.087\" transform=\"translate(0 -54.285)\" fill=\"#364548\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_15-2\" data-name=\"Path 15\" d=\"M69.561,124.344a15.89,15.89,0,0,1-8.284-8.148,27.673,27.673,0,0,1-5.972.906q-1.31.073-2.755.074-1.667,0-3.512-.1c4.1,4.095,9.137,7.247,18.47,7.3q1.033,0,2.053-.039\" transform=\"translate(-33.463 -79.209)\" fill=\"#bdd9d7\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_16-2\" data-name=\"Path 16\" d=\"M75.663,119.1a15.658,15.658,0,0,1-1.658-2.9,27.656,27.656,0,0,1-5.973.907,19.017,19.017,0,0,0,7.632,2\" transform=\"translate(-46.283 -79.208)\" fill=\"#d3ecec\" fill-rule=\"evenodd\"/>\n+    </g>\n+    <g id=\"docker-3\" data-name=\"docker\" transform=\"translate(205.039 87.202)\">\n+      <path id=\"Path_1-3\" data-name=\"Path 1\" d=\"M68.367,14.662h7.3v7.494H79.36a15.951,15.951,0,0,0,5.073-.854A12.965,12.965,0,0,0,86.9,20.184a9.212,9.212,0,0,1-1.711-4.739c-.211-2.288.249-5.265,1.791-7.055l.768-.891.915.738a11.68,11.68,0,0,1,4.58,7.412,11.119,11.119,0,0,1,8.472.791l1,.581-.528,1.034c-2.067,4.05-6.388,5.3-10.613,5.083C85.254,38.947,71.491,46.431,54.8,46.431c-8.621,0-16.531-3.236-21.035-10.916L33.7,35.39,33.04,34.05a21.341,21.341,0,0,1-1.685-10.785l.1-1.108H37.7V14.662H45V7.331h14.6V0h8.762V14.662\" transform=\"translate(-20.94)\" fill=\"#364548\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_2-3\" data-name=\"Path 2\" d=\"M94.535,37.147c.489-3.932-2.355-7.02-4.118-8.486-2.032,2.431-2.348,8.8.84,11.485-1.779,1.635-5.528,3.117-9.367,3.117H35.161A20.991,20.991,0,0,0,37.1,54.5L37.63,55.5a18.716,18.716,0,0,0,1.1,1.71q2.882.192,5.313.135h0A25.242,25.242,0,0,0,51.8,56.18a.558.558,0,0,1,.7.364.587.587,0,0,1-.33.733c-.262.094-.535.182-.817.266h0a23.631,23.631,0,0,1-5.359.9c.128,0-.133.02-.133.02-.073,0-.165.016-.238.02-.844.049-1.756.059-2.687.059-1.019,0-2.023-.02-3.145-.079l-.029.02c3.893,4.528,9.981,7.244,17.613,7.244,16.15,0,29.849-7.408,35.915-24.039,4.3.457,8.44-.679,10.321-4.479a9.848,9.848,0,0,0-9.07-.065\" transform=\"translate(-23.494 -20.557)\" fill=\"#22a0c8\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_3-3\" data-name=\"Path 3\" d=\"M102.6,37.147c.506-3.932-2.436-7.02-4.261-8.486-2.1,2.431-2.429,8.8.869,11.485-1.841,1.635-5.72,3.117-9.693,3.117H44.025c-.2,6.35,2.159,11.171,6.329,14.085h0a26.954,26.954,0,0,0,8.022-1.168.581.581,0,0,1,.38,1.1c-.271.094-.554.182-.845.266h0a26.522,26.522,0,0,1-5.693.943l-.054-.051c5.687,2.917,13.934,2.907,23.389-.725A65.732,65.732,0,0,0,102.9,37q-.155.07-.3.144\" transform=\"translate(-31.873 -20.557)\" fill=\"#37b1d9\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_4-3\" data-name=\"Path 4\" d=\"M35.645,88.186a19.327,19.327,0,0,0,1.914,6.2l.55,1.006a18.638,18.638,0,0,0,1.14,1.71q2.982.192,5.5.135a26.954,26.954,0,0,0,8.022-1.168.581.581,0,0,1,.38,1.1c-.271.094-.554.182-.845.266h0a26.532,26.532,0,0,1-5.683.924c-.076,0-.209.006-.286.01-.873.049-1.807.079-2.771.079-1.054,0-2.133-.02-3.293-.079,4.029,4.528,10.367,7.254,18.264,7.254,14.307,0,26.754-5.43,33.978-17.429H35.645\" transform=\"translate(-23.884 -60.441)\" fill=\"#1b81a5\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_5-3\" data-name=\"Path 5\" d=\"M45.367,88.186a14.52,14.52,0,0,0,5.894,9.047,26.954,26.954,0,0,0,8.022-1.168.581.581,0,0,1,.38,1.1c-.271.094-.554.182-.845.266h0a26.929,26.929,0,0,1-5.723.924c5.687,2.917,13.91,2.875,23.364-.757a61.75,61.75,0,0,0,16.162-9.409H45.367\" transform=\"translate(-31.364 -60.441)\" fill=\"#1d91b4\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_6-3\" data-name=\"Path 6\" d=\"M55.26,34.222H61.8v6.539H55.26Zm.545.545h.516v5.449H55.8Zm.97,0h.537v5.449h-.537V34.767Zm.991,0H58.3v5.449h-.537Zm.991,0h.536v5.449h-.536Zm.991,0h.536v5.449h-.536Zm.991,0h.516v5.449h-.516Zm2.067-8.09h6.539v6.539H62.8V26.677Zm.545.545h.516v5.449H63.35Zm.97,0h.537v5.449H64.32V27.222Zm.991,0h.537v5.449H65.31Zm.991,0h.537v5.449H66.3Zm.991,0h.537v5.449h-.537Zm.991,0H68.8v5.449h-.517Z\" transform=\"translate(-37.647 -18.228)\" fill=\"#23a3c2\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_7-3\" data-name=\"Path 7\" d=\"M78.125,49.543h6.539v6.539h-6.54V49.543Zm.545.545h.516v5.449H78.67Zm.97,0h.537v5.449h-.536V50.088Zm.991,0h.536v5.449h-.536Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.517v5.449H83.6Z\" transform=\"translate(-53.138 -33.732)\" fill=\"#34bbde\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_8-3\" data-name=\"Path 8\" d=\"M100.993,49.543h6.539v6.539h-6.539Zm.545.545h.516v5.449h-.516Zm.97,0h.537v5.449h-.537Zm.991,0h.536v5.449H103.5Zm.991,0h.536v5.449h-.536Zm.991,0h.537v5.449h-.537Zm.991,0h.516v5.449h-.516Z\" transform=\"translate(-68.693 -33.732)\" fill=\"#23a3c2\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_9-3\" data-name=\"Path 9\" d=\"M100.993,26.677h6.539v6.539h-6.539Zm.545.545h.516v5.449h-.516Zm.97,0h.537v5.449h-.537Zm.991,0h.536v5.449H103.5Zm.991,0h.536v5.449h-.536Zm.991,0h.537v5.449h-.537Zm.991,0h.516v5.449h-.516Zm2.067,7h6.539v6.539h-6.539Zm.545.545h.516v5.449h-.516Zm.97,0h.536v5.449h-.536Zm.99,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.516v5.449h-.516Z\" transform=\"translate(-68.803 -18.228)\" fill=\"#34bbde\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_10-3\" data-name=\"Path 10\" d=\"M123.859,26.677H130.4v6.539h-6.539Zm.545.545h.516v5.449H124.4Zm.97,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.516v5.449h-.516Z\" transform=\"translate(-84.245 -18.163)\" fill=\"#23a3c2\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_11-3\" data-name=\"Path 11\" d=\"M123.859,3.81H130.4v6.54h-6.539Zm.545.545h.516V9.8H124.4V4.354Zm.97,0h.537V9.8h-.537V4.354Zm.991,0h.537V9.8h-.537V4.354Zm.991,0h.537V9.8h-.537V4.354Zm.991,0h.537V9.8h-.537V4.354Zm.991,0h.516V9.8h-.516V4.354Z\" transform=\"translate(-84.245 -2.553)\" fill=\"#34bbde\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_12-3\" data-name=\"Path 12\" d=\"M146.725,49.543h6.539v6.539h-6.539Zm.544.545h.516v5.449h-.516V50.088Zm.97,0h.537v5.449h-.536V50.088Zm.991,0h.537v5.449h-.537Zm.991,0h.537v5.449h-.537Zm.991,0h.536v5.449h-.536Zm.99,0h.516v5.449H152.2Z\" transform=\"translate(-99.798 -33.732)\" fill=\"#23a3c2\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_13-3\" data-name=\"Path 13\" d=\"M93.04,101.492a1.8,1.8,0,1,1-1.8,1.8,1.8,1.8,0,0,1,1.8-1.8\" transform=\"translate(-62.022 -69.029)\" fill=\"#d3ecec\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_14-3\" data-name=\"Path 14\" d=\"M31,87.336a1.219,1.219,0,0,1,.454.087.534.534,0,0,0-.238.592.515.515,0,0,0,.493.391.51.51,0,0,0,.449-.274,1.317,1.317,0,0,1-.362,1.493,1.227,1.227,0,0,1-1.494.076,1.311,1.311,0,0,1-.5-1.449,1.26,1.26,0,0,1,1.2-.917M0,83.086H81.534c-1.775-.463-5.617-1.09-4.983-3.484-3.228,3.845-11.013,2.7-12.978.8-2.188,3.266-14.925,2.024-15.813-.52-2.743,3.313-11.242,3.313-13.985,0-.889,2.544-13.626,3.786-15.814.52C16,82.3,8.212,83.447,4.984,79.6,5.617,82,1.776,82.623,0,83.087\" transform=\"translate(0 -54.285)\" fill=\"#364548\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_15-3\" data-name=\"Path 15\" d=\"M69.561,124.344a15.89,15.89,0,0,1-8.284-8.148,27.673,27.673,0,0,1-5.972.906q-1.31.073-2.755.074-1.667,0-3.512-.1c4.1,4.095,9.137,7.247,18.47,7.3q1.033,0,2.053-.039\" transform=\"translate(-33.463 -79.209)\" fill=\"#bdd9d7\" fill-rule=\"evenodd\"/>\n+      <path id=\"Path_16-3\" data-name=\"Path 16\" d=\"M75.663,119.1a15.658,15.658,0,0,1-1.658-2.9,27.656,27.656,0,0,1-5.973.907,19.017,19.017,0,0,0,7.632,2\" transform=\"translate(-46.283 -79.208)\" fill=\"#d3ecec\" fill-rule=\"evenodd\"/>\n+    </g>\n+    <path id=\"Polygon_5\" data-name=\"Polygon 5\" d=\"M3,0,6,4H0Z\" transform=\"translate(128 182) rotate(90)\" fill=\"#364548\"/>\n+    <path id=\"Polygon_6\" data-name=\"Polygon 6\" d=\"M2.719,0,5.439,5.439H0Z\" transform=\"translate(149.373 145.484)\" fill=\"#364548\"/>\n+    <path id=\"Polygon_7\" data-name=\"Polygon 7\" d=\"M2.719,0,5.439,5.439H0Z\" transform=\"translate(149.373 71.155)\" fill=\"#364548\"/>\n+    <line id=\"Line_7\" data-name=\"Line 7\" x1=\"189.447\" transform=\"translate(152.092 33.991)\" fill=\"none\" stroke=\"#364548\" stroke-width=\"1\"/>\n+    <path id=\"Polygon_4\" data-name=\"Polygon 4\" d=\"M3,0,6,6H0Z\" transform=\"translate(412 182) rotate(90)\" fill=\"#364548\"/>\n+    <text id=\"Parallel\" transform=\"translate(40 216)\" fill=\"#414141\" font-size=\"9\" font-family=\"HelveticaNeue, Helvetica Neue\" letter-spacing=\"0.08em\"><tspan x=\"-24.273\" y=\"0\">PARALLEL</tspan></text>\n+  </g>\n+</svg>"}, {"sha": "28caa0042030440f1b3821a21a6ffe62c10e64b5", "filename": "docs/kubernetes/cluster-management.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/kubernetes/cluster-management.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/kubernetes/cluster-management.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/kubernetes/cluster-management.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,7 @@\n+---\n+title: Cluster Management\n+---\n+\n+## Contributions Welcome\n+\n+This part of the documentation does not yet exist. You can help create it on github, see the link below."}, {"sha": "c0e2678632a7502f8d5384154a14602df898b605", "filename": "docs/kubernetes/pushing-and-running.md", "status": "added", "additions": 65, "deletions": 0, "changes": 65, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/kubernetes/pushing-and-running.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/kubernetes/pushing-and-running.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/kubernetes/pushing-and-running.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,65 @@\n+---\n+title: Pushing & running\n+---\n+\n+How to push tasks to your registry so that they can later be run on Kubernetes.\n+\n+## Pushing\n+\n+Before you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n+\n+```\n+my-project/\n+  \u251c\u2500\u2500 cowait.yml\n+  \u251c\u2500\u2500 hello.py\n+  \u251c\u2500\u2500 requirements.txt\n+  \u2514\u2500\u2500 sleep.py\n+```\n+\n+```yml:title=cowait.yml\n+version: 1\n+cowait:\n+  image: your-repo/task-image-name\n+```\n+\n+1. Build your tasks into your image\n+\n+```shell\n+cowait build\n+```\n+\n+2. Make sure you're authenticated to your registry.\n+\n+```shell\n+docker login\n+```\n+\n+3. Push the image\n+\n+This will push the image to registry you defined in your `cowait.yml`\n+\n+```shell\n+cowait push\n+```\n+\n+## Configuring Pull Secrets\n+\n+If your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n+\n+Once you have created a secret, configure the kubernetes provider to use it for pulling images:\n+\n+```yml:title=cowait.yml\n+version: 1\n+cowait:\n+  kubernetes:\n+    pull_secrets:\n+      - your_secret_name\n+```\n+\n+## Running\n+\n+You should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n+\n+```shell\n+cowait run your_task --provider kubernetes\n+```"}, {"sha": "097ab990fb27a3adbae969842f7aa17d7c88aeab", "filename": "docs/kubernetes/routing.md", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/kubernetes/routing.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/kubernetes/routing.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/kubernetes/routing.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,14 @@\n+---\n+title: Routing\n+---\n+\n+Automated HTTP proxying for tasks\n+\n+## Using Traefik\n+\n+Cowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n+\n+- Deploy Traefik to your cluster.\n+- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n+\n+Tasks with route mappings will be available at `task123.cluster.yourdomain.com`"}, {"sha": "677fc5df6d911a0b2383f619792c0216071f4bbb", "filename": "docs/kubernetes/setup.md", "status": "added", "additions": 48, "deletions": 0, "changes": 48, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/kubernetes/setup.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/kubernetes/setup.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/kubernetes/setup.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,48 @@\n+---\n+title: Setup\n+---\n+\n+## Permissions\n+\n+Task pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n+\n+### Basic\n+\n+The most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n+\n+```yml\n+kind: ClusterRole\n+apiVersion: rbac.authorization.k8s.io/v1\n+metadata:\n+  name: task-basic-permissions\n+rules:\n+  - apiGroups: [\"\"]\n+    resources: [\"pods\", \"pods/log\"]\n+    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n+---\n+kind: ClusterRoleBinding\n+apiVersion: rbac.authorization.k8s.io/v1\n+metadata:\n+  name: default-sa-task-permissions\n+subjects:\n+  - kind: ServiceAccount\n+    name: default\n+    namespace: default\n+roleRef:\n+  kind: ClusterRole\n+  name: task-basic-permissions\n+  apiGroup: rbac.authorization.k8s.io\n+```\n+\n+### Extended\n+\n+If you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\n+Apply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n+\n+```shell\n+kubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n+```\n+\n+## Repository Secrets\n+\n+If you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials."}, {"sha": "38618758a490194504c255efb5b89b0f8749c5cf", "filename": "docs/kubernetes/testing.md", "status": "added", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/kubernetes/testing.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/kubernetes/testing.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/kubernetes/testing.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,20 @@\n+---\n+title: Testing on Kubernetes\n+---\n+\n+## Prerequisites\n+\n+- Basic Kubernetes knowledge\n+- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n+- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n+- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n+\n+## Testing on Kubernetes\n+\n+To make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n+\n+```\n+cowait test --cluster my_kubernetes\n+```\n+\n+Further, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image."}, {"sha": "c3a06b4b037e2b03d6e9824c1f886a9bcadbad3f", "filename": "docs/quick-start.md", "status": "modified", "additions": 117, "deletions": 0, "changes": 117, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/quick-start.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/quick-start.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/quick-start.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -1,3 +1,120 @@\n ---\n title: Quick Start\n ---\n+\n+This quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n+\n+## Cowait quick start\n+\n+1. Install cowait\n+\n+```shell\n+pip install cowait\n+```\n+\n+2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n+\n+```shell\n+docker pull cowait/task\n+```\n+\n+3. Create a new Cowait task, `hello.py`:\n+\n+```python:title=hello.py\n+import asyncio\n+from cowait import task\n+\n+@task\n+async def Hello():\n+    print(\"Hello World\")\n+\n+```\n+\n+4. Run your Cowait task, this spins up a new docker container.\n+\n+```shell\n+cowait run hello\n+```\n+\n+5. Start the Cowait UI\n+\n+```shell\n+cowait agent\n+```\n+\n+You can visit the UI at `http://localhost:1339`\n+\n+6. If you run your task again, it should show up in the UI.\n+\n+## Asyncio, Inputs & Outputs\n+\n+1. Create a new file `sleep.py`.\n+\n+Cowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n+\n+```python:title=sleep.py\n+import asyncio\n+from cowait import task\n+\n+@task\n+async def Sleep():\n+    for i in range(5):\n+      await asyncio.sleep(1)\n+      print(\"slept\", i + 1)\n+\n+```\n+\n+2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n+\n+   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n+   - Outputs can be consumed by other tasks or systems.\n+\n+```python:title=sleep.py\n+import asyncio\n+from cowait import task\n+\n+@task\n+async def Sleep(duration: int = 5, **inputs):\n+    for i in range(duration):\n+        await asyncio.sleep(1)\n+        print(\"slept\", i + 1)\n+\n+    return {\n+        \"duration\": duration,\n+    }\n+```\n+\n+3. The Cowait CLI allows you to pass inputs when running your task:\n+\n+```shell\n+cowait run sleep --input duration=7\n+```\n+\n+## Parallel Tasks\n+\n+One of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n+\n+```python:title=parallel.py\n+import asyncio\n+from cowait import task, join\n+from sleep import Sleep\n+\n+@task\n+async def Parallel():\n+    tasks = [Sleep(duration=5), Sleep(duration=5)]\n+\n+    result = await join(tasks)\n+\n+    return result\n+\n+```\n+\n+```shell\n+cowait run parallel\n+```\n+\n+Nice! Here's an illustration of what you just ran, in terms of containers:\n+\n+![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n+\n+You will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks."}, {"sha": "c6cabd424e1dee19be39bbbd1318cadf9244b839", "filename": "docs/setup/configuration.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/setup/configuration.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/setup/configuration.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/setup/configuration.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,7 @@\n+---\n+title: Configuration\n+---\n+\n+## Contributions Welcome\n+\n+This part of the documentation does not yet exist. You can help create it on github, see the link below."}, {"sha": "5a0e0d9a736aad281198eebc4b56cb12536f92de", "filename": "docs/setup/custom-dockerfile.md", "status": "added", "additions": 39, "deletions": 0, "changes": 39, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/setup/custom-dockerfile.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/setup/custom-dockerfile.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/setup/custom-dockerfile.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,39 @@\n+---\n+title: Custom Dockerfile\n+---\n+\n+Details on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n+\n+## Overview\n+\n+To use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n+\n+Example file structure:\n+\n+```\n+hello-world\n+  \u2514\u2500\u2500 hello.py\n+  \u2514\u2500\u2500 Dockerfile\n+```\n+\n+## Extending the default Dockerfile\n+\n+The easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n+\n+```Dockerfile\n+# Dockerfile\n+FROM cowait/task\n+\n+RUN echo \"Your custom command\"\n+```\n+\n+## Writing your own Dockerfile\n+\n+If extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n+\n+You need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n+\n+- Ensure Python 3.6+ is installed\n+- Install Cowait with `pip`\n+- Create a directory call `/var/task` and set it as `WORKDIR`\n+- Set `python -Bum cowait.exec` as entrypoint"}, {"sha": "5836d77df9ffe32c653afbdd1f490821aeed9b84", "filename": "docs/tasks/built-in-tasks.md", "status": "added", "additions": 63, "deletions": 0, "changes": 63, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/tasks/built-in-tasks.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/tasks/built-in-tasks.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/tasks/built-in-tasks.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,63 @@\n+---\n+title: Built in tasks\n+---\n+\n+Some useful library tasks that can simplify your life.\n+\n+## ShellTask\n+\n+ShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n+\n+### `cowait.tasks.shell.ShellTask`\n+\n+| Input   |  Type  |              Description |\n+| ------- | :----: | -----------------------: |\n+| command | string | Shell command to execute |\n+| env     |  dict  |              Environment |\n+\n+**Returns**: shell command return code (integer)\n+\n+```python:title=example-ls.py\n+from cowait.tasks.shell import ShellTask\n+\n+@task\n+async def MyTask():\n+    await ShellTask(command='ls')\n+```\n+\n+```shell\n+# Example using the CLI\n+cowait run cowait.tasks.shell --input command=ls\n+```\n+\n+## ContainerTask\n+\n+`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n+\n+### `cowait.tasks.container.ContainerTask`\n+\n+| Input  |    Type    |           Description |\n+| ------ | :--------: | --------------------: |\n+| name   |   string   |             Task Name |\n+| image  |   string   |     Docker image name |\n+| env    |    dict    | Environment variables |\n+| routes | Route Dict |                       |\n+| ports  | Port Dict  |                       |\n+| cpu    |   string   |        CPU allocation |\n+| memory |   string   |     Memory allocation |\n+\n+```python:title=mongo.py\n+from cowait.tasks.container import ContainerTask\n+\n+@task\n+async def MyTask():\n+    await ContainerTask(\n+      name=\"mongodb-task\"\n+      image=\"mongo\"\n+    )\n+```\n+\n+```shell\n+# Example using the CLI\n+cowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n+```"}, {"sha": "2366fdc3e0381c12cba06617fcddce1644e2b68d", "filename": "docs/tasks/custom-dockerfile.md", "status": "added", "additions": 39, "deletions": 0, "changes": 39, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/tasks/custom-dockerfile.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/tasks/custom-dockerfile.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/tasks/custom-dockerfile.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,39 @@\n+---\n+title: Custom Dockerfile\n+---\n+\n+Details on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n+\n+## Overview\n+\n+To use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n+\n+Example file structure:\n+\n+```bash\n+hello-world\n+  \u2514\u2500\u2500 hello.py\n+  \u2514\u2500\u2500 Dockerfile\n+```\n+\n+## Extending the default Dockerfile\n+\n+The easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n+\n+```Dockerfile\n+# Dockerfile\n+FROM cowait/task\n+\n+RUN echo \"Your custom command\"\n+```\n+\n+## Writing your own Dockerfile\n+\n+If extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n+\n+You need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n+\n+- Ensure Python 3.6+ is installed\n+- Install Cowait with `pip`\n+- Create a directory call `/var/task` and set it as `WORKDIR`\n+- Set `python -Bum cowait.exec` as entrypoint"}, {"sha": "8aea6c17979abcf32fa78a7f7e83e552872655db", "filename": "docs/tasks/remote-procedure-calls.md", "status": "added", "additions": 80, "deletions": 0, "changes": 80, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/tasks/remote-procedure-calls.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/tasks/remote-procedure-calls.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/tasks/remote-procedure-calls.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,80 @@\n+---\n+title: Remote Procedure Calls (RPC)\n+---\n+\n+Advanced task communication\n+\n+## Introduction\n+\n+Cowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n+\n+RPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n+\n+## Parent to Child RPC\n+\n+The parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n+\n+1. Define an RPC method on your child task\n+\n+```python:title=rpc_child.py\n+from cowait.tasks Task, rpc, sleep\n+\n+class RpcChild(Task):\n+    async def run(self):\n+        # wait forever\n+        while True:\n+            await sleep(1)\n+\n+    @rpc\n+    async def some_rpc_call(self):\n+        return 1337\n+```\n+\n+2. Call it from the parent, after saving a reference to the child task.\n+\n+```python:title=rpc_parent.py\n+from cowait.tasks Task\n+from rpc_child import RpcChild # your child task\n+\n+class RpcParent(Task):\n+    async def run(self):\n+        child = RpcChild()\n+        result = await child.some_rpc_call()\n+        print('RPC result:', result)\n+        return result\n+```\n+\n+## Child to parent RPC\n+\n+Similarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n+\n+1. Have your parent task create the child task.\n+\n+```python:title=rpc_parent.py\n+from cowait.tasks import Task, rpc, sleep\n+from rpc_child import RpcChild\n+\n+class RpcParent(Task):\n+    async def run(self):\n+        self.called = False\n+\n+        # spawn child and wait for it to make an RPC call:\n+        child = RpcChild()\n+        while not self.called:\n+            await sleep(1)\n+\n+    @rpc\n+    async def set_called(self):\n+        self.called = True\n+```\n+\n+2. Call the parent's RPC method through `self.parent`:\n+\n+```python:title=rpc_child.py\n+from cowait.tasks import Task\n+\n+class RpcChild(Task):\n+    async def run(self):\n+        # rpc call to parent:\n+        await self.parent.set_called()\n+```"}, {"sha": "53c3c8a051594d216f7423779341a981347c4863", "filename": "docs/tasks/task-lifecycle-methods.md", "status": "added", "additions": 36, "deletions": 0, "changes": 36, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/tasks/task-lifecycle-methods.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/tasks/task-lifecycle-methods.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/tasks/task-lifecycle-methods.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,36 @@\n+---\n+title: Task Lifecycle Methods\n+---\n+\n+Tasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n+\n+Task lifecycle methods are added as class methods on tasks.\n+\n+## init\n+\n+Tasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n+\n+```python\n+def init(self) -> None:\n+    pass\n+```\n+\n+## before\n+\n+The `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n+\n+```python\n+# inputs can be modified before run() is executed:\n+async def before(self, inputs: dict) -> dict:\n+    inputs['new_input'] = 2\n+    return inputs\n+```\n+\n+## after\n+\n+The `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n+\n+```python\n+async def after(self, inputs: dict) -> None:\n+    return\n+```"}, {"sha": "13c90c13a993cafbd4102bb0e4eb027ee106d616", "filename": "docs/tasks/type-system.md", "status": "added", "additions": 59, "deletions": 0, "changes": 59, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/tasks/type-system.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/tasks/type-system.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/tasks/type-system.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,59 @@\n+---\n+title: Type system\n+---\n+\n+Customizable type checking & input/output serialization\n+\n+## Built in types\n+\n+Cowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n+\n+```python:title=example.py\n+from cowait import task\n+from cowait.types import Dict\n+\n+TypecheckedDict = Dict({\n+    'text': str,\n+    'number': int,\n+})\n+\n+@task\n+def test_task(input_dict: TypecheckedDict) -> int:\n+    print(input_dict['text'])\n+    return input_dict['number']\n+```\n+\n+### Input Values\n+\n+If you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n+\n+### Return Values\n+\n+Type information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n+\n+## Custom Types\n+\n+Custom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n+\n+```python:title=datetime_type.py\n+from cowait.types import Type, TypeAlias\n+\n+@TypeAlias(datetime)\n+class DateTime(Type):\n+    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n+\n+    def validate(self, value: str, name: str) -> None:\n+        if isinstance(value, datetime):\n+            return\n+\n+        if not isinstance(value, str):\n+            raise ValueError('Expected ISO8601 datetime')\n+\n+        datetime.fromisoformat(value)\n+\n+    def serialize(self, value: datetime) -> str:\n+        return value.isoformat()\n+\n+    def deserialize(self, value: str) -> datetime:\n+        return datetime.fromisoformat(value)\n+```"}, {"sha": "2801a74b72e7d457f329bce520c31aba2c5c262a", "filename": "docs/why-cowait.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/backtick-se/cowait/blob/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/why-cowait.md", "raw_url": "https://github.com/backtick-se/cowait/raw/cb3bb128bc29286848b861b907037896f6c0e3ad/docs/why-cowait.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/why-cowait.md?ref=cb3bb128bc29286848b861b907037896f6c0e3ad", "patch": "@@ -0,0 +1,7 @@\n+---\n+title: Why Cowait?\n+---\n+\n+## Contributions Welcome\n+\n+This part of the documentation does not yet exist. You can help create it on github, see the link below."}], "stats": {"total": 1382, "additions": 1382, "deletions": 0}}]}, {"docs": [["cowait/docs/quick-start.md", "---\ntitle: Quick Start\n---\n"]], "number": 294, "title": "Add example quickstart docs", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/294", "html_url": "https://github.com/backtick-se/cowait/pull/294", "diff_url": "https://github.com/backtick-se/cowait/pull/294.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/294.patch", "merged_at": "2021-03-29T14:22:09Z"}, "body": "- Removed docs from gitignore\r\n- Added test quick-start.md for external documentation building", "commits": [{"sha": "2a4847126bd797a62c6204c54964e08bd0f42876", "html_url": "https://github.com/backtick-se/cowait/commit/2a4847126bd797a62c6204c54964e08bd0f42876", "commit": {"author": {"name": "Oskar Handmark", "email": "oskarhandmark@gmail.com", "date": "2021-03-29T14:15:23Z"}, "committer": {"name": "Oskar Handmark", "email": "oskarhandmark@gmail.com", "date": "2021-03-29T14:15:23Z"}, "message": "remove docs from gitignore", "tree": {"sha": "d5a14d3b944218650b94fdf0ea94607fe280620c", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/d5a14d3b944218650b94fdf0ea94607fe280620c"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/2a4847126bd797a62c6204c54964e08bd0f42876", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "86c81e3edf0aac7f740260f76d3dddd410df0c4b", "filename": ".gitignore", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/backtick-se/cowait/blob/2a4847126bd797a62c6204c54964e08bd0f42876/.gitignore", "raw_url": "https://github.com/backtick-se/cowait/raw/2a4847126bd797a62c6204c54964e08bd0f42876/.gitignore", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/.gitignore?ref=2a4847126bd797a62c6204c54964e08bd0f42876", "patch": "@@ -1,5 +1,4 @@\n dist/\n-docs/\n build/\n cowait.egg-info/\n dask-worker-space/"}], "stats": {"total": 1, "additions": 0, "deletions": 1}}, {"sha": "c9ef10719e05c6f453fccb16a788a0d52022f2af", "html_url": "https://github.com/backtick-se/cowait/commit/c9ef10719e05c6f453fccb16a788a0d52022f2af", "commit": {"author": {"name": "Oskar Handmark", "email": "oskarhandmark@gmail.com", "date": "2021-03-29T14:15:35Z"}, "committer": {"name": "Oskar Handmark", "email": "oskarhandmark@gmail.com", "date": "2021-03-29T14:15:35Z"}, "message": "add sample quick-start markdown", "tree": {"sha": "39c7b448fbf7e8bb1bd7404a3ee36df6163f24ad", "url": "https://api.github.com/repos/backtick-se/cowait/git/trees/39c7b448fbf7e8bb1bd7404a3ee36df6163f24ad"}, "url": "https://api.github.com/repos/backtick-se/cowait/git/commits/c9ef10719e05c6f453fccb16a788a0d52022f2af", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "files": [{"sha": "f84ac1cb3bb9ac4e482c1dc724b4f79cc2923b93", "filename": "docs/quick-start.md", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/backtick-se/cowait/blob/c9ef10719e05c6f453fccb16a788a0d52022f2af/docs/quick-start.md", "raw_url": "https://github.com/backtick-se/cowait/raw/c9ef10719e05c6f453fccb16a788a0d52022f2af/docs/quick-start.md", "contents_url": "https://api.github.com/repos/backtick-se/cowait/contents/docs/quick-start.md?ref=c9ef10719e05c6f453fccb16a788a0d52022f2af", "patch": "@@ -0,0 +1,3 @@\n+---\n+title: Quick Start\n+---"}], "stats": {"total": 3, "additions": 3, "deletions": 0}}]}]