[{"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 350, "title": "add node selector field to task definition", "labels": [{"id": 1562464622, "node_id": "MDU6TGFiZWwxNTYyNDY0NjIy", "url": "https://api.github.com/repos/backtick-se/cowait/labels/feature", "name": "feature", "color": "7057ff", "default": false, "description": "New feature"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/350", "html_url": "https://github.com/backtick-se/cowait/pull/350", "diff_url": "https://github.com/backtick-se/cowait/pull/350.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/350.patch", "merged_at": "2022-03-11T13:12:36Z"}, "body": null}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 349, "title": "Update server.py", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/349", "html_url": "https://github.com/backtick-se/cowait/pull/349", "diff_url": "https://github.com/backtick-se/cowait/pull/349.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/349.patch", "merged_at": "2022-03-09T14:02:37Z"}, "body": "Fix bug when emitting SocketError"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 348, "title": "Fix build on M1 processors", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/348", "html_url": "https://github.com/backtick-se/cowait/pull/348", "diff_url": "https://github.com/backtick-se/cowait/pull/348.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/348.patch", "merged_at": "2022-03-09T14:03:33Z"}, "body": "This PR solves #347 using the third alternative mentioned in the issue"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 346, "title": "Bump url-parse from 1.5.7 to 1.5.10 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/346", "html_url": "https://github.com/backtick-se/cowait/pull/346", "diff_url": "https://github.com/backtick-se/cowait/pull/346.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/346.patch", "merged_at": "2022-03-09T14:03:07Z"}, "body": "Bumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.7 to 1.5.10.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/8cd4c6c6435c1ea32243ec20c9cfe535251ec524\"><code>8cd4c6c</code></a> 1.5.10</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/ce7a01f2e10738b17812f57c7b6b5de4ea4c0298\"><code>ce7a01f</code></a> [fix] Improve handling of empty port</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/00714900ea1e8ba0a1f87b9f8399001e47f060ec\"><code>0071490</code></a> [doc] Update JSDoc comment</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/a7044e3e8bb2308ac0f74264d01951aeaca0d66f\"><code>a7044e3</code></a> [minor] Use more descriptive variable name</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/d547792414a414b2f341a805141beafee728addf\"><code>d547792</code></a> [security] Add credits for CVE-2022-0691</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/ad23357ad5fd9a6b011d049466e9ecff723e52b8\"><code>ad23357</code></a> 1.5.9</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/0e3fb542d60ddbf6933f22eb9b1e06e25eaa5b63\"><code>0e3fb54</code></a> [fix] Strip all control characters from the beginning of the URL</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/61864a8eccff714a45d23db85a814e3c6ee0baba\"><code>61864a8</code></a> [security] Add credits for CVE-2022-0686</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/bb0104d6439cf7c2662afbd9411e0772a9639664\"><code>bb0104d</code></a> 1.5.8</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/d5c64791ef496ca5459ae7f2176a31ea53b127e5\"><code>d5c6479</code></a> [fix] Handle the case where the port is specified but empty</li>\n<li>Additional commits viewable in <a href=\"https://github.com/unshiftio/url-parse/compare/1.5.7...1.5.10\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=url-parse&package-manager=npm_and_yarn&previous-version=1.5.7&new-version=1.5.10)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 345, "title": "add simple test for kubernetes provider", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/345", "html_url": "https://github.com/backtick-se/cowait/pull/345", "diff_url": "https://github.com/backtick-se/cowait/pull/345.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/345.patch", "merged_at": "2022-02-26T12:28:48Z"}, "body": "This test fails if the `KubernetesProvider` cannot be instantiated. Such a problem has already happened, and is being addressed by #344."}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 343, "title": "Bump url-parse from 1.5.1 to 1.5.7 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/343", "html_url": "https://github.com/backtick-se/cowait/pull/343", "diff_url": "https://github.com/backtick-se/cowait/pull/343.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/343.patch", "merged_at": "2022-02-21T09:51:14Z"}, "body": "Bumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.1 to 1.5.7.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/8b3f5f2c88a4cfc2880f2319c307994cb25bb10a\"><code>8b3f5f2</code></a> 1.5.7</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/ef45a1355375a8244063793a19059b4f62fc8788\"><code>ef45a13</code></a> [fix] Readd the empty userinfo to <code>url.href</code> (<a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/226\">#226</a>)</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/88df2346855f70cec9713b362ca32a4691dc271a\"><code>88df234</code></a> [doc] Add soft deprecation notice</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/78e9f2f41285d83e7d91706be5bd439656fe3bc3\"><code>78e9f2f</code></a> [security] Fix nits</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/e6fa43422c52f34c73146552ec9916125dc59525\"><code>e6fa434</code></a> [security] Add credits for incorrect handling of userinfo vulnerability</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/4c9fa234c01dca52698666378360ad2fdfb05470\"><code>4c9fa23</code></a> 1.5.6</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/7b0b8a6671f806458e88b1f44feb0fdd742cdf06\"><code>7b0b8a6</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/223\">#223</a> from unshiftio/fix/at-sign-handling-in-userinfo</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/e4a5807d95b971577e4d888f5b99d64a40851386\"><code>e4a5807</code></a> 1.5.5</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/193b44baf3d203560735e05eedc99d8244c9e16c\"><code>193b44b</code></a> [minor] Simplify whitespace regex</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/319851bf1c294796fc73e29ff31b14d9084e4a0d\"><code>319851b</code></a> [fix] Remove CR, HT, and LF</li>\n<li>Additional commits viewable in <a href=\"https://github.com/unshiftio/url-parse/compare/1.5.1...1.5.7\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=url-parse&package-manager=npm_and_yarn&previous-version=1.5.1&new-version=1.5.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 342, "title": "Bump follow-redirects from 1.13.0 to 1.14.8 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/342", "html_url": "https://github.com/backtick-se/cowait/pull/342", "diff_url": "https://github.com/backtick-se/cowait/pull/342.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/342.patch", "merged_at": "2022-02-21T09:51:23Z"}, "body": "Bumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.13.0 to 1.14.8.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/3d81dc3237b4ffe8b722bb3d1c70a7866657166e\"><code>3d81dc3</code></a> Release version 1.14.8 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/62e546a99c07c3ee5e4e0718c84a6ca127c5c445\"><code>62e546a</code></a> Drop confidential headers across schemes.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/2ede36d7c60d3acdcd324dcd99a9dbd52e4fb3a6\"><code>2ede36d</code></a> Release version 1.14.7 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/8b347cbcef7c7b72a6e9be20f5710c17d6163c22\"><code>8b347cb</code></a> Drop Cookie header across domains.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/6f5029ae1a0fdab4dc25f6379a5ee303c2319070\"><code>6f5029a</code></a> Release version 1.14.6 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/af706bee57de954414c0bde0a9f33e62beea3e52\"><code>af706be</code></a> Ignore null headers.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/d01ab7a5c5df3617c7a40a03de7af6427fdfac55\"><code>d01ab7a</code></a> Release version 1.14.5 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/40052ea8aa13559becee5795715c1d45b1f0eb76\"><code>40052ea</code></a> Make compatible with Node 17.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/86f7572f9365dadc39f85916259b58973819617f\"><code>86f7572</code></a> Fix: clear internal timer on request abort to avoid leakage</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/2e1eaf0218c5315a2ab27f53964d0535d4dafb51\"><code>2e1eaf0</code></a> Keep Authorization header on subdomain redirects.</li>\n<li>Additional commits viewable in <a href=\"https://github.com/follow-redirects/follow-redirects/compare/v1.13.0...v1.14.8\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=follow-redirects&package-manager=npm_and_yarn&previous-version=1.13.0&new-version=1.14.8)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 339, "title": "Bump follow-redirects from 1.13.0 to 1.14.7 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/339", "html_url": "https://github.com/backtick-se/cowait/pull/339", "diff_url": "https://github.com/backtick-se/cowait/pull/339.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/339.patch", "merged_at": null}, "body": "Bumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.13.0 to 1.14.7.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/2ede36d7c60d3acdcd324dcd99a9dbd52e4fb3a6\"><code>2ede36d</code></a> Release version 1.14.7 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/8b347cbcef7c7b72a6e9be20f5710c17d6163c22\"><code>8b347cb</code></a> Drop Cookie header across domains.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/6f5029ae1a0fdab4dc25f6379a5ee303c2319070\"><code>6f5029a</code></a> Release version 1.14.6 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/af706bee57de954414c0bde0a9f33e62beea3e52\"><code>af706be</code></a> Ignore null headers.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/d01ab7a5c5df3617c7a40a03de7af6427fdfac55\"><code>d01ab7a</code></a> Release version 1.14.5 of the npm package.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/40052ea8aa13559becee5795715c1d45b1f0eb76\"><code>40052ea</code></a> Make compatible with Node 17.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/86f7572f9365dadc39f85916259b58973819617f\"><code>86f7572</code></a> Fix: clear internal timer on request abort to avoid leakage</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/2e1eaf0218c5315a2ab27f53964d0535d4dafb51\"><code>2e1eaf0</code></a> Keep Authorization header on subdomain redirects.</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/2ad9e82b6277ae2104f7770e9ff1186cc6da29d4\"><code>2ad9e82</code></a> Carry over Host header on relative redirects (<a href=\"https://github-redirect.dependabot.com/follow-redirects/follow-redirects/issues/172\">#172</a>)</li>\n<li><a href=\"https://github.com/follow-redirects/follow-redirects/commit/77e2a581e1d1811674b7b74745a9c20a5b939488\"><code>77e2a58</code></a> Release version 1.14.4 of the npm package.</li>\n<li>Additional commits viewable in <a href=\"https://github.com/follow-redirects/follow-redirects/compare/v1.13.0...v1.14.7\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=follow-redirects&package-manager=npm_and_yarn&previous-version=1.13.0&new-version=1.14.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 338, "title": "Bump tensorflow-cpu from 2.4.0 to 2.5.2 in /examples/10-imdb", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2588709704, "node_id": "MDU6TGFiZWwyNTg4NzA5NzA0", "url": "https://api.github.com/repos/backtick-se/cowait/labels/python", "name": "python", "color": "2b67c6", "default": false, "description": "Pull requests that update Python code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/338", "html_url": "https://github.com/backtick-se/cowait/pull/338", "diff_url": "https://github.com/backtick-se/cowait/pull/338.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/338.patch", "merged_at": null}, "body": "Bumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.2.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/releases\">tensorflow-cpu's releases</a>.</em></p>\n<blockquote>\n<h2>TensorFlow 2.5.2</h2>\n<h1>Release 2.5.2</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a code injection issue in <code>saved_model_cli</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41228\">CVE-2021-41228</a>)</li>\n<li>Fixes a vulnerability due to use of uninitialized value in Tensorflow (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41225\">CVE-2021-41225</a>)</li>\n<li>Fixes a heap OOB in <code>FusedBatchNorm</code> kernels (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41223\">CVE-2021-41223</a>)</li>\n<li>Fixes an arbitrary memory read in <code>ImmutableConst</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41227\">CVE-2021-41227</a>)</li>\n<li>Fixes a heap OOB in <code>SparseBinCount</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41226\">CVE-2021-41226</a>)</li>\n<li>Fixes a heap OOB in <code>SparseFillEmptyRows</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41224\">CVE-2021-41224</a>)</li>\n<li>Fixes a segfault due to negative splits in <code>SplitV</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41222\">CVE-2021-41222</a>)</li>\n<li>Fixes segfaults and vulnerabilities caused by accesses to invalid memory during shape inference in <code>Cudnn*</code> ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41221\">CVE-2021-41221</a>)</li>\n<li>Fixes a null pointer exception when <code>Exit</code> node is not preceded by <code>Enter</code> op (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41217\">CVE-2021-41217</a>)</li>\n<li>Fixes an integer division by 0 in <code>tf.raw_ops.AllToAll</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41218\">CVE-2021-41218</a>)</li>\n<li>Fixes an undefined behavior via <code>nullptr</code> reference binding in sparse matrix multiplication (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41219\">CVE-2021-41219</a>)</li>\n<li>Fixes a heap buffer overflow in <code>Transpose</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41216\">CVE-2021-41216</a>)</li>\n<li>Prevents deadlocks arising from mutually recursive <code>tf.function</code> objects (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41213\">CVE-2021-41213</a>)</li>\n<li>Fixes a null pointer exception in <code>DeserializeSparse</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41215\">CVE-2021-41215</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to <code>nullptr</code> in <code>tf.ragged.cross</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41214\">CVE-2021-41214</a>)</li>\n<li>Fixes a heap OOB read in <code>tf.ragged.cross</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41212\">CVE-2021-41212</a>)</li>\n<li>Fixes a heap OOB read in all <code>tf.raw_ops.QuantizeAndDequantizeV*</code> ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41205\">CVE-2021-41205</a>)</li>\n<li>Fixes an FPE in <code>ParallelConcat</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207\">CVE-2021-41207</a>)</li>\n<li>Fixes FPE issues in convolutions with zero size filters (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41209\">CVE-2021-41209</a>)</li>\n<li>Fixes a heap OOB read in <code>tf.raw_ops.SparseCountSparseOutput</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41210\">CVE-2021-41210</a>)</li>\n<li>Fixes vulnerabilities caused by incomplete validation in boosted trees code (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208\">CVE-2021-41208</a>)</li>\n<li>Fixes vulnerabilities caused by incomplete validation of shapes in multiple TF ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41206\">CVE-2021-41206</a>)</li>\n<li>Fixes a segfault produced while copying constant resource tensor (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41204\">CVE-2021-41204</a>)</li>\n<li>Fixes a vulnerability caused by unitialized access in <code>EinsumHelper::ParseEquation</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41201\">CVE-2021-41201</a>)</li>\n<li>Fixes several vulnerabilities and segfaults caused by missing validation during checkpoint loading (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41203\">CVE-2021-41203</a>)</li>\n<li>Fixes an overflow producing a crash in <code>tf.range</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41202\">CVE-2021-41202</a>)</li>\n<li>Fixes an overflow producing a crash in <code>tf.image.resize</code> when size is large (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41199\">CVE-2021-41199</a>)</li>\n<li>Fixes an overflow producing a crash in <code>tf.tile</code> when tiling tensor is large (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41198\">CVE-2021-41198</a>)</li>\n<li>Fixes a vulnerability produced due to incomplete validation in <code>tf.summary.create_file_writer</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41200\">CVE-2021-41200</a>)</li>\n<li>Fixes multiple crashes due to overflow and <code>CHECK</code>-fail in ops with large tensor shapes (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41197\">CVE-2021-41197</a>)</li>\n<li>Fixes a crash in <code>max_pool3d</code> when size argument is 0 or negative (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41196\">CVE-2021-41196</a>)</li>\n<li>Fixes a crash in <code>tf.math.segment_*</code> operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41195\">CVE-2021-41195</a>)</li>\n<li>Updates <code>curl</code> to <code>7.78.0</code> to handle <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22922\">CVE-2021-22922</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22923\">CVE-2021-22923</a>,  <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22924\">CVE-2021-22924</a>, <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22925\">CVE-2021-22925</a>, and <a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22926\">CVE-2021-22926</a>.</li>\n</ul>\n<h2>TensorFlow 2.5.1</h2>\n<h1>Release 2.5.1</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a heap out of bounds access in sparse reduction operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635\">CVE-2021-37635</a>)</li>\n<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636\">CVE-2021-37636</a>)</li>\n<li>Fixes a null pointer dereference in <code>CompressElement</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637\">CVE-2021-37637</a>)</li>\n<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638\">CVE-2021-37638</a>)</li>\n<li>Fixes a null pointer dereference and a heap OOB read arising from operations restoring tensors (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639\">CVE-2021-37639</a>)</li>\n<li>Fixes an integer division by 0 in sparse reshaping (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640\">CVE-2021-37640</a>)</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\">tensorflow-cpu's changelog</a>.</em></p>\n<blockquote>\n<h1>Release 2.5.2</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a code injection issue in <code>saved_model_cli</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41228\">CVE-2021-41228</a>)</li>\n<li>Fixes a vulnerability due to use of uninitialized value in Tensorflow\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41225\">CVE-2021-41225</a>)</li>\n<li>Fixes a heap OOB in <code>FusedBatchNorm</code> kernels\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41223\">CVE-2021-41223</a>)</li>\n<li>Fixes an arbitrary memory read in <code>ImmutableConst</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41227\">CVE-2021-41227</a>)</li>\n<li>Fixes a heap OOB in <code>SparseBinCount</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41226\">CVE-2021-41226</a>)</li>\n<li>Fixes a heap OOB in <code>SparseFillEmptyRows</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41224\">CVE-2021-41224</a>)</li>\n<li>Fixes a segfault due to negative splits in <code>SplitV</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41222\">CVE-2021-41222</a>)</li>\n<li>Fixes segfaults and vulnerabilities caused by accesses to invalid memory\nduring shape inference in <code>Cudnn*</code> ops\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41221\">CVE-2021-41221</a>)</li>\n<li>Fixes a null pointer exception when <code>Exit</code> node is not preceded by\n<code>Enter</code> op (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41217\">CVE-2021-41217</a>)</li>\n<li>Fixes an integer division by 0 in <code>tf.raw_ops.AllToAll</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41218\">CVE-2021-41218</a>)</li>\n<li>Fixes an undefined behavior via <code>nullptr</code> reference binding in sparse matrix\nmultiplication (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41219\">CVE-2021-41219</a>)</li>\n<li>Fixes a heap buffer overflow in <code>Transpose</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41216\">CVE-2021-41216</a>)</li>\n<li>Prevents deadlocks arising from mutually recursive <code>tf.function</code> objects\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41213\">CVE-2021-41213</a>)</li>\n<li>Fixes a null pointer exception in <code>DeserializeSparse</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41215\">CVE-2021-41215</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to\n<code>nullptr</code> in <code>tf.ragged.cross</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41214\">CVE-2021-41214</a>)</li>\n<li>Fixes a heap OOB read in <code>tf.ragged.cross</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41212\">CVE-2021-41212</a>)</li>\n<li>Fixes a heap OOB read in all <code>tf.raw_ops.QuantizeAndDequantizeV*</code>\nops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41205\">CVE-2021-41205</a>)</li>\n<li>Fixes an FPE in <code>ParallelConcat</code> ([CVE-2021-41207]\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207\">https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41207</a>))</li>\n<li>Fixes FPE issues in convolutions with zero size filters\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41209\">CVE-2021-41209</a>)</li>\n<li>Fixes a heap OOB read in <code>tf.raw_ops.SparseCountSparseOutput</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41210\">CVE-2021-41210</a>)</li>\n<li>Fixes vulnerabilities caused by incomplete validation in boosted trees code\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41208\">CVE-2021-41208</a>)</li>\n<li>Fixes vulnerabilities caused by incomplete validation of shapes in multiple\nTF ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-41206\">CVE-2021-41206</a>)</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/957590ea15cc03ee2e00fc61934647d54836676f\"><code>957590e</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52873\">#52873</a> from tensorflow-jenkins/relnotes-2.5.2-20787</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/2e1d16d7aac34983e4ff0d55f434e4d07fea7bce\"><code>2e1d16d</code></a> Update RELEASE.md</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/2fa6dd95659985a9ee9429d146c29c27f12e342c\"><code>2fa6dd9</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52877\">#52877</a> from tensorflow-jenkins/version-numbers-2.5.2-192</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/480748994b151d28818cdfc659f4332bce8a97b2\"><code>4807489</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52881\">#52881</a> from tensorflow/fix-build-1-on-r2.5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/d398bdfd5d2190a3274141416c54f3e7207e96f3\"><code>d398bdf</code></a> Disable failing test</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/857ad5ef1eb23bbeb378b1f3c3cbe6f38286c2d0\"><code>857ad5e</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52878\">#52878</a> from tensorflow/fix-build-1-on-r2.5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/6c2a215be0afd10008046bd072daaf81228a19a1\"><code>6c2a215</code></a> Disable failing test</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/f5c57d495753bbc166abc928430ea808aa8aa6b3\"><code>f5c57d4</code></a> Update version numbers to 2.5.2</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/e51f9495418eb373074a652b5bf4bba1c41aa132\"><code>e51f949</code></a> Insert release notes place-fill</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/2620d2cd5c4e03df6d02ae18fda2a9fdc2466738\"><code>2620d2c</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/52863\">#52863</a> from tensorflow/fix-build-3-on-r2.5</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.2\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-cpu&package-manager=pip&previous-version=2.4.0&new-version=2.5.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 336, "title": "Version 0.4.30", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/336", "html_url": "https://github.com/backtick-se/cowait/pull/336", "diff_url": "https://github.com/backtick-se/cowait/pull/336.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/336.patch", "merged_at": "2021-11-01T09:50:18Z"}, "body": "- Added an option to provide a fallback value for environment values (e.g. to provide secret defaults) when using docker.\r\n- Properly mark the `--capture` option for `cowait run` as a boolean flag.\r\n- Improved error reporting for return type errors.\r\n- Fix `aiohttp` to version `3.7.4` to avoid import errors"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 333, "title": "Bump url-parse from 1.5.1 to 1.5.3 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/333", "html_url": "https://github.com/backtick-se/cowait/pull/333", "diff_url": "https://github.com/backtick-se/cowait/pull/333.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/333.patch", "merged_at": null}, "body": "Bumps [url-parse](https://github.com/unshiftio/url-parse) from 1.5.1 to 1.5.3.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/ad444931666a30bad11472d89a216461cf16cae2\"><code>ad44493</code></a> [dist] 1.5.3</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/c7984617e235892cc22e0f47bb5ff1c012e6e39f\"><code>c798461</code></a> [fix] Fix host parsing for file URLs (<a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/210\">#210</a>)</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/201034b8670c2aa382d7ec410ee750ac6f2f9c38\"><code>201034b</code></a> [dist] 1.5.2</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/2d9ac2c94067742b2116332c1e03be9f37371dff\"><code>2d9ac2c</code></a> [fix] Sanitize only special URLs (<a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/209\">#209</a>)</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/fb128af4f43fa17f351d50cf615c7598c751f50a\"><code>fb128af</code></a> [fix] Use <code>'null'</code> as <code>origin</code> for non special URLs</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/fed6d9e338ea39de2d68bb66607066d71328c62f\"><code>fed6d9e</code></a> [fix] Add a leading slash only if the URL is special</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/94872e7ab9103ee69b958959baa14c9e682a7f10\"><code>94872e7</code></a> [fix] Do not incorrectly set the <code>slashes</code> property to <code>true</code></li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/81ab967889b08112d3356e451bf03e6aa0cbb7e0\"><code>81ab967</code></a> [fix] Ignore slashes after the protocol for special URLs</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/ee22050a48a67409aa5f7c87947284156d615bd1\"><code>ee22050</code></a> [ci] Use GitHub Actions</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/d2979b586d8c7751e0c77f127d9ce1b2143cc0c9\"><code>d2979b5</code></a> [fix] Special case the <code>file:</code> protocol (<a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/204\">#204</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/unshiftio/url-parse/compare/1.5.1...1.5.3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=url-parse&package-manager=npm_and_yarn&previous-version=1.5.1&new-version=1.5.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 332, "title": "Bump tmpl from 1.0.4 to 1.0.5 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/332", "html_url": "https://github.com/backtick-se/cowait/pull/332", "diff_url": "https://github.com/backtick-se/cowait/pull/332.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/332.patch", "merged_at": "2022-02-21T09:51:06Z"}, "body": "Bumps [tmpl](https://github.com/daaku/nodejs-tmpl) from 1.0.4 to 1.0.5.\n<details>\n<summary>Commits</summary>\n<ul>\n<li>See full diff in <a href=\"https://github.com/daaku/nodejs-tmpl/commits/v1.0.5\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tmpl&package-manager=npm_and_yarn&previous-version=1.0.4&new-version=1.0.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 331, "title": "Bump tensorflow-cpu from 2.4.0 to 2.5.1 in /examples/10-imdb", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2588709704, "node_id": "MDU6TGFiZWwyNTg4NzA5NzA0", "url": "https://api.github.com/repos/backtick-se/cowait/labels/python", "name": "python", "color": "2b67c6", "default": false, "description": "Pull requests that update Python code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/331", "html_url": "https://github.com/backtick-se/cowait/pull/331", "diff_url": "https://github.com/backtick-se/cowait/pull/331.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/331.patch", "merged_at": null}, "body": "Bumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/releases\">tensorflow-cpu's releases</a>.</em></p>\n<blockquote>\n<h2>TensorFlow 2.5.1</h2>\n<h1>Release 2.5.1</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a heap out of bounds access in sparse reduction operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635\">CVE-2021-37635</a>)</li>\n<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636\">CVE-2021-37636</a>)</li>\n<li>Fixes a null pointer dereference in <code>CompressElement</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637\">CVE-2021-37637</a>)</li>\n<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638\">CVE-2021-37638</a>)</li>\n<li>Fixes a null pointer dereference and a heap OOB read arising from operations restoring tensors (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639\">CVE-2021-37639</a>)</li>\n<li>Fixes an integer division by 0 in sparse reshaping (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640\">CVE-2021-37640</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceScatterDiv</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37642\">CVE-2021-37642</a>)</li>\n<li>Fixes a heap OOB in <code>RaggedGather</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37641\">CVE-2021-37641</a>)</li>\n<li>Fixes a <code>std::abort</code> raised from <code>TensorListReserve</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37644\">CVE-2021-37644</a>)</li>\n<li>Fixes a null pointer dereference in <code>MatrixDiagPartOp</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37643\">CVE-2021-37643</a>)</li>\n<li>Fixes an integer overflow due to conversion to unsigned (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37645\">CVE-2021-37645</a>)</li>\n<li>Fixes a bad allocation error in <code>StringNGrams</code> caused by integer conversion (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37646\">CVE-2021-37646</a>)</li>\n<li>Fixes a null pointer dereference in <code>SparseTensorSliceDataset</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37647\">CVE-2021-37647</a>)</li>\n<li>Fixes an incorrect validation of <code>SaveV2</code> inputs (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37648\">CVE-2021-37648</a>)</li>\n<li>Fixes a null pointer dereference in <code>UncompressElement</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37649\">CVE-2021-37649</a>)</li>\n<li>Fixes a segfault and a heap buffer overflow in <code>{Experimental,}DatasetToTFRecord</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37650\">CVE-2021-37650</a>)</li>\n<li>Fixes a heap buffer overflow in <code>FractionalAvgPoolGrad</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37651\">CVE-2021-37651</a>)</li>\n<li>Fixes a use after free in boosted trees creation (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37652\">CVE-2021-37652</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceGather</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37653\">CVE-2021-37653</a>)</li>\n<li>Fixes a heap OOB and a <code>CHECK</code> fail in <code>ResourceGather</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37654\">CVE-2021-37654</a>)</li>\n<li>Fixes a heap OOB in <code>ResourceScatterUpdate</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37655\">CVE-2021-37655</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>RaggedTensorToSparse</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37656\">CVE-2021-37656</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>MatrixDiagV*</code> ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37657\">CVE-2021-37657</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>MatrixSetDiagV*</code> ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37658\">CVE-2021-37658</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr and heap OOB in binary cwise ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37659\">CVE-2021-37659</a>)</li>\n<li>Fixes a division by 0 in inplace operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37660\">CVE-2021-37660</a>)</li>\n<li>Fixes a crash caused by integer conversion to unsigned (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37661\">CVE-2021-37661</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in boosted trees (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37662\">CVE-2021-37662</a>)</li>\n<li>Fixes a heap OOB in boosted trees (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37664\">CVE-2021-37664</a>)</li>\n<li>Fixes vulnerabilities arising from incomplete validation in <code>QuantizeV2</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37663\">CVE-2021-37663</a>)</li>\n<li>Fixes vulnerabilities arising from incomplete validation in MKL requantization (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37665\">CVE-2021-37665</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>RaggedTensorToVariant</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37666\">CVE-2021-37666</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in unicode encoding (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37667\">CVE-2021-37667</a>)</li>\n<li>Fixes an FPE in <code>tf.raw_ops.UnravelIndex</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37668\">CVE-2021-37668</a>)</li>\n<li>Fixes a crash in NMS ops caused by integer conversion to unsigned (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37669\">CVE-2021-37669</a>)</li>\n<li>Fixes a heap OOB in <code>UpperBound</code> and <code>LowerBound</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37670\">CVE-2021-37670</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in map operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37671\">CVE-2021-37671</a>)</li>\n<li>Fixes a heap OOB in <code>SdcaOptimizerV2</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37672\">CVE-2021-37672</a>)</li>\n<li>Fixes a <code>CHECK</code>-fail in <code>MapStage</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37673\">CVE-2021-37673</a>)</li>\n<li>Fixes a vulnerability arising from incomplete validation in <code>MaxPoolGrad</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37674\">CVE-2021-37674</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in shape inference (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37676\">CVE-2021-37676</a>)</li>\n<li>Fixes a division by 0 in most convolution operators (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37675\">CVE-2021-37675</a>)</li>\n<li>Fixes vulnerabilities arising from missing validation in shape inference for <code>Dequantize</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37677\">CVE-2021-37677</a>)</li>\n<li>Fixes an arbitrary code execution due to YAML deserialization (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37678\">CVE-2021-37678</a>)</li>\n<li>Fixes a heap OOB in nested <code>tf.map_fn</code> with <code>RaggedTensor</code>s (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37679\">CVE-2021-37679</a>)</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\">tensorflow-cpu's changelog</a>.</em></p>\n<blockquote>\n<h1>Release 2.5.1</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a heap out of bounds access in sparse reduction operations\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635\">CVE-2021-37635</a>)</li>\n<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636\">CVE-2021-37636</a>)</li>\n<li>Fixes a null pointer dereference in <code>CompressElement</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637\">CVE-2021-37637</a>)</li>\n<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638\">CVE-2021-37638</a>)</li>\n<li>Fixes a null pointer dereference and a heap OOB read arising from operations\nrestoring tensors\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639\">CVE-2021-37639</a>)</li>\n<li>Fixes an integer division by 0 in sparse reshaping\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640\">CVE-2021-37640</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceScatterDiv</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37642\">CVE-2021-37642</a>)</li>\n<li>Fixes a heap OOB in <code>RaggedGather</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37641\">CVE-2021-37641</a>)</li>\n<li>Fixes a <code>std::abort</code> raised from <code>TensorListReserve</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37644\">CVE-2021-37644</a>)</li>\n<li>Fixes a null pointer dereference in <code>MatrixDiagPartOp</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37643\">CVE-2021-37643</a>)</li>\n<li>Fixes an integer overflow due to conversion to unsigned\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37645\">CVE-2021-37645</a>)</li>\n<li>Fixes a bad allocation error in <code>StringNGrams</code> caused by integer conversion\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37646\">CVE-2021-37646</a>)</li>\n<li>Fixes a null pointer dereference in <code>SparseTensorSliceDataset</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37647\">CVE-2021-37647</a>)</li>\n<li>Fixes an incorrect validation of <code>SaveV2</code> inputs\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37648\">CVE-2021-37648</a>)</li>\n<li>Fixes a null pointer dereference in <code>UncompressElement</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37649\">CVE-2021-37649</a>)</li>\n<li>Fixes a segfault and a heap buffer overflow in\n<code>{Experimental,}DatasetToTFRecord</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37650\">CVE-2021-37650</a>)</li>\n<li>Fixes a heap buffer overflow in <code>FractionalAvgPoolGrad</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37651\">CVE-2021-37651</a>)</li>\n<li>Fixes a use after free in boosted trees creation\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37652\">CVE-2021-37652</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceGather</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37653\">CVE-2021-37653</a>)</li>\n<li>Fixes a heap OOB and a <code>CHECK</code> fail in <code>ResourceGather</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37654\">CVE-2021-37654</a>)</li>\n<li>Fixes a heap OOB in <code>ResourceScatterUpdate</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37655\">CVE-2021-37655</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in\n<code>RaggedTensorToSparse</code></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/8222c1cfc866126111f23bd9872998480cebf2c1\"><code>8222c1c</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51381\">#51381</a> from tensorflow/mm-fix-r2.5-build</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/d5842603e03504d8ed30b0622e03869899c9f41d\"><code>d584260</code></a> Disable broken/flaky test</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/f6c6ce30bab35320e5da6e25fbdd8c369de75ab7\"><code>f6c6ce3</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51367\">#51367</a> from tensorflow-jenkins/version-numbers-2.5.1-17468</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/3ca781272c60959f3a24a2b440f2f275aab71a76\"><code>3ca7812</code></a> Update version numbers to 2.5.1</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/4fdf683c878574bc2c39fe8ac152ffc26183efb6\"><code>4fdf683</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51361\">#51361</a> from tensorflow/mm-update-relnotes-on-r2.5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/05fc01aa0ffe973a2b1517bd92479e38f5d2c72a\"><code>05fc01a</code></a> Put CVE numbers for fixes in parentheses</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/bee1dc4a6116b53101fc8773f43662a89514847d\"><code>bee1dc4</code></a> Update release notes for the new patch release</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/47beb4c1987293659784d6aa1dfaacc86bc07d84\"><code>47beb4c</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/50597\">#50597</a> from kruglov-dmitry/v2.5.0-sync-abseil-cmake-bazel</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/6f39597952e230d2a782547380cdf8143bdcdc5d\"><code>6f39597</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49383\">#49383</a> from ashahab/abin-load-segfault-r2.5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/0539b34641ee0773f07d859fe69dc0dfc71069d3\"><code>0539b34</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/48979\">#48979</a> from liufengdb/r2.5-cherrypick</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-cpu&package-manager=pip&previous-version=2.4.0&new-version=2.5.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 330, "title": "Bump tensorflow-cpu from 2.4.0 to 2.5.1 in /examples/06-tensorflow", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2588709704, "node_id": "MDU6TGFiZWwyNTg4NzA5NzA0", "url": "https://api.github.com/repos/backtick-se/cowait/labels/python", "name": "python", "color": "2b67c6", "default": false, "description": "Pull requests that update Python code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/330", "html_url": "https://github.com/backtick-se/cowait/pull/330", "diff_url": "https://github.com/backtick-se/cowait/pull/330.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/330.patch", "merged_at": null}, "body": "Bumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/releases\">tensorflow-cpu's releases</a>.</em></p>\n<blockquote>\n<h2>TensorFlow 2.5.1</h2>\n<h1>Release 2.5.1</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a heap out of bounds access in sparse reduction operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635\">CVE-2021-37635</a>)</li>\n<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636\">CVE-2021-37636</a>)</li>\n<li>Fixes a null pointer dereference in <code>CompressElement</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637\">CVE-2021-37637</a>)</li>\n<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638\">CVE-2021-37638</a>)</li>\n<li>Fixes a null pointer dereference and a heap OOB read arising from operations restoring tensors (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639\">CVE-2021-37639</a>)</li>\n<li>Fixes an integer division by 0 in sparse reshaping (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640\">CVE-2021-37640</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceScatterDiv</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37642\">CVE-2021-37642</a>)</li>\n<li>Fixes a heap OOB in <code>RaggedGather</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37641\">CVE-2021-37641</a>)</li>\n<li>Fixes a <code>std::abort</code> raised from <code>TensorListReserve</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37644\">CVE-2021-37644</a>)</li>\n<li>Fixes a null pointer dereference in <code>MatrixDiagPartOp</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37643\">CVE-2021-37643</a>)</li>\n<li>Fixes an integer overflow due to conversion to unsigned (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37645\">CVE-2021-37645</a>)</li>\n<li>Fixes a bad allocation error in <code>StringNGrams</code> caused by integer conversion (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37646\">CVE-2021-37646</a>)</li>\n<li>Fixes a null pointer dereference in <code>SparseTensorSliceDataset</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37647\">CVE-2021-37647</a>)</li>\n<li>Fixes an incorrect validation of <code>SaveV2</code> inputs (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37648\">CVE-2021-37648</a>)</li>\n<li>Fixes a null pointer dereference in <code>UncompressElement</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37649\">CVE-2021-37649</a>)</li>\n<li>Fixes a segfault and a heap buffer overflow in <code>{Experimental,}DatasetToTFRecord</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37650\">CVE-2021-37650</a>)</li>\n<li>Fixes a heap buffer overflow in <code>FractionalAvgPoolGrad</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37651\">CVE-2021-37651</a>)</li>\n<li>Fixes a use after free in boosted trees creation (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37652\">CVE-2021-37652</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceGather</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37653\">CVE-2021-37653</a>)</li>\n<li>Fixes a heap OOB and a <code>CHECK</code> fail in <code>ResourceGather</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37654\">CVE-2021-37654</a>)</li>\n<li>Fixes a heap OOB in <code>ResourceScatterUpdate</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37655\">CVE-2021-37655</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>RaggedTensorToSparse</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37656\">CVE-2021-37656</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>MatrixDiagV*</code> ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37657\">CVE-2021-37657</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>MatrixSetDiagV*</code> ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37658\">CVE-2021-37658</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr and heap OOB in binary cwise ops (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37659\">CVE-2021-37659</a>)</li>\n<li>Fixes a division by 0 in inplace operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37660\">CVE-2021-37660</a>)</li>\n<li>Fixes a crash caused by integer conversion to unsigned (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37661\">CVE-2021-37661</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in boosted trees (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37662\">CVE-2021-37662</a>)</li>\n<li>Fixes a heap OOB in boosted trees (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37664\">CVE-2021-37664</a>)</li>\n<li>Fixes vulnerabilities arising from incomplete validation in <code>QuantizeV2</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37663\">CVE-2021-37663</a>)</li>\n<li>Fixes vulnerabilities arising from incomplete validation in MKL requantization (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37665\">CVE-2021-37665</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in <code>RaggedTensorToVariant</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37666\">CVE-2021-37666</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in unicode encoding (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37667\">CVE-2021-37667</a>)</li>\n<li>Fixes an FPE in <code>tf.raw_ops.UnravelIndex</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37668\">CVE-2021-37668</a>)</li>\n<li>Fixes a crash in NMS ops caused by integer conversion to unsigned (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37669\">CVE-2021-37669</a>)</li>\n<li>Fixes a heap OOB in <code>UpperBound</code> and <code>LowerBound</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37670\">CVE-2021-37670</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in map operations (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37671\">CVE-2021-37671</a>)</li>\n<li>Fixes a heap OOB in <code>SdcaOptimizerV2</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37672\">CVE-2021-37672</a>)</li>\n<li>Fixes a <code>CHECK</code>-fail in <code>MapStage</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37673\">CVE-2021-37673</a>)</li>\n<li>Fixes a vulnerability arising from incomplete validation in <code>MaxPoolGrad</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37674\">CVE-2021-37674</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in shape inference (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37676\">CVE-2021-37676</a>)</li>\n<li>Fixes a division by 0 in most convolution operators (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37675\">CVE-2021-37675</a>)</li>\n<li>Fixes vulnerabilities arising from missing validation in shape inference for <code>Dequantize</code> (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37677\">CVE-2021-37677</a>)</li>\n<li>Fixes an arbitrary code execution due to YAML deserialization (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37678\">CVE-2021-37678</a>)</li>\n<li>Fixes a heap OOB in nested <code>tf.map_fn</code> with <code>RaggedTensor</code>s (<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37679\">CVE-2021-37679</a>)</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\">tensorflow-cpu's changelog</a>.</em></p>\n<blockquote>\n<h1>Release 2.5.1</h1>\n<p>This release introduces several vulnerability fixes:</p>\n<ul>\n<li>Fixes a heap out of bounds access in sparse reduction operations\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37635\">CVE-2021-37635</a>)</li>\n<li>Fixes a floating point exception in <code>SparseDenseCwiseDiv</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37636\">CVE-2021-37636</a>)</li>\n<li>Fixes a null pointer dereference in <code>CompressElement</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37637\">CVE-2021-37637</a>)</li>\n<li>Fixes a null pointer dereference in <code>RaggedTensorToTensor</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37638\">CVE-2021-37638</a>)</li>\n<li>Fixes a null pointer dereference and a heap OOB read arising from operations\nrestoring tensors\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37639\">CVE-2021-37639</a>)</li>\n<li>Fixes an integer division by 0 in sparse reshaping\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37640\">CVE-2021-37640</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceScatterDiv</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37642\">CVE-2021-37642</a>)</li>\n<li>Fixes a heap OOB in <code>RaggedGather</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37641\">CVE-2021-37641</a>)</li>\n<li>Fixes a <code>std::abort</code> raised from <code>TensorListReserve</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37644\">CVE-2021-37644</a>)</li>\n<li>Fixes a null pointer dereference in <code>MatrixDiagPartOp</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37643\">CVE-2021-37643</a>)</li>\n<li>Fixes an integer overflow due to conversion to unsigned\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37645\">CVE-2021-37645</a>)</li>\n<li>Fixes a bad allocation error in <code>StringNGrams</code> caused by integer conversion\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37646\">CVE-2021-37646</a>)</li>\n<li>Fixes a null pointer dereference in <code>SparseTensorSliceDataset</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37647\">CVE-2021-37647</a>)</li>\n<li>Fixes an incorrect validation of <code>SaveV2</code> inputs\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37648\">CVE-2021-37648</a>)</li>\n<li>Fixes a null pointer dereference in <code>UncompressElement</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37649\">CVE-2021-37649</a>)</li>\n<li>Fixes a segfault and a heap buffer overflow in\n<code>{Experimental,}DatasetToTFRecord</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37650\">CVE-2021-37650</a>)</li>\n<li>Fixes a heap buffer overflow in <code>FractionalAvgPoolGrad</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37651\">CVE-2021-37651</a>)</li>\n<li>Fixes a use after free in boosted trees creation\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37652\">CVE-2021-37652</a>)</li>\n<li>Fixes a division by 0 in <code>ResourceGather</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37653\">CVE-2021-37653</a>)</li>\n<li>Fixes a heap OOB and a <code>CHECK</code> fail in <code>ResourceGather</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37654\">CVE-2021-37654</a>)</li>\n<li>Fixes a heap OOB in <code>ResourceScatterUpdate</code>\n(<a href=\"https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-37655\">CVE-2021-37655</a>)</li>\n<li>Fixes an undefined behavior arising from reference binding to nullptr in\n<code>RaggedTensorToSparse</code></li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/8222c1cfc866126111f23bd9872998480cebf2c1\"><code>8222c1c</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51381\">#51381</a> from tensorflow/mm-fix-r2.5-build</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/d5842603e03504d8ed30b0622e03869899c9f41d\"><code>d584260</code></a> Disable broken/flaky test</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/f6c6ce30bab35320e5da6e25fbdd8c369de75ab7\"><code>f6c6ce3</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51367\">#51367</a> from tensorflow-jenkins/version-numbers-2.5.1-17468</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/3ca781272c60959f3a24a2b440f2f275aab71a76\"><code>3ca7812</code></a> Update version numbers to 2.5.1</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/4fdf683c878574bc2c39fe8ac152ffc26183efb6\"><code>4fdf683</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/51361\">#51361</a> from tensorflow/mm-update-relnotes-on-r2.5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/05fc01aa0ffe973a2b1517bd92479e38f5d2c72a\"><code>05fc01a</code></a> Put CVE numbers for fixes in parentheses</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/bee1dc4a6116b53101fc8773f43662a89514847d\"><code>bee1dc4</code></a> Update release notes for the new patch release</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/47beb4c1987293659784d6aa1dfaacc86bc07d84\"><code>47beb4c</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/50597\">#50597</a> from kruglov-dmitry/v2.5.0-sync-abseil-cmake-bazel</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/6f39597952e230d2a782547380cdf8143bdcdc5d\"><code>6f39597</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49383\">#49383</a> from ashahab/abin-load-segfault-r2.5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/0539b34641ee0773f07d859fe69dc0dfc71069d3\"><code>0539b34</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/48979\">#48979</a> from liufengdb/r2.5-cherrypick</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-cpu&package-manager=pip&previous-version=2.4.0&new-version=2.5.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 329, "title": "Bump path-parse from 1.0.6 to 1.0.7 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/329", "html_url": "https://github.com/backtick-se/cowait/pull/329", "diff_url": "https://github.com/backtick-se/cowait/pull/329.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/329.patch", "merged_at": "2022-02-21T09:50:58Z"}, "body": "Bumps [path-parse](https://github.com/jbgutierrez/path-parse) from 1.0.6 to 1.0.7.\n<details>\n<summary>Commits</summary>\n<ul>\n<li>See full diff in <a href=\"https://github.com/jbgutierrez/path-parse/commits/v1.0.7\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=path-parse&package-manager=npm_and_yarn&previous-version=1.0.6&new-version=1.0.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 327, "title": "Improve logs command", "labels": [{"id": 1562464620, "node_id": "MDU6TGFiZWwxNTYyNDY0NjIw", "url": "https://api.github.com/repos/backtick-se/cowait/labels/enhancement", "name": "enhancement", "color": "9cdd56", "default": true, "description": "Improve existing features"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/327", "html_url": "https://github.com/backtick-se/cowait/pull/327", "diff_url": "https://github.com/backtick-se/cowait/pull/327.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/327.patch", "merged_at": "2021-07-27T12:26:44Z"}, "body": "- `logs` command now allows reading logs from terminated Kubernetes pods\r\n- Logs show an error if the task appears to have been lost during execution (i.e. OOM killed or manually deleted)"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 326, "title": "File descriptor based output capturing", "labels": [{"id": 1562464620, "node_id": "MDU6TGFiZWwxNTYyNDY0NjIw", "url": "https://api.github.com/repos/backtick-se/cowait/labels/enhancement", "name": "enhancement", "color": "9cdd56", "default": true, "description": "Improve existing features"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/326", "html_url": "https://github.com/backtick-se/cowait/pull/326", "diff_url": "https://github.com/backtick-se/cowait/pull/326.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/326.patch", "merged_at": "2021-07-23T09:19:06Z"}, "body": "Allows capturing of subprocess output.\r\n\r\nImplementation based on pytest's `FDCapture`"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 325, "title": "Improve cowait test", "labels": [{"id": 1562464620, "node_id": "MDU6TGFiZWwxNTYyNDY0NjIw", "url": "https://api.github.com/repos/backtick-se/cowait/labels/enhancement", "name": "enhancement", "color": "9cdd56", "default": true, "description": "Improve existing features"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/325", "html_url": "https://github.com/backtick-se/cowait/pull/325", "diff_url": "https://github.com/backtick-se/cowait/pull/325.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/325.patch", "merged_at": "2021-07-23T09:18:11Z"}, "body": "Adds two new flags:\r\n- `--verbose` enables verbose output from pytest (false by default)\r\n- `--capture` toggles output capturing (true by default)\r\n\r\nImproved the pytest argument generation code"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 322, "title": "Bump ws from 5.2.2 to 5.2.3 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/322", "html_url": "https://github.com/backtick-se/cowait/pull/322", "diff_url": "https://github.com/backtick-se/cowait/pull/322.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/322.patch", "merged_at": "2021-06-22T09:57:42Z"}, "body": "Bumps [ws](https://github.com/websockets/ws) from 5.2.2 to 5.2.3.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/websockets/ws/releases\">ws's releases</a>.</em></p>\n<blockquote>\n<h2>5.2.3</h2>\n<h1>Bug fixes</h1>\n<ul>\n<li>Backported 00c425ec to the 5.x release line (76d47c14).</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/websockets/ws/commit/6dd88e7e968ef2416445d8f8620c17d99b15c77c\"><code>6dd88e7</code></a> [dist] 5.2.3</li>\n<li><a href=\"https://github.com/websockets/ws/commit/76d47c1479002022a3e4357b3c9f0e23a68d4cd2\"><code>76d47c1</code></a> [security] Fix ReDoS vulnerability</li>\n<li>See full diff in <a href=\"https://github.com/websockets/ws/compare/5.2.2...5.2.3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=ws&package-manager=npm_and_yarn&previous-version=5.2.2&new-version=5.2.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 321, "title": "Bump color-string from 1.5.3 to 1.5.5 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/321", "html_url": "https://github.com/backtick-se/cowait/pull/321", "diff_url": "https://github.com/backtick-se/cowait/pull/321.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/321.patch", "merged_at": "2021-06-22T09:57:31Z"}, "body": "Bumps [color-string](https://github.com/Qix-/color-string) from 1.5.3 to 1.5.5.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/Qix-/color-string/releases\">color-string's releases</a>.</em></p>\n<blockquote>\n<h2>1.5.5 (Patch/Security Release) - hwb() ReDos patch (low-severity)</h2>\n<blockquote>\n<p>Release notes copied verbatim from the commit message, which can be found here: 0789e21284c33d89ebc4ab4ca6f759b9375ac9d3</p>\n</blockquote>\n<pre><code>Discovered by Yeting Li, c/o Colin Ife via Snyk.io.\n<p>A ReDos (Regular Expression Denial of Service) vulnerability\nwas responsibly disclosed to me via email by Colin on\nMar 5 2021 regarding an exponential time complexity for\nlinearly increasing input lengths for <code>hwb()</code> color strings.</p>\n<p>Strings reaching more than 5000 characters would see several\nmilliseconds of processing time; strings reaching more than\n50,000 characters began seeing 1500ms (1.5s) of processing time.</p>\n<p>The cause was due to a the regular expression that parses\nhwb() strings - specifically, the hue value - where\nthe integer portion of the hue value used a 0-or-more quantifier\nshortly thereafter followed by a 1-or-more quantifier.</p>\n<p>This caused excessive backtracking and a cartesian scan,\nresulting in exponential time complexity given a linear\nincrease in input length.</p>\n<p>Thank you Yeting Li and Colin Ife for bringing this to my\nattention in a secure, responsible and professional manner.</p>\n<p>A CVE will not be assigned for this vulnerability.\n</code></pre></p>\n<h2>1.5.4 (Patch Release)</h2>\n<ul>\n<li>Removes rounding of alpha values in RGBA hex (<code>#rrggbbaa</code>) and condensed-hex (<code>#rgba</code>) parsers, which caused certain unique inputs to result in identical outputs (see <a href=\"https://github.com/qix-/color/issues/174\">https://github.com/qix-/color/issues/174</a>).</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li>See full diff in <a href=\"https://github.com/Qix-/color-string/commits/1.5.5\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=color-string&package-manager=npm_and_yarn&previous-version=1.5.3&new-version=1.5.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 320, "title": "Pytest marks support for cowait test", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/320", "html_url": "https://github.com/backtick-se/cowait/pull/320", "diff_url": "https://github.com/backtick-se/cowait/pull/320.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/320.patch", "merged_at": "2021-06-22T10:06:13Z"}, "body": ""}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 319, "title": "Bump merge-deep from 3.0.2 to 3.0.3 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/319", "html_url": "https://github.com/backtick-se/cowait/pull/319", "diff_url": "https://github.com/backtick-se/cowait/pull/319.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/319.patch", "merged_at": "2021-06-22T09:57:02Z"}, "body": "Bumps [merge-deep](https://github.com/jonschlinkert/merge-deep) from 3.0.2 to 3.0.3.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/jonschlinkert/merge-deep/commit/628ff47c9d824ccf21adf9a2b7cc6b74632e11a1\"><code>628ff47</code></a> 3.0.3</li>\n<li><a href=\"https://github.com/jonschlinkert/merge-deep/commit/cfbe20ccdb00255b711de57e37ed8ce9f109ef3f\"><code>cfbe20c</code></a> run verb to generate README documentation</li>\n<li><a href=\"https://github.com/jonschlinkert/merge-deep/commit/e370968581413a2e5ffdbbf7c2f5094e0e0b3861\"><code>e370968</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/jonschlinkert/merge-deep/issues/17\">#17</a> from jonschlinkert/key-properties</li>\n<li><a href=\"https://github.com/jonschlinkert/merge-deep/commit/393e2cbaeacf54e77a307c3620a00f0ac057b8d5\"><code>393e2cb</code></a> adding a test to ensure using merge-deep for inheritance still works</li>\n<li><a href=\"https://github.com/jonschlinkert/merge-deep/commit/c39b16134a6a9704be2e661b49b92e8561f10d90\"><code>c39b161</code></a> add test to ensure constructor is not cloned</li>\n<li><a href=\"https://github.com/jonschlinkert/merge-deep/commit/11e5dd56de8a6aed0b1ed022089dbce6968d82a5\"><code>11e5dd5</code></a> add isValidKey function to ensure only valid keys are merged</li>\n<li>See full diff in <a href=\"https://github.com/jonschlinkert/merge-deep/compare/3.0.2...3.0.3\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=merge-deep&package-manager=npm_and_yarn&previous-version=3.0.2&new-version=3.0.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 318, "title": "Bump tensorflow-cpu from 2.4.0 to 2.5.0 in /examples/06-tensorflow", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2588709704, "node_id": "MDU6TGFiZWwyNTg4NzA5NzA0", "url": "https://api.github.com/repos/backtick-se/cowait/labels/python", "name": "python", "color": "2b67c6", "default": false, "description": "Pull requests that update Python code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/318", "html_url": "https://github.com/backtick-se/cowait/pull/318", "diff_url": "https://github.com/backtick-se/cowait/pull/318.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/318.patch", "merged_at": null}, "body": "Bumps [tensorflow-cpu](https://github.com/tensorflow/tensorflow) from 2.4.0 to 2.5.0.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/releases\">tensorflow-cpu's releases</a>.</em></p>\n<blockquote>\n<h2>TensorFlow 2.5.0</h2>\n<h1>Release 2.5.0</h1>\n<h2>Major Features and Improvements</h2>\n<ul>\n<li>Support for Python3.9 has been added.</li>\n<li><code>tf.data</code>:\n<ul>\n<li><code>tf.data</code> service now supports strict round-robin reads, which is useful for synchronous training workloads where example sizes vary. With strict round robin reads, users can guarantee that consumers get similar-sized examples in the same step.</li>\n<li>tf.data service now supports optional compression. Previously data would always be compressed, but now you can disable compression by passing <code>compression=None</code> to <code>tf.data.experimental.service.distribute(...)</code>.</li>\n<li><code>tf.data.Dataset.batch()</code> now supports <code>num_parallel_calls</code> and <code>deterministic</code> arguments. <code>num_parallel_calls</code> is used to indicate that multiple input batches should be computed in parallel. With <code>num_parallel_calls</code> set, <code>deterministic</code> is used to indicate that outputs can be obtained in the non-deterministic order.</li>\n<li>Options returned by <code>tf.data.Dataset.options()</code> are no longer mutable.</li>\n<li>tf.data input pipelines can now be executed in debug mode, which disables any asynchrony, parallelism, or non-determinism and forces Python execution (as opposed to trace-compiled graph execution) of user-defined functions passed into transformations such as <code>map</code>. The debug mode can be enabled through <code>tf.data.experimental.enable_debug_mode()</code>.</li>\n</ul>\n</li>\n<li><code>tf.lite</code>\n<ul>\n<li>Enabled the new MLIR-based quantization backend by default\n<ul>\n<li>The new backend is used for 8 bits full integer post-training quantization</li>\n<li>The new backend removes the redundant rescales and fixes some bugs (shared weight/bias, extremely small scales, etc)</li>\n<li>Set <code>experimental_new_quantizer</code> in tf.lite.TFLiteConverter to False to disable this change</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><code>tf.keras</code>\n<ul>\n<li><code>tf.keras.metrics.AUC</code> now support logit predictions.</li>\n<li>Enabled a new supported input type in <code>Model.fit</code>, <code>tf.keras.utils.experimental.DatasetCreator</code>, which takes a callable, <code>dataset_fn</code>. <code>DatasetCreator</code> is intended to work across all <code>tf.distribute</code> strategies, and is the only input type supported for Parameter Server strategy.</li>\n</ul>\n</li>\n<li><code>tf.distribute</code>\n<ul>\n<li><code>tf.distribute.experimental.ParameterServerStrategy</code> now supports training with Keras <code>Model.fit</code> when used with <code>DatasetCreator</code>.</li>\n<li>Creating <code>tf.random.Generator</code> under <code>tf.distribute.Strategy</code> scopes is now allowed (except for <code>tf.distribute.experimental.CentralStorageStrategy</code> and <code>tf.distribute.experimental.ParameterServerStrategy</code>). Different replicas will get different random-number streams.</li>\n</ul>\n</li>\n<li>TPU embedding support\n<ul>\n<li>Added <code>profile_data_directory</code> to <code>EmbeddingConfigSpec</code> in <code>_tpu_estimator_embedding.py</code>. This allows embedding lookup statistics gathered at runtime to be used in embedding layer partitioning decisions.</li>\n</ul>\n</li>\n<li>PluggableDevice\n<ul>\n<li>Third-party devices can now connect to TensorFlow as plug-ins through <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api.md\">StreamExecutor C API</a>.\nand <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.md\">PluggableDevice</a> interface.\n<ul>\n<li>Add custom ops and kernels through <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20190814-kernel-and-op-registration.md\">kernel and op registration C API</a>.</li>\n<li>Register custom graph optimization passes with <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20201027-modular-tensorflow-graph-c-api.md\">graph optimization C API</a>.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"https://github.com/oneapi-src/oneDNN\">oneAPI Deep Neural Network Library (oneDNN)</a> CPU performance optimizations from <a href=\"https://software.intel.com/content/www/us/en/develop/articles/intel-optimization-for-tensorflow-installation-guide.html\">Intel-optimized TensorFlow</a> are now available in the official x86-64 Linux and Windows builds.\n<ul>\n<li>They are off by default. Enable them by setting the environment variable <code>TF_ENABLE_ONEDNN_OPTS=1</code>.</li>\n<li>We do not recommend using them in GPU systems, as they have not been sufficiently tested with GPUs yet.</li>\n</ul>\n</li>\n<li>TensorFlow pip packages are now built with CUDA11.2 and cuDNN 8.1.0</li>\n</ul>\n<h2>Breaking Changes</h2>\n<ul>\n<li>The <code>TF_CPP_MIN_VLOG_LEVEL</code> environment variable has been renamed to to <code>TF_CPP_MAX_VLOG_LEVEL</code> which correctly describes its effect.</li>\n</ul>\n<h2>Bug Fixes and Other Changes</h2>\n<ul>\n<li><code>tf.keras</code>:\n<ul>\n<li>Preprocessing layers API consistency changes:\n<ul>\n<li><code>StringLookup</code> added <code>output_mode</code>, <code>sparse</code>, and <code>pad_to_max_tokens</code> arguments with same semantics as <code>TextVectorization</code>.</li>\n<li><code>IntegerLookup</code> added <code>output_mode</code>, <code>sparse</code>, and <code>pad_to_max_tokens</code> arguments with same semantics as <code>TextVectorization</code>. Renamed <code>max_values</code>, <code>oov_value</code> and <code>mask_value</code> to <code>max_tokens</code>, <code>oov_token</code> and <code>mask_token</code> to align with <code>StringLookup</code> and <code>TextVectorization</code>.</li>\n<li><code>TextVectorization</code> default for <code>pad_to_max_tokens</code> switched to False.</li>\n<li><code>CategoryEncoding</code> no longer supports <code>adapt</code>, <code>IntegerLookup</code> now supports equivalent functionality. <code>max_tokens</code> argument renamed to <code>num_tokens</code>.</li>\n<li><code>Discretization</code> added <code>num_bins</code> argument for learning bins boundaries through calling <code>adapt</code> on a dataset. Renamed <code>bins</code> argument to <code>bin_boundaries</code> for specifying bins without <code>adapt</code>.</li>\n</ul>\n</li>\n<li>Improvements to model saving/loading:\n<ul>\n<li><code>model.load_weights</code> now accepts paths to saved models.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\">tensorflow-cpu's changelog</a>.</em></p>\n<blockquote>\n<h1>Release 2.5.0</h1>\n<!-- raw HTML omitted -->\n<h2>Breaking Changes</h2>\n<ul>\n<li>\n<!-- raw HTML omitted -->\n</li>\n<li>The <code>TF_CPP_MIN_VLOG_LEVEL</code> environment variable has been renamed to to\n<code>TF_CPP_MAX_VLOG_LEVEL</code> which correctly describes its effect.</li>\n</ul>\n<h2>Known Caveats</h2>\n<ul>\n<li><!-- raw HTML omitted --></li>\n<li><!-- raw HTML omitted --></li>\n<li><!-- raw HTML omitted --></li>\n</ul>\n<h2>Major Features and Improvements</h2>\n<ul>\n<li>\n<p><!-- raw HTML omitted --></p>\n</li>\n<li>\n<p><!-- raw HTML omitted --></p>\n</li>\n<li>\n<p>TPU embedding support</p>\n<ul>\n<li>Added <code>profile_data_directory</code> to <code>EmbeddingConfigSpec</code> in\n<code>_tpu_estimator_embedding.py</code>. This allows embedding lookup statistics\ngathered at runtime to be used in embedding layer partitioning decisions.</li>\n</ul>\n</li>\n<li>\n<p><code>tf.keras.metrics.AUC</code> now support logit predictions.</p>\n</li>\n<li>\n<p>Creating <code>tf.random.Generator</code> under <code>tf.distribute.Strategy</code> scopes is now allowed (except for <code>tf.distribute.experimental.CentralStorageStrategy</code> and <code>tf.distribute.experimental.ParameterServerStrategy</code>). Different replicas will get different random-number streams.</p>\n</li>\n<li>\n<p><code>tf.data</code>:</p>\n<ul>\n<li>tf.data service now supports strict round-robin reads, which is useful\nfor synchronous training workloads where example sizes vary. With strict\nround robin reads, users can guarantee that consumers get similar-sized\nexamples in the same step.</li>\n<li>tf.data service now supports optional compression. Previously data would\nalways be compressed, but now you can disable compression by passing\n<code>compression=None</code> to <code>tf.data.experimental.service.distribute(...)</code>.</li>\n<li><code>tf.data.Dataset.batch()</code> now supports <code>num_parallel_calls</code> and\n<code>deterministic</code> arguments. <code>num_parallel_calls</code> is used to indicate that\nmultiple input batches should be computed in parallel. With\n<code>num_parallel_calls</code> set, <code>deterministic</code> is used to indicate that\noutputs can be obtained in the non-deterministic order.</li>\n<li>Options returned by <code>tf.data.Dataset.options()</code> are no longer mutable.</li>\n<li>tf.data input pipelines can now be executed in debug mode, which\ndisables any asynchrony, parallelism, or non-determinism and forces\nPython execution (as opposed to trace-compiled graph execution) of\nuser-defined functions passed into transformations such as <code>map</code>. The\ndebug mode can be enabled through <code>tf.data.experimental.enable_debug_mode()</code>.</li>\n</ul>\n</li>\n<li>\n<p><code>tf.lite</code></p>\n<ul>\n<li>Enabled the new MLIR-based quantization backend by default\n<ul>\n<li>The new backend is used for 8 bits full integer post-training quantization</li>\n<li>The new backend removes the redundant rescales and fixes some bugs (shared weight/bias, extremely small scales, etc)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d\"><code>a4dfb8d</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49124\">#49124</a> from tensorflow/mm-cherrypick-tf-data-segfault-fix-...</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/2107b1dc414edb3fc78e632bca4f4936171093b2\"><code>2107b1d</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49116\">#49116</a> from tensorflow-jenkins/version-numbers-2.5.0-17609</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/16b813906fcb46306aef29a04ddd0cbdb4e77918\"><code>16b8139</code></a> Update snapshot_dataset_op.cc</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/86a0d86cb5da6a28b78ea7f886ec2831d23f6d6b\"><code>86a0d86</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49126\">#49126</a> from geetachavan1/cherrypicks_X9ZNY</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/9436ae693ef66a9efb7e7e7888134173d9a0821d\"><code>9436ae6</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49128\">#49128</a> from geetachavan1/cherrypicks_D73J5</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/6b2bf99cd9336026689579b683a709c5efcb4ae9\"><code>6b2bf99</code></a> Validate that a and b are proper sparse tensors</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/c03ad1a46d5b3f23df67dad03185a0ee16020c96\"><code>c03ad1a</code></a> Ensure validation sticks in banded_triangular_solve_op</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/12a6ead7ac968c402feb85ce0a8069ccbc6bf735\"><code>12a6ead</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49120\">#49120</a> from geetachavan1/cherrypicks_KJ5M9</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/b67f5b8a0a098c34c71c679aa46480035c46886e\"><code>b67f5b8</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/49118\">#49118</a> from geetachavan1/cherrypicks_BIDTR</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/a13c0ade86295bd3a8356b4b8cc980cf0c5e70e0\"><code>a13c0ad</code></a> [tf.data][cherrypick] Fix snapshot segfault when using repeat and prefecth</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tensorflow/tensorflow/compare/v2.4.0...v2.5.0\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow-cpu&package-manager=pip&previous-version=2.4.0&new-version=2.5.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 317, "title": "Bump dns-packet from 1.3.1 to 1.3.4 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/317", "html_url": "https://github.com/backtick-se/cowait/pull/317", "diff_url": "https://github.com/backtick-se/cowait/pull/317.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/317.patch", "merged_at": "2021-06-22T09:56:34Z"}, "body": "Bumps [dns-packet](https://github.com/mafintosh/dns-packet) from 1.3.1 to 1.3.4.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/mafintosh/dns-packet/commit/ebdf849da5dc0d96836e87628349776c623c5be7\"><code>ebdf849</code></a> 1.3.4</li>\n<li><a href=\"https://github.com/mafintosh/dns-packet/commit/ac578722f2707310b841b65aae61d6332f8882a1\"><code>ac57872</code></a> move all allocUnsafes to allocs for easier maintenance</li>\n<li><a href=\"https://github.com/mafintosh/dns-packet/commit/c64c9507e51532c9e9a3cbefa146a134ecc025fd\"><code>c64c950</code></a> 1.3.3</li>\n<li><a href=\"https://github.com/mafintosh/dns-packet/commit/0598ba19d18da4568b32415e60a9629061b3c45c\"><code>0598ba1</code></a> fix .. in encodingLength</li>\n<li><a href=\"https://github.com/mafintosh/dns-packet/commit/010aedb33c1ee8c3f558db5249c1d46e2bd7a101\"><code>010aedb</code></a> 1.3.2</li>\n<li><a href=\"https://github.com/mafintosh/dns-packet/commit/0d0d593f8df4e2712c43957a6c62e95047f12b2d\"><code>0d0d593</code></a> backport encodingLength fix to v1</li>\n<li>See full diff in <a href=\"https://github.com/mafintosh/dns-packet/compare/v1.3.1...v1.3.4\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=dns-packet&package-manager=npm_and_yarn&previous-version=1.3.1&new-version=1.3.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 316, "title": "Fix CVE\u20132020\u201328275", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/316", "html_url": "https://github.com/backtick-se/cowait/pull/316", "diff_url": "https://github.com/backtick-se/cowait/pull/316.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/316.patch", "merged_at": null}, "body": "<h3>Debricked has created this Pull Request to remediate CVE\u20132020\u201328275.</h3></br>\n                    This pull request could introduce breaking changes, please review the changes under the tab <strong>Files changed</strong> above before merging.\n                </br></br>This message is a <strong>work in progress</strong>; more information about these changes and how they fix the vulnerability coming in future iterations. Until then, check out the CVE details at <a href=\"https://app.debricked.com/en/service/vulnerability/204989?repositoryId=11964&commitId=324968\">Debricked</a>."}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 314, "title": "Bump lodash from 4.17.19 to 4.17.21 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/314", "html_url": "https://github.com/backtick-se/cowait/pull/314", "diff_url": "https://github.com/backtick-se/cowait/pull/314.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/314.patch", "merged_at": "2021-05-10T14:41:16Z"}, "body": "Bumps [lodash](https://github.com/lodash/lodash) from 4.17.19 to 4.17.21.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/lodash/lodash/commit/f299b52f39486275a9e6483b60a410e06520c538\"><code>f299b52</code></a> Bump to v4.17.21</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/c4847ebe7d14540bb28a8b932a9ce1b9ecbfee1a\"><code>c4847eb</code></a> Improve performance of <code>toNumber</code>, <code>trim</code> and <code>trimEnd</code> on large input strings</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/3469357cff396a26c363f8c1b5a91dde28ba4b1c\"><code>3469357</code></a> Prevent command injection through <code>_.template</code>'s <code>variable</code> option</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/ded9bc66583ed0b4e3b7dc906206d40757b4a90a\"><code>ded9bc6</code></a> Bump to v4.17.20.</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/63150ef7645ac07961b63a86490f419f356429aa\"><code>63150ef</code></a> Documentation fixes.</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/00f0f62a979d2f5fa0287c06eae70cf9a62d8794\"><code>00f0f62</code></a> test.js: Remove trailing comma.</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/846e434c7a5b5692c55ebf5715ed677b70a32389\"><code>846e434</code></a> Temporarily use a custom fork of <code>lodash-cli</code>.</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/5d046f39cbd27f573914768e3b36eeefcc4f1229\"><code>5d046f3</code></a> Re-enable Travis tests on <code>4.17</code> branch.</li>\n<li><a href=\"https://github.com/lodash/lodash/commit/aa816b36d402a1ad9385142ce7188f17dae514fd\"><code>aa816b3</code></a> Remove <code>/npm-package</code>.</li>\n<li>See full diff in <a href=\"https://github.com/lodash/lodash/compare/4.17.19...4.17.21\">compare view</a></li>\n</ul>\n</details>\n<details>\n<summary>Maintainer changes</summary>\n<p>This version was pushed to npm by <a href=\"https://www.npmjs.com/~bnjmnt4n\">bnjmnt4n</a>, a new releaser for lodash since your current version.</p>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=lodash&package-manager=npm_and_yarn&previous-version=4.17.19&new-version=4.17.21)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 313, "title": "Bump hosted-git-info from 2.8.8 to 2.8.9 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/313", "html_url": "https://github.com/backtick-se/cowait/pull/313", "diff_url": "https://github.com/backtick-se/cowait/pull/313.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/313.patch", "merged_at": "2021-05-10T14:41:07Z"}, "body": "Bumps [hosted-git-info](https://github.com/npm/hosted-git-info) from 2.8.8 to 2.8.9.\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/npm/hosted-git-info/blob/v2.8.9/CHANGELOG.md\">hosted-git-info's changelog</a>.</em></p>\n<blockquote>\n<h2><a href=\"https://github.com/npm/hosted-git-info/compare/v2.8.8...v2.8.9\">2.8.9</a> (2021-04-07)</h2>\n<h3>Bug Fixes</h3>\n<ul>\n<li>backport regex fix from <a href=\"https://github-redirect.dependabot.com/npm/hosted-git-info/issues/76\">#76</a> (<a href=\"https://github.com/npm/hosted-git-info/commit/29adfe5\">29adfe5</a>), closes <a href=\"https://github-redirect.dependabot.com/npm/hosted-git-info/issues/84\">#84</a></li>\n</ul>\n<p><!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/npm/hosted-git-info/commit/8d4b3697d79bcd89cdb36d1db165e3696c783a01\"><code>8d4b369</code></a> chore(release): 2.8.9</li>\n<li><a href=\"https://github.com/npm/hosted-git-info/commit/29adfe5ef789784c861b2cdeb15051ec2ba651a7\"><code>29adfe5</code></a> fix: backport regex fix from <a href=\"https://github-redirect.dependabot.com/npm/hosted-git-info/issues/76\">#76</a></li>\n<li>See full diff in <a href=\"https://github.com/npm/hosted-git-info/compare/v2.8.8...v2.8.9\">compare view</a></li>\n</ul>\n</details>\n<details>\n<summary>Maintainer changes</summary>\n<p>This version was pushed to npm by <a href=\"https://www.npmjs.com/~nlf\">nlf</a>, a new releaser for hosted-git-info since your current version.</p>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=hosted-git-info&package-manager=npm_and_yarn&previous-version=2.8.8&new-version=2.8.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 312, "title": "Add resource limit arguments to cowait test", "labels": [{"id": 2465797289, "node_id": "MDU6TGFiZWwyNDY1Nzk3Mjg5", "url": "https://api.github.com/repos/backtick-se/cowait/labels/cli", "name": "cli", "color": "ffc8a8", "default": false, "description": "Command Line Tool"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/312", "html_url": "https://github.com/backtick-se/cowait/pull/312", "diff_url": "https://github.com/backtick-se/cowait/pull/312.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/312.patch", "merged_at": "2021-05-10T11:11:12Z"}, "body": "resolve #311 "}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 310, "title": "Bump url-parse from 1.4.7 to 1.5.1 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/310", "html_url": "https://github.com/backtick-se/cowait/pull/310", "diff_url": "https://github.com/backtick-se/cowait/pull/310.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/310.patch", "merged_at": "2021-05-10T09:14:35Z"}, "body": "Bumps [url-parse](https://github.com/unshiftio/url-parse) from 1.4.7 to 1.5.1.\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/eb6d9f51e395b7e47bf2594e457d541db21c713b\"><code>eb6d9f5</code></a> [dist] 1.5.1</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/750d8e8a9d45dbce9ff09759f0fe4564cdd47d74\"><code>750d8e8</code></a> [fix] Fixes relative path resolving <a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/199\">#199</a> <a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/200\">#200</a> (<a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/201\">#201</a>)</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/3ac777474ba5dc48a7e33771cbb311fc6f69bef8\"><code>3ac7774</code></a> [test] Make test consistent for browser testing</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/267a0c6f7ef1a58271be61611c5103daace602c9\"><code>267a0c6</code></a> [dist] 1.5.0</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/d1e7e8822f26e8a49794b757123b51386325b2b0\"><code>d1e7e88</code></a> [security] More backslash fixes (<a href=\"https://github-redirect.dependabot.com/unshiftio/url-parse/issues/197\">#197</a>)</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/d99bf4cf259b7378c855f786edc253e70405ffdc\"><code>d99bf4c</code></a> [ignore] Remove npm-debug.log from .gitignore</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/422c8b5e4cac6a79cd35b4e86731476dcbeec7e4\"><code>422c8b5</code></a> [pkg] Replace nyc with c8</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/933809d630c7b21399b4e5df59fccccd80033b21\"><code>933809d</code></a> [pkg] Move coveralls to dev dependencies</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/190b2168035899a2a88f2dc2625963bf7e2f338f\"><code>190b216</code></a> [pkg] Add .npmrc</li>\n<li><a href=\"https://github.com/unshiftio/url-parse/commit/ce3783f4ea25753cfa36376769c14e4e2fe6ea80\"><code>ce3783f</code></a> [test] Do not test on all available versions of Edge and Safari</li>\n<li>Additional commits viewable in <a href=\"https://github.com/unshiftio/url-parse/compare/1.4.7...1.5.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=url-parse&package-manager=npm_and_yarn&previous-version=1.4.7&new-version=1.5.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 309, "title": "Fix RPC call race condition", "labels": [{"id": 1910804761, "node_id": "MDU6TGFiZWwxOTEwODA0NzYx", "url": "https://api.github.com/repos/backtick-se/cowait/labels/bug", "name": "bug", "color": "f2eb30", "default": true, "description": "Bug report"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/309", "html_url": "https://github.com/backtick-se/cowait/pull/309", "diff_url": "https://github.com/backtick-se/cowait/pull/309.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/309.patch", "merged_at": "2021-05-10T09:14:53Z"}, "body": "Refactor to avoid a potential race condition where the `RpcCall` object may be deleted from the pending call list before reaching the `wrap_future`\r\n\r\nresolve #308 "}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 307, "title": "Bump ssri from 6.0.1 to 6.0.2 in /cloud", "labels": [{"id": 2125220463, "node_id": "MDU6TGFiZWwyMTI1MjIwNDYz", "url": "https://api.github.com/repos/backtick-se/cowait/labels/dependencies", "name": "dependencies", "color": "0366d6", "default": false, "description": "Pull requests that update a dependency file"}, {"id": 2578623548, "node_id": "MDU6TGFiZWwyNTc4NjIzNTQ4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/javascript", "name": "javascript", "color": "168700", "default": false, "description": "Pull requests that update Javascript code"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/307", "html_url": "https://github.com/backtick-se/cowait/pull/307", "diff_url": "https://github.com/backtick-se/cowait/pull/307.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/307.patch", "merged_at": "2021-05-10T09:14:23Z"}, "body": "Bumps [ssri](https://github.com/npm/ssri) from 6.0.1 to 6.0.2.\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/npm/ssri/blob/v6.0.2/CHANGELOG.md\">ssri's changelog</a>.</em></p>\n<blockquote>\n<h2><a href=\"https://github.com/zkat/ssri/compare/v6.0.1...v6.0.2\">6.0.2</a> (2021-04-07)</h2>\n<h3>Bug Fixes</h3>\n<ul>\n<li>backport regex change from 8.0.1 (<a href=\"https://github.com/zkat/ssri/commit/b30dfdb\">b30dfdb</a>), closes <a href=\"https://github-redirect.dependabot.com/zkat/ssri/issues/19\">#19</a></li>\n</ul>\n<p><!-- raw HTML omitted --><!-- raw HTML omitted --></p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/npm/ssri/commit/b7c8c7c61db89aeb9fbf7596c0ef17071bc216ef\"><code>b7c8c7c</code></a> chore(release): 6.0.2</li>\n<li><a href=\"https://github.com/npm/ssri/commit/b30dfdb00bb94ddc49a25a85a18fb27afafdfbb1\"><code>b30dfdb</code></a> fix: backport regex change from 8.0.1</li>\n<li>See full diff in <a href=\"https://github.com/npm/ssri/compare/v6.0.1...v6.0.2\">compare view</a></li>\n</ul>\n</details>\n<details>\n<summary>Maintainer changes</summary>\n<p>This version was pushed to npm by <a href=\"https://www.npmjs.com/~nlf\">nlf</a>, a new releaser for ssri since your current version.</p>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=ssri&package-manager=npm_and_yarn&previous-version=6.0.1&new-version=6.0.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/backtick-se/cowait/network/alerts).\n\n</details>"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 304, "title": "Cowait Deploy command", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/304", "html_url": "https://github.com/backtick-se/cowait/pull/304", "diff_url": "https://github.com/backtick-se/cowait/pull/304.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/304.patch", "merged_at": "2021-06-07T12:14:09Z"}, "body": "Leverage Restart Policies to implement `cowait deploy`"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 303, "title": "Fix ContainerTask", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/303", "html_url": "https://github.com/backtick-se/cowait/pull/303", "diff_url": "https://github.com/backtick-se/cowait/pull/303.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/303.patch", "merged_at": "2021-04-20T19:52:04Z"}, "body": "resolve #301 "}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 302, "title": "cowait test correctly mounts the context root when running locally", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/302", "html_url": "https://github.com/backtick-se/cowait/pull/302", "diff_url": "https://github.com/backtick-se/cowait/pull/302.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/302.patch", "merged_at": "2021-04-20T19:49:54Z"}, "body": "- Add `--mount`/`--no-mount` flags to `cowait test`.\r\n\r\nIf `--mount` is set, the working directory is mounted into the container when running in Docker. Mount is enabled by default."}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 300, "title": "Test command fixes", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/300", "html_url": "https://github.com/backtick-se/cowait/pull/300", "diff_url": "https://github.com/backtick-se/cowait/pull/300.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/300.patch", "merged_at": "2021-04-12T13:02:05Z"}, "body": "- Remove built-in `build` and `push` functionality from `cowait test`\r\n- Pass `environment` and `volumes` configuration when running `cowait test`\r\n- Implement `KubernetesProvider.wait()` so that tests may run on kubernetes clusters. `wait()` should probably be removed or changes, since its API currently breaks the convention of passing task ids to `ClusterProvider` methods."}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 299, "title": "Traefik2 fixes", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/299", "html_url": "https://github.com/backtick-se/cowait/pull/299", "diff_url": "https://github.com/backtick-se/cowait/pull/299.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/299.patch", "merged_at": "2021-04-06T21:07:40Z"}, "body": "- Cast port numbers to integers\r\n- Properly clean up IngressRoutes on exit\r\n- Add support for configuring certificate resolvers"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 298, "title": "Notebook/Clientfs Integration", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/298", "html_url": "https://github.com/backtick-se/cowait/pull/298", "diff_url": "https://github.com/backtick-se/cowait/pull/298.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/298.patch", "merged_at": "2021-05-06T14:39:56Z"}, "body": "Kubernetes notebooks now require `clientfs`. For now, `clientfs-darwin` or `clientfs-linux` must exist in the working directory."}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/why-cowait.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/type-system.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/task-lifecycle-methods.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/built-in-tasks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/tasks/remote-procedure-calls.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/custom-dockerfile.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/setup/configuration.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/contributing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/overview.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/no-scheduler.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/everything-is-a-task.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/task-hierarchy.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/core-concepts/engines.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/routing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/cluster-management.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/setup.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/testing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/kubernetes/pushing-and-running.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/first-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/dependencies.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/building-and-pushing.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/tests.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/next-steps.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/installation.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/get-started/asyncio.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/spark.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dashboard.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/dask.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/databricks.md", "/Users/shabo/Documents/Backtick/exjobb/cowait/docs/extras/notebook-integration.md"], "contents": ["---\ntitle: Why Cowait?\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. cowait build will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```bash\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Type system\n---\n\nCustomizable type checking & input/output serialization\n\n## Built in types\n\nCowait provides a simple system for defining data types. By annotating task functions and [RPC methods](/docs/tasks/remote-procedure-calls/) with these types, Cowait can perform automatic type checking and serialization/deserialization of complex objects.\n\n```python:title=example.py\nfrom cowait import task\nfrom cowait.types import Dict\n\nTypecheckedDict = Dict({\n    'text': str,\n    'number': int,\n})\n\n@task\ndef test_task(input_dict: TypecheckedDict) -> int:\n    print(input_dict['text'])\n    return input_dict['number']\n```\n\n### Input Values\n\nIf you need to pass any value that is not a simple type (str, int, float, boolean, list, dict), you must annotate the argument. This tells the runtime how to deserialize the object before passing it to the task function. Because the incoming object is serialized, its type can not be automatically inferred.\n\n### Return Values\n\nType information for result serialization can usually be automatically inferred from the returned object. However, to benefit from type checking, the return type should be annotated on the task function.\n\n## Custom Types\n\nCustom types can be implemented by creating a subclass of `cowait.types.Type` and implementing its `validate()`, `serialize()` and `deserialize()` methods. To register it with the type system, decorate it with the `@TypeAlias()` decorator.\n\n```python:title=datetime_type.py\nfrom cowait.types import Type, TypeAlias\n\n@TypeAlias(datetime)\nclass DateTime(Type):\n    \"\"\" Python datetime object serialized as an ISO8601 string \"\"\"\n\n    def validate(self, value: str, name: str) -> None:\n        if isinstance(value, datetime):\n            return\n\n        if not isinstance(value, str):\n            raise ValueError('Expected ISO8601 datetime')\n\n        datetime.fromisoformat(value)\n\n    def serialize(self, value: datetime) -> str:\n        return value.isoformat()\n\n    def deserialize(self, value: str) -> datetime:\n        return datetime.fromisoformat(value)\n```\n", "---\ntitle: Task Lifecycle Methods\n---\n\nTasks have several methods that can be overridden to customize their behavior. Task lifecycle methods can be used to accomplish more control when implementing for example task inheritance. **However, this functionality is mostly intended for more advanced use cases, and should be avoided if possible.**\n\nTask lifecycle methods are added as class methods on tasks.\n\n## init\n\nTasks should never override the default python `__init__()`\u001b constructor, so the framework provides its own initialization function, ` init()`. It is called before `before()` and must be a synchronous python function.\n\n```python\ndef init(self) -> None:\n    pass\n```\n\n## before\n\nThe `before()` hook is called immediately before `run()`. All task inputs are passed as a dict, and `before()` can be used to modify the task inputs before the `run()` function is executed.\n\n```python\n# inputs can be modified before run() is executed:\nasync def before(self, inputs: dict) -> dict:\n    inputs['new_input'] = 2\n    return inputs\n```\n\n## after\n\nThe `after()` hook can be used to perform actions after the task has finished, such as cleaning up any running child tasks.\n\n```python\nasync def after(self, inputs: dict) -> None:\n    return\n```\n", "---\ntitle: Built in tasks\n---\n\nSome useful library tasks that can simplify your life.\n\n## ShellTask\n\nShellTask can be used to run any shell command. `stdout` and `stderr` is forwarded to the task log.\n\n### `cowait.tasks.shell.ShellTask`\n\n| Input   |  Type  |              Description |\n| ------- | :----: | -----------------------: |\n| command | string | Shell command to execute |\n| env     |  dict  |              Environment |\n\n**Returns**: shell command return code (integer)\n\n```python:title=example-ls.py\nfrom cowait.tasks.shell import ShellTask\n\n@task\nasync def MyTask():\n    await ShellTask(command='ls')\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.shell --input command=ls\n```\n\n## ContainerTask\n\n`ContainerTask` can be used to launch and monitor any Docker container. This can be useful for setting up side-car containers. Container logs are forwarded to the task log.\n\n### `cowait.tasks.container.ContainerTask`\n\n| Input  |    Type    |           Description |\n| ------ | :--------: | --------------------: |\n| name   |   string   |             Task Name |\n| image  |   string   |     Docker image name |\n| env    |    dict    | Environment variables |\n| routes | Route Dict |                       |\n| ports  | Port Dict  |                       |\n| cpu    |   string   |        CPU allocation |\n| memory |   string   |     Memory allocation |\n\n```python:title=mongo.py\nfrom cowait.tasks.container import ContainerTask\n\n@task\nasync def MyTask():\n    await ContainerTask(\n      name=\"mongodb-task\"\n      image=\"mongo\"\n    )\n```\n\n```shell\n# Example using the CLI\ncowait run cowait.tasks.container --input name=\"mongodb-task\" -i image=mongo\n```\n", "---\ntitle: Remote Procedure Calls (RPC)\n---\n\nAdvanced task communication\n\n## Introduction\n\nCowait provides a simple RPC system for advanced communication between tasks. RPC-callable methods are defined directly on the task classes and marked with the `@rpc` decorator. RPC calls can accept any JSON serializable arguments and return any JSON serializable value.\n\nRPC communication can be used to send commands or updates to and from tasks, after they've been created. Defining RPC methods on tasks is a good place to introduce side effects to your tasks.\n\n## Parent to Child RPC\n\nThe parent task can call RPC methods on child tasks by invoking methods on the remote task reference object.\n\n1. Define an RPC method on your child task\n\n```python:title=rpc_child.py\nfrom cowait.tasks Task, rpc, sleep\n\nclass RpcChild(Task):\n    async def run(self):\n        # wait forever\n        while True:\n            await sleep(1)\n\n    @rpc\n    async def some_rpc_call(self):\n        return 1337\n```\n\n2. Call it from the parent, after saving a reference to the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks Task\nfrom rpc_child import RpcChild # your child task\n\nclass RpcParent(Task):\n    async def run(self):\n        child = RpcChild()\n        result = await child.some_rpc_call()\n        print('RPC result:', result)\n        return result\n```\n\n## Child to parent RPC\n\nSimilarly, child tasks can call RPC methods on their parent task by invoking methods on `self.parent`\n\n1. Have your parent task create the child task.\n\n```python:title=rpc_parent.py\nfrom cowait.tasks import Task, rpc, sleep\nfrom rpc_child import RpcChild\n\nclass RpcParent(Task):\n    async def run(self):\n        self.called = False\n\n        # spawn child and wait for it to make an RPC call:\n        child = RpcChild()\n        while not self.called:\n            await sleep(1)\n\n    @rpc\n    async def set_called(self):\n        self.called = True\n```\n\n2. Call the parent's RPC method through `self.parent`:\n\n```python:title=rpc_child.py\nfrom cowait.tasks import Task\n\nclass RpcChild(Task):\n    async def run(self):\n        # rpc call to parent:\n        await self.parent.set_called()\n```\n", "---\ntitle: Custom Dockerfile\n---\n\nDetails on docker, configuration and custom images. The default Cowait Dockerfile can be found [here](https://github.com/backtick-se/cowait/blob/master/Dockerfile).\n\n## Overview\n\nTo use your own custom docker image, simply add a Dockerfile in the root of your project. `cowait build` will automatically build your task using your own dockerfile.\n\nExample file structure:\n\n```\nhello-world\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 Dockerfile\n```\n\n## Extending the default Dockerfile\n\nThe easiest way to customize the task image is to extend the default task image. This should be sufficient for most use cases, such as installing extra dependencies or adding files.\n\n```Dockerfile\n# Dockerfile\nFROM cowait/task\n\nRUN echo \"Your custom command\"\n```\n\n## Writing your own Dockerfile\n\nIf extending cowait/task is not possible for your use case, you could also create a completely custom docker image. Improvements to this process are planned.\n\nYou need to install Cowait in the Dockerfile. In addition there a few things you need to do (you can look at the [default](https://github.com/backtick-se/cowait/blob/master/Dockerfile) Cowait Dockerfile for inspiration):\n\n- Ensure Python 3.6+ is installed\n- Install Cowait with `pip`\n- Create a directory call `/var/task` and set it as `WORKDIR`\n- Set `python -Bum cowait.exec` as entrypoint\n", "---\ntitle: Configuration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Quick Start\n---\n\nThis quick start assumes you have intermediate programming skills and are familiar with python, asyncio and Docker.\n\n## Cowait quick start\n\n1. Install cowait\n\n```shell\npip install cowait\n```\n\n2. Pull the base Cowait image. Don't worry - you can use your own Dockerfile if you want to.\n\n```shell\ndocker pull cowait/task\n```\n\n3. Create a new Cowait task, `hello.py`:\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello():\n    print(\"Hello World\")\n\n```\n\n4. Run your Cowait task, this spins up a new docker container.\n\n```shell\ncowait run hello\n```\n\n5. Start the Cowait UI\n\n```shell\ncowait agent\n```\n\nYou can visit the UI at `http://localhost:1339`\n\n6. If you run your task again, it should show up in the UI.\n\n## Asyncio, Inputs & Outputs\n\n1. Create a new file `sleep.py`.\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep():\n    for i in range(5):\n      await asyncio.sleep(1)\n      print(\"slept\", i + 1)\n\n```\n\n2. Modify the Sleep task to take duration as an input. Also return how long it slept.\n\n   - Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n   - Outputs can be consumed by other tasks or systems.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5, **inputs):\n    for i in range(duration):\n        await asyncio.sleep(1)\n        print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\n3. The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run sleep --input duration=7\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to paralellize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\n```shell\ncowait run parallel\n```\n\nNice! Here's an illustration of what you just ran, in terms of containers:\n\n![Parallel Docker Illustration](./images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Contributing\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Overview\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: No scheduler\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Everything is a task\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Task hierarchy\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Engines\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Routing\n---\n\nAutomated HTTP proxying for tasks\n\n## Using Traefik\n\nCowait can automatically integrate with a [Traefik](https://traefik.io/traefik/) reverse proxy if it is deployed in your cluster.\n\n- Deploy Traefik to your cluster.\n- Point a wildcard subdomain \\*.cluster.yourdomain.com to the traefik service\n\nTasks with route mappings will be available at `task123.cluster.yourdomain.com`\n", "---\ntitle: Cluster Management\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Setup\n---\n\n## Permissions\n\nTask pods must be able to manage the cluster in order to schedule other tasks. Currently, tasks are deployed in the default namespace and use the default service account.\n\n### Basic\n\nThe most basic set of permissions allow tasks to create, list and destroy pods. This allows tasks to schedule other tasks on the cluster. This should be sufficient if you do not wish to use any automated routing features.\n\n```yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: task-basic-permissions\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"create\", \"list\", \"delete\", \"deletecollection\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: default-sa-task-permissions\nsubjects:\n  - kind: ServiceAccount\n    name: default\n    namespace: default\nroleRef:\n  kind: ClusterRole\n  name: task-basic-permissions\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### Extended\n\nIf you wish to use routing features, your task pods also need permissions to create, list and destroy ingresses and services.\nApply the [default configuration](https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml) with:\n\n```shell\nkubectl apply -f https://raw.githubusercontent.com/backtick-se/cowait/master/k8setup.yml\n```\n\n## Repository Secrets\n\nIf you would like to pull images from a private repository, you must create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) containing the repository credentials.\n", "---\ntitle: Testing on Kubernetes\n---\n\n## Prerequisites\n\n- Basic Kubernetes knowledge\n- A basic understanding of `cowait build` and `cowait push`, see [building and pushing](/docs/get-started/building-and-pushing/)\n- Knowledge of `cowait.yml`, see [Configuration](/docs/setup/configuration/)\n- A configured kubernetes cluster, see [Cluster Management](/docs/kubernetes/cluster-management/).\n\n## Testing on Kubernetes\n\nTo make sure your tasks work in a cluster environment, Cowait provides running tests on Kubernetes via the `--cluster` argument to `cowait test`.\n\n```\ncowait test --cluster my_kubernetes\n```\n\nFurther, you can include the `--push` argument, to build and push your image to Kubernetes before running. This is just a convenience - you could also just do `cowait build` and `cowait push` before running `cowait test`. Either way, you need to make sure that your recent changes are in the image on your docker registry so that Kubernetes picks up the corrent image.\n", "---\ntitle: Pushing & running\n---\n\nHow to push tasks to your registry so that they can later be run on Kubernetes.\n\n## Pushing\n\nBefore you can run anything on kubernetes, your task image must be pushed to a docker registry that can be accessed from the cluster. To push the image to a repository, you must define the image name in `cowait.yml`.\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  image: your-repo/task-image-name\n```\n\n1. Build your tasks into your image\n\n```shell\ncowait build\n```\n\n2. Make sure you're authenticated to your registry.\n\n```shell\ndocker login\n```\n\n3. Push the image\n\nThis will push the image to registry you defined in your `cowait.yml`\n\n```shell\ncowait push\n```\n\n## Configuring Pull Secrets\n\nIf your repository is not publicly available, you must create a kubernetes secret containing the authentication information. See the [kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).\n\nOnce you have created a secret, configure the kubernetes provider to use it for pulling images:\n\n```yml:title=cowait.yml\nversion: 1\ncowait:\n  kubernetes:\n    pull_secrets:\n      - your_secret_name\n```\n\n## Running\n\nYou should now be ready to run your task on the your cluster. To use the kubernetes task provider, simply use the `--provider` option to `cowait run` as follows. You may pass inputs and other options as you would normally.\n\n```shell\ncowait run your_task --provider kubernetes\n```\n", "---\ntitle: First steps\n---\n\nYour first steps into the world of Cowait.\n\n## Overview\n\nCowait organizes code into Tasks. A Task is essentially nothing more than a function, and just like your typical function, it can accept input arguments and return values. Similarly they may also invoke other tasks, with one key difference: a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\n## Tasks\n\nThe basic unit of execution in Cowait is the Task. Tasks can be implemented either as simple functions, or classes deriving from `cowait.Task`.\n\n### Creating a task\n\nCreate a new folder called `my-project` and a python file called `hello.py`. We assume you've managed to [install Cowait](/docs/get-started/installation/).\n\n```\nmy-project/\n  \u2514\u2500\u2500 hello.py\n```\n\n```python:title=hello.py\nfrom cowait import task\n\n# function style\n@task\nasync def Hello():\n    print('Hello World')\n```\n\n```python\nfrom cowait import Task\n\n# class style\nclass Hello(Task):\n    async def run(self):\n        print('Hello World')\n```\n\n### Running the task\n\nYou can now run your task. Unlike Python code that you execute directly, this will run inside a Docker Container. You can run your task like so:\n\n```shell\ncd my-project\ncowait run hello\n```\n\nYou should see something like this:\n\n```\n-- TASK ---------------------------------------------\n   task:       \"hello-plapdnoy\"\n   cluster:    \"docker\" {  }\n   image:      \"cowait/task\"\n   volumes:    { /var/task: { bind: { src: \"/Users/cowait-demo/my-project/demo\", mode: \"rw\" } } }\n-- TASK OUTPUT --------------------------------------\n15:53:28 hello * started with {  }\n15:53:28 hello = returned null\n15:53:28 hello   Hello World\n-----------------------------------------------------\n```\n\n### Volume Mounts\n\nBehind the scenes, Cowait uses Docker Volume Mounts to speed up local development. Notice that you did not have to build anything. This is because you pulled the base Cowait image in the installation process.\n\nIf you would like to build your Docker image with your added code, simply run:\n\n```shell\ncowait build\n```\n\n## Inputs & Outputs\n\nCowait tasks can accept inputs and return outputs.\n\n```python:title=hello.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Hello(name: str, **inputs):\n    print(\"Hello\", name)\n\n    return {\n        \"hello\": name,\n    }\n```\n\n- Inputs that you do not define explicitly in the function signature are passed in `**inputs`.\n- You can return whatever you would like, as long as it can be serialized. This work out of the box with python types (`str`, `int`, `float`, `boolean`, `list`, `dict`). You can also [create your own types](/docs/tasks/type-system/)\n- The Cowait CLI allows you to pass inputs when running your task:\n\n```shell\ncowait run hello --input name=world\n```\n\n## Notes\n\n- `hello` supplied to `cowait run` is the python module name. This module should contain exactly one task class. Modules can be single python files or subdirectories with **init**.py files.\n- The actual function/class name of the task does not matter when running from the CLI, only when importing and executing tasks from python.\n", "---\ntitle: Dashboard\n---\n\n## Cowait Agent\n\nThe Cowait Agent is capable of picking up information that happen within Cowait in real time. It hosts a UI that allows you to view and interact with your tasks.\n\nThe Agent is actually a Task in itself(!), and runs in a docker container.\n\nYou can start it with a simple CLI command:\n\n```shell\ncowait agent\n```\n\nYou should now be able to visit the Cowait UI at [http://localhost:1339](http://localhost:1339)\n\n## Looking at tasks and logs in the Dashboard\n\nIf you followed along the previous steps, you should have a project structure like this:\n\n```\nmy-project/\n  \u251c\u2500\u2500 sleep.py\n  \u251c\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n```\n\nIf you run your `hello`\u001b task again after the Cowait agent successfully started, it should show up in the UI:\n\n```shell\ncowait run hello\n```\n\n![Dashboard 1](../images/dashboard_overview.png)\n\nThe Cowait Agent Dashboard gives you an overview of the tasks you run and their statuses. It also shows you metadata and logs:\n\n![Dashboard 2](../images/dashboard_hello_task.png)\n\n## Parallel\n\nTry running your `parallel` task and explore the task hierarchy in the UI!\n\n```shell\ncowait run parallel\n```\n", "---\ntitle: Dependency management\n---\n\n## Adding dependencies\n\nCowait can automatically install Python dependencies as part of the build step. Adding a `requirements.txt` in your project root folder installs the requirements during `cowait build`.\n\n1. Add a `requirements.txt` to the root of your project\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u2514\u2500\u2500 sleep.py\n```\n\n2. Populate it\n\n```\npandas==1.2.4\n```\n\n3. Build\n\n```shell\ncowait build\n```\n\nCowait will identify the `requirements.txt` file and install dependencies in the build step using `pip install`. During local development, Cowait mounts your directory into the container. However, adding new dependencies requires you to build your image using `cowait build`.\n", "---\ntitle: Building and pushing\n---\n\n## Overview\n\nCowait comes with sensible defaults, but allows you to easily configure certain aspects of your environments, such as docker image names, kubernetes clusters and more.\n\nCowait uses a concept called Task Context, which is achieved through a simple `cowait.yml` file added to the root of your project.\n\n## Task Context\n\nA task context is defined as a directory containing a `cowait.yml` file. This directory will act as the root of a project. Everything in this folder is copied into the resulting docker image during the build step. If you have not created a `cowait.yml` file, the current working directory (when exectuing `cowait build`) will be used.\n\nExample:\n\n```\n/my_project\n  \u2514\u2500\u2500 cowait.yml\n  \u2514\u2500\u2500 hello.py\n  \u2514\u2500\u2500 parallel.py\n  \u2514\u2500\u2500 sleep.py\n```\n\nIn this case, `my_project` will be the context directory.\n\n## Cowait.yml\n\nIn a scenario when you want to run your task(s) on a remote machine or cluster, Cowait provides `cowait build` to package your code into a Docker image and `cowait push` to distribute it to docker registries.\n\nTo do this, you simply provide your docker image name (and registry) in `cowait.yml`:\n\n```yml\nversion: 1\ncowait:\n  image: docker.io/username/cowait-task\n```\n\nNow, if you run\n\n```shell\ncowait build\ncowait push\n```\n\nCowait will build your image and push it to the registry. You can use the shorthand `--push` to `cowait build` to push it after building completes:\n\n```shell\ncowait build --push\n```\n", "---\ntitle: Tests\n---\n\n## Overview\n\nCowait supports testing of tasks, asynchronous code and library code. Cowait uses [pytest](https://docs.pytest.org/en/6.2.x/).\n\nWriting tests for your asynchronous tasks is simple. The cowait test runner will spawn a task(!) that allows you to perform assertions on your code and other tasks.\n\nGood to know:\n\n- For tests to be picked up by the test runner, make sure to prefix your test files with `test_`.\n- Cowait will run **one** task that will execute all your tests, even if they're in different files.\n- Cowait will create real instances of your tasks if you create them from your test code.\n- Of course, you're free to import library code in the Cowait test runner to unit test smaller building blocks and functions.\n\n## Black box task testing\n\nIn this example, we have added a `test_sleep.py` file to our project:\n\n```\nmy-project/\n  \u251c\u2500\u2500 cowait.yml\n  \u251c\u2500\u2500 hello.py\n  \u251c\u2500\u2500 parallel.py\n  \u251c\u2500\u2500 requirements.txt\n  \u251c\u2500\u2500 sleep.py\n  \u2514\u2500\u2500 test_sleep.py\n```\n\n```python:title=test_sleep.py\nfrom sleep import Sleep\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n\n    assert result == {\n        'duration': 1,\n    }\n```\n\nTo run the test, use the Cowait CLI:\n\n```shell\ncowait test\n```\n\n## Testing functions and library code\n\nOf course, you can import your functions and library modules in the test task and write tests like you normally would (as long as the code is packaged into the same Docker image). Let's say you have a simple function that doesn't run any async code that you would like to test as well.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\ndef add(a: int, b: int):\n  return a + b\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nYou could simply import it in your test file and perform assertions like you normally would.\n\n```python:title=test_sleep.py\nfrom sleep import Sleep, add\n\ndef test_add():\n    assert add(1, 2) == 3\n\nasync def test_sleep():\n    result = await Sleep(duration=1)\n    assert result == {\n        'duration': 1,\n    }\n```\n\n```bash\n============================= test session starts ==============================\nplatform linux -- Python 3.7.10, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\nrootdir: /var/task, configfile: ../cowait/pytest.ini\nplugins: cov-2.11.1, alt-pytest-asyncio-0.5.4, sugar-0.9.4\ncollected 2 items\n\ntest_sleep.py ..                                                         [100%]\n\n============================== 2 passed in 4.19s ===============================\n```\n\nMoreover, you are free to create multiple files (`test_sleep.py`, `test_sleep2.py`). Cowait will pick up and run all defined tests. Tests will run in one Cowait task.\n\n## Testing reads and write of datasets\n\nIn this example we assume you are doing some transformations on a dataset on `s3`. Let's assume your task takes a fair amount of time, and it would be sad to see it fail after running for 4 hours. You have decided to solve this problem by writing a test for your task.\n\nLet's say your preprocessing task looks something like this:\n\n```python:title=preprocess.py\nfrom cowait import task\n\n@task\nasync def Preprocess(dataset_url='s3://big-data-set'):\n    #\n    # data reading and data transformation code\n    # ...\n\n    return {\n      # New output location. We use self.task.id to\n      # generate a unique identifier for this dataset.\n      new_location: f's3://preprocessed/{self.task.id}'\n    }\n```\n\nBefore investing time to run the big job, let's make sure everything works (inputs, reading data code, outputs, writing data) with a smaller dataset:\n\n```python:title=test_preprocess.py\nfrom preprocess import Preprocess\n\nasync def test_preprocess():\n    # define the task so we can grab the task id.\n    # The task will start executing in the background\n    task = Preprocess(dataset_url='s3://small-data-set')\n\n    # Wait for the task to finish\n    result = await task\n\n    assert result == {\n      new_location: f's3://preprocesssed/{task.id}'\n    }\n    # ...\n    # further assertions like data written, rows, size, columns or whatever\n    # ...\n```\n\n```shell\ncowait test\n```\n\nOf course, the above example would read data to your local machine. For very small datasets, this is probably fine, but you probably want to test on medium or large datasets as well, in a production cluster environment. For this use case, Cowait provides the CLI argument `--cluster` to `cowait test` that allows you to run your [tests on Kubernetes](/docs/kubernetes/testing/).\n", "---\ntitle: Next steps\n---\n\n## Where to next?\n\nYou should now have a grasp of the basic building blocks of Cowait. Of course, you have only scratched the surface. We recommend you read through the [Setup](/docs/setup/configuration) section where you will learn more about setting up Cowait.\n\nSome logical next steps after that:\n\n1. Look at some example tasks at [github](https://github.com/backtick-se/cowait/tree/master/examples)\n2. Learn how to run your tasks on [Kubernetes](/docs/kubernetes/setup/)\n3. Learn more about Cowait's [type system](/docs/tasks/type-system/)\n4. Learn how to communicate between your tasks using [RPC](/docs/tasks/remote-procedure-calls/)\n5. Learn how to run background tasks in [jupyter notebooks](/docs/extras/notebook-integration/)\n6. Look at some [built in tasks](/docs/tasks/built-in-tasks/).\n", "---\ntitle: Installation\n---\n\nInstalling Cowait on your local machine.\n\n## Requirements\n\nCowait is a python library that packages and runs tasks in Docker containers, both locally and on [Kubernetes](https://kubernetes.io/). The base requirements are:\n\n- Python 3.6+\n- [Docker](https://docs.docker.com/get-docker/)\n\n## Installation\n\nCowait is available on [Pypi](https://pypi.org/project/cowait/), you can install it with `pip`:\n\n```shell\npython -m pip install cowait\n```\n\nWe recommend installing in a virtual environment ([virtualenv](https://github.com/pypa/virtualenv)/[venv](https://docs.python.org/3/library/venv.html)) or using a python package manager such as [Poetry](https://python-poetry.org/) or [Pipenv](https://pipenv.pypa.io/en/latest/).\n\nTo quickly get started with Cowait, we provide a slim Docker image (~59 MB) that includes the Cowait library. It is based on this [Dockerfile](https://github.com/backtick-se/cowait/blob/master/Dockerfile). Pull the latest image.\n\n```shell\ndocker pull cowait/task\n```\n\nYou are now ready for your [first steps](/docs/get-started/first-steps/).\n\n## Development\n\nIf you would like to contribute to Cowait, you may install Cowait from source:\n\n1. Clone the repository\n\n```shell\ngit clone git@github.com:backtick-se/cowait.git\ncd cowait\n```\n\n2. It is recommended to first setup a virtual env of your choice. A `pyproject.toml` for Poetry is provided for your convenience in the root of the repository.\n\n3. Install the library using pip's editable mode.\n\n```shell\npython -m pip install -e .\n```\n\n4. Make changes to the library. Note that changes to the `cowait/` directory require a rebuild of the base image. You can do this with the provided helper script in the root of the repository:\n\n```shell\n./build.sh\n```\n\n5. Note that tasks you use to test your new feature or bug-fix will have to be rebuilt with `cowait build` for the changes to take effect.\n", "---\ntitle: Asyncio\n---\n\n`asyncio` is a library to write concurrent code using the `async / await` syntax. `asyncio` is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\n## Asyncio & Cowait\n\nCowait uses `asyncio` for running concurrent tasks. `asyncio` has a very simple interface to write sequential (blocking) and parallel (non-blocking) operations.\n\nRemember that in Cowait, a call to another task will be intercepted by the Cowait runtime and executed in a separate container \u2014 potentially on a different machine.\n\nBefore we get into running concurrent tasks, let's look at a very simple example that uses `asyncio` to sleep for 5 seconds.\n\n```python:title=sleep.py\nimport asyncio\nfrom cowait import task\n\n@task\nasync def Sleep(duration: int = 5):\n    for i in range(duration):\n      await asyncio.sleep(1) # blocking\n      print(\"slept\", i + 1)\n\n    return {\n        \"duration\": duration,\n    }\n```\n\nCowait tasks are defined with the `async` keyword. This allows us to wait for other tasks in an asynchronous fashion, or to use basic features from `asyncio`, like `sleep(n)`. Run the above example:\n\n```shell\ncowait run sleep\n```\n\n## Parallel Tasks\n\nOne of the core features of Cowait is its simple interface to parallelize work on multiple containers. Let's add a new task that spawns multiple `Sleep` tasks in parallel:\n\n```python:title=parallel.py\nimport asyncio\nfrom cowait import task, join\nfrom sleep import Sleep\n\n@task\nasync def Parallel():\n    tasks = [Sleep(duration=5), Sleep(duration=5)]\n\n    result = await join(tasks)\n\n    return result\n\n```\n\nHere, Cowait provides a utility method `join` to wait for a list of tasks. It is a very simple wrapper for [asyncio.gather()](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/ops.py). Run your new parallel task:\n\n```shell\ncowait run parallel\n```\n\nThe `parallel` task creates two child containers (so 3 in total), that each sleeps for 5 seconds (in parallel). Here's an illustration of the above example:\n\n![Parallel Docker Illustration](../images/parallel_tasks_docker.svg)\n\nYou will note that the program doesn't run for precisely 5 seconds, and that the `Sleep` containers may start / exit at different times (however `parallel` will block until both are done). This is because there is some overhead in the underlying docker engine to create and spawn new containers for the tasks.\n", "---\ntitle: Spark\n---\n\nAutomatically deploy Spark clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Spark Cluster\n\nThis requires you to manually first install `pyspark`. Add it to your `requirements.txt` (or install it in your Dockerfile).\n\nSpark clusters can be created using the `SparkCluster` task.\n\n```python:title=spark_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.spark import SparkCluster\nfrom pyspark.sql import SparkSession\n\nclass YourSparkJob(Task):\n    async def run(self, inputs**):\n        cluster = SparkCluster(workers=5)\n        conf = await cluster.get_config()\n\n        # create spark session\n        session = SparkSession.builder \\\n            .config(conf=conf) \\\n            .getOrCreate()\n\n        # use your Spark SQL session!\n\n        # you can also scale the cluster at will:\n        await cluster.scale(workers=2)\n\n        return \"Spark job exited\"\n```\n\nRun it:\n\n```shell\ncowait run Spark_cluster\n```\n\n## SparkCluster RPC Methods\n\nThe SparkCluster task will automatically set up a Spark scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.Spark.SparkCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |     Get informations about all Spark workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_config()`        |              Returns the Spark configuration |\n| `teardown()`          |    Stop your Spark cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/spark/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Dashboard\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Dask\n---\n\nAutomatically deploy Dask clusters for your projects with a single line of code.\n\n**WARNING**: This is very experimental and currently just a proof of concept. Proceed at your own risk.\n\n## Defining and running a Dask Cluster\n\nDask clusters can be created using the `DaskCluster` task.\n\n```python:title=dask_cluster.py\nfrom cowait.tasks import Task\nfrom cowait.tasks.dask import DaskCluster\n\nclass YourDaskJob(Task):\n    async def run(self, dask, inputs**):\n        cluster = DaskCluster(workers=5)\n        client = await cluster.get_client()\n        # dask client ready to use!\n\n        def square(x):\n            return x ** 2\n\n        def neg(x):\n            return -x\n\n        A = client.map(square, range(10))\n        B = client.map(neg, A)\n\n        total = client.submit(sum, B)\n        result = total.result()\n\n        print(result)\n\n        return result\n```\n\nRun it:\n\n```shell\ncowait run dask_cluster\n```\n\n## DaskCluster RPC Methods\n\nThe DaskCluster task will automatically set up a Dask scheduler and a set of workers. It provides a number of RPC methods for controlling your cluster, summarized below:\n\n### `cowait.tasks.dask.DaskCluster`\n\n| RPC Method            |                                  Description |\n| --------------------- | -------------------------------------------: |\n| `get_workers()`       |      Get informations about all Dask workers |\n| `scale(workers: int)` | Can be used to scale up or down your cluster |\n| `get_scheduler_uri()` |               Returns the Dask scheduler URI |\n| `get_client()`        |                      Returns the dask client |\n| `teardown()`          |     Stop your Dask cluster task from running |\n\nSee [github](https://github.com/backtick-se/cowait/blob/master/cowait/tasks/dask/cluster.py) for full reference.\n\n**WARNING**: This is very experimental and currently just a proof of concept.\n", "---\ntitle: Databricks\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n", "---\ntitle: Notebook Integration\n---\n\n## Contributions Welcome\n\nThis part of the documentation does not yet exist. You can help create it on github, see the link below.\n"], "number": 296, "title": "Docs content", "labels": [{"id": 1562464618, "node_id": "MDU6TGFiZWwxNTYyNDY0NjE4", "url": "https://api.github.com/repos/backtick-se/cowait/labels/documentation", "name": "documentation", "color": "0075ca", "default": true, "description": "Improvements or additions to documentation"}, {"id": 1562464620, "node_id": "MDU6TGFiZWwxNTYyNDY0NjIw", "url": "https://api.github.com/repos/backtick-se/cowait/labels/enhancement", "name": "enhancement", "color": "9cdd56", "default": true, "description": "Improve existing features"}, {"id": 2035840512, "node_id": "MDU6TGFiZWwyMDM1ODQwNTEy", "url": "https://api.github.com/repos/backtick-se/cowait/labels/onboarding", "name": "onboarding", "color": "f4d9a6", "default": false, "description": "Onboarding / Installation"}], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/296", "html_url": "https://github.com/backtick-se/cowait/pull/296", "diff_url": "https://github.com/backtick-se/cowait/pull/296.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/296.patch", "merged_at": "2021-05-17T12:33:47Z"}, "body": "Leaving this PR open for some time to test some markdown + images rendering.\r\n\r\nEdit:\r\nAdded markdown folders and content. The `/docs` folder specifies to docs content on https://cowait.io/docs/"}, {"paths": ["/Users/shabo/Documents/Backtick/exjobb/cowait/docs/quick-start.md"], "contents": ["---\ntitle: Quick Start\n---\n"], "number": 294, "title": "Add example quickstart docs", "labels": [], "state": "closed", "pull_request": {"url": "https://api.github.com/repos/backtick-se/cowait/pulls/294", "html_url": "https://github.com/backtick-se/cowait/pull/294", "diff_url": "https://github.com/backtick-se/cowait/pull/294.diff", "patch_url": "https://github.com/backtick-se/cowait/pull/294.patch", "merged_at": "2021-03-29T14:22:09Z"}, "body": "- Removed docs from gitignore\r\n- Added test quick-start.md for external documentation building"}]